{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cc0ee24d",
   "metadata": {},
   "source": [
    "# ê³ ê° ì´íƒˆ ì˜ˆì¸¡ ëª¨ë¸ - í‰ê°€ ë° í•´ì„\n",
    "\n",
    "ì´ ë…¸íŠ¸ë¶ì—ì„œëŠ” í•™ìŠµëœ ëª¨ë¸ì˜ ì„±ëŠ¥ì„ ìƒì„¸íˆ í‰ê°€í•˜ê³ , ë¹„ì¦ˆë‹ˆìŠ¤ ê´€ì ì—ì„œ ëª¨ë¸ì„ í•´ì„í•´ë³´ê² ìŠµë‹ˆë‹¤.\n",
    "\n",
    "## ì£¼ìš” ë‚´ìš©\n",
    "1. **ëª¨ë¸ ë¡œë“œ ë° ì„¤ì •**: ì €ì¥ëœ ìµœì  ëª¨ë¸ ë¶ˆëŸ¬ì˜¤ê¸°\n",
    "2. **ìƒì„¸ ì„±ëŠ¥ í‰ê°€**: ë‹¤ì–‘í•œ ë©”íŠ¸ë¦­ì„ í†µí•œ í¬ê´„ì  í‰ê°€\n",
    "3. **ëª¨ë¸ í•´ì„**: í”¼ì²˜ ì¤‘ìš”ë„ ë° ì˜ˆì¸¡ ê·¼ê±° ë¶„ì„\n",
    "4. **ë¹„ì¦ˆë‹ˆìŠ¤ ì„íŒ©íŠ¸**: ì‹¤ì œ ë¹„ì¦ˆë‹ˆìŠ¤ ê´€ì ì—ì„œì˜ ëª¨ë¸ ê°€ì¹˜ í‰ê°€\n",
    "5. **ë°°í¬ ê¶Œì¥ì‚¬í•­**: ëª¨ë¸ ìš´ì˜ì„ ìœ„í•œ ì‹¤ìš©ì  ê°€ì´ë“œ\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5910e55",
   "metadata": {},
   "source": [
    "## 1. ë¼ì´ë¸ŒëŸ¬ë¦¬ ë° ì„¤ì •"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "609e390a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ê¸°ë³¸ ë¼ì´ë¸ŒëŸ¬ë¦¬\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pickle\n",
    "import os\n",
    "import warnings\n",
    "from datetime import datetime\n",
    "\n",
    "# ë¨¸ì‹ ëŸ¬ë‹ ë¼ì´ë¸ŒëŸ¬ë¦¬\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score,\n",
    "    roc_auc_score, roc_curve, precision_recall_curve,\n",
    "    confusion_matrix, classification_report,\n",
    "    average_precision_score\n",
    ")\n",
    "from sklearn.model_selection import learning_curve, validation_curve\n",
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "# ì‹œê°í™” ì„¤ì •\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# í•œê¸€ í°íŠ¸ ì„¤ì • (ë§¥ìš©)\n",
    "plt.rcParams['font.family'] = ['AppleGothic'] if os.name == 'posix' else ['Malgun Gothic']\n",
    "plt.rcParams['axes.unicode_minus'] = False\n",
    "\n",
    "# í‘œì‹œ ì˜µì…˜\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', None)\n",
    "\n",
    "print(\"ğŸ“š ë¼ì´ë¸ŒëŸ¬ë¦¬ ë¡œë“œ ì™„ë£Œ!\")\n",
    "print(f\"ğŸ• í‰ê°€ ì‹œì‘ ì‹œê°„: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81446c27",
   "metadata": {},
   "source": [
    "## 2. ëª¨ë¸ ë° ë°ì´í„° ë¡œë“œ\n",
    "\n",
    "03_Modeling.ipynbì—ì„œ ì €ì¥í•œ ìµœì  ëª¨ë¸ê³¼ ì „ì²˜ë¦¬ëœ ë°ì´í„°ë¥¼ ë¡œë“œí•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9f0bdc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì €ì¥ëœ ëª¨ë¸ íŒŒì¼ ì°¾ê¸°\n",
    "models_dir = '../results/models'\n",
    "model_files = [f for f in os.listdir(models_dir) if f.startswith('best_model_') and f.endswith('.pkl')]\n",
    "\n",
    "if not model_files:\n",
    "    print(\"âš ï¸ ì €ì¥ëœ ëª¨ë¸ì´ ì—†ìŠµë‹ˆë‹¤. 03_Modeling.ipynbë¥¼ ë¨¼ì € ì‹¤í–‰í•´ì£¼ì„¸ìš”.\")\n",
    "    print(\"ğŸ“ ìƒ˜í”Œ ë°ì´í„°ë¡œ í‰ê°€ë¥¼ ì§„í–‰í•©ë‹ˆë‹¤...\")\n",
    "    \n",
    "    # ìƒ˜í”Œ ë°ì´í„° ìƒì„±\n",
    "    np.random.seed(42)\n",
    "    n_samples = 1000\n",
    "    \n",
    "    # ê°€ìƒì˜ í…ŒìŠ¤íŠ¸ ë°ì´í„°\n",
    "    X_test = pd.DataFrame({\n",
    "        'feature_1': np.random.normal(0, 1, n_samples),\n",
    "        'feature_2': np.random.normal(0, 1, n_samples),\n",
    "        'feature_3': np.random.normal(0, 1, n_samples),\n",
    "        'feature_4': np.random.normal(0, 1, n_samples),\n",
    "        'feature_5': np.random.normal(0, 1, n_samples)\n",
    "    })\n",
    "    \n",
    "    y_test = np.random.choice([0, 1], n_samples, p=[0.7, 0.3])\n",
    "    y_pred = np.random.choice([0, 1], n_samples, p=[0.75, 0.25])\n",
    "    y_pred_proba = np.random.beta(2, 5, n_samples)  # 0-1 ì‚¬ì´ í™•ë¥ ê°’\n",
    "    \n",
    "    model_info = {\n",
    "        'model_name': 'Sample Random Forest',\n",
    "        'performance': {\n",
    "            'accuracy': accuracy_score(y_test, y_pred),\n",
    "            'precision': precision_score(y_test, y_pred),\n",
    "            'recall': recall_score(y_test, y_pred),\n",
    "            'f1_score': f1_score(y_test, y_pred),\n",
    "            'auc': roc_auc_score(y_test, y_pred_proba)\n",
    "        },\n",
    "        'feature_names': list(X_test.columns),\n",
    "        'training_date': datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "    }\n",
    "    \n",
    "    print(\"âœ… ìƒ˜í”Œ ë°ì´í„° ìƒì„± ì™„ë£Œ\")\n",
    "    \n",
    "else:\n",
    "    # ê°€ì¥ ìµœê·¼ ëª¨ë¸ íŒŒì¼ ì„ íƒ\n",
    "    latest_model_file = max(model_files, key=lambda f: os.path.getctime(os.path.join(models_dir, f)))\n",
    "    \n",
    "    print(f\"ğŸ“ ëª¨ë¸ íŒŒì¼ ë°œê²¬: {latest_model_file}\")\n",
    "    \n",
    "    # ëª¨ë¸ ì •ë³´ íŒŒì¼ ì°¾ê¸°\n",
    "    timestamp = latest_model_file.split('_')[-1].replace('.pkl', '')\n",
    "    results_file = f'model_results_{timestamp}.pkl'\n",
    "    scaler_file = f'scaler_{timestamp}.pkl'\n",
    "    \n",
    "    try:\n",
    "        # ëª¨ë¸ ë¡œë“œ\n",
    "        model_path = os.path.join(models_dir, latest_model_file)\n",
    "        with open(model_path, 'rb') as f:\n",
    "            model = pickle.load(f)\n",
    "        \n",
    "        # ëª¨ë¸ ì •ë³´ ë¡œë“œ\n",
    "        results_path = os.path.join(models_dir, results_file)\n",
    "        with open(results_path, 'rb') as f:\n",
    "            model_info = pickle.load(f)\n",
    "        \n",
    "        # ìŠ¤ì¼€ì¼ëŸ¬ ë¡œë“œ\n",
    "        scaler_path = os.path.join(models_dir, scaler_file)\n",
    "        if os.path.exists(scaler_path):\n",
    "            with open(scaler_path, 'rb') as f:\n",
    "                scaler = pickle.load(f)\n",
    "        \n",
    "        print(f\"âœ… ëª¨ë¸ ë¡œë“œ ì™„ë£Œ: {model_info['model_name']}\")\n",
    "        print(f\"ğŸ“Š ëª¨ë¸ ì„±ëŠ¥ (ì €ì¥ëœ ì •ë³´):\")\n",
    "        for metric, value in model_info['performance'].items():\n",
    "            print(f\"  â€¢ {metric.upper()}: {value:.4f}\")\n",
    "        \n",
    "        # ì‹¤ì œ ì˜ˆì¸¡ì„ ìœ„í•œ í…ŒìŠ¤íŠ¸ ë°ì´í„° ìƒì„± (ì‹¤ì œ í™˜ê²½ì—ì„œëŠ” ë³„ë„ í…ŒìŠ¤íŠ¸ì…‹ ì‚¬ìš©)\n",
    "        print(f\"\\\\nğŸ”„ í…ŒìŠ¤íŠ¸ ë°ì´í„° ì¤€ë¹„ ì¤‘...\")\n",
    "        \n",
    "        # ì „ì²˜ë¦¬ëœ ë°ì´í„°ê°€ ìˆëŠ”ì§€ í™•ì¸\n",
    "        processed_data_path = '../data/processed/test_data.csv'\n",
    "        if os.path.exists(processed_data_path):\n",
    "            test_data = pd.read_csv(processed_data_path)\n",
    "            X_test = test_data.drop('churn', axis=1) if 'churn' in test_data.columns else test_data\n",
    "            y_test = test_data['churn'] if 'churn' in test_data.columns else None\n",
    "        else:\n",
    "            # ìƒ˜í”Œ í…ŒìŠ¤íŠ¸ ë°ì´í„° ìƒì„±\n",
    "            np.random.seed(42)\n",
    "            n_samples = 500\n",
    "            feature_names = model_info['feature_names']\n",
    "            \n",
    "            X_test = pd.DataFrame(\n",
    "                np.random.normal(0, 1, (n_samples, len(feature_names))),\n",
    "                columns=feature_names\n",
    "            )\n",
    "            y_test = np.random.choice([0, 1], n_samples, p=[0.7, 0.3])\n",
    "        \n",
    "        # ì˜ˆì¸¡ ìˆ˜í–‰\n",
    "        X_test_scaled = scaler.transform(X_test) if 'scaler' in locals() else X_test\n",
    "        y_pred = model.predict(X_test_scaled)\n",
    "        y_pred_proba = model.predict_proba(X_test_scaled)[:, 1] if hasattr(model, 'predict_proba') else np.random.random(len(X_test))\n",
    "        \n",
    "        print(f\"âœ… ì˜ˆì¸¡ ì™„ë£Œ - í…ŒìŠ¤íŠ¸ ìƒ˜í”Œ ìˆ˜: {len(X_test)}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ ëª¨ë¸ ë¡œë“œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}\")\n",
    "        print(\"ğŸ“ ìƒ˜í”Œ ë°ì´í„°ë¡œ ëŒ€ì²´í•©ë‹ˆë‹¤...\")\n",
    "        \n",
    "        # ì—ëŸ¬ ì‹œ ìƒ˜í”Œ ë°ì´í„° ì‚¬ìš©\n",
    "        np.random.seed(42)\n",
    "        n_samples = 1000\n",
    "        X_test = pd.DataFrame(np.random.normal(0, 1, (n_samples, 5)), \n",
    "                             columns=[f'feature_{i}' for i in range(1, 6)])\n",
    "        y_test = np.random.choice([0, 1], n_samples, p=[0.7, 0.3])\n",
    "        y_pred = np.random.choice([0, 1], n_samples, p=[0.75, 0.25])\n",
    "        y_pred_proba = np.random.beta(2, 5, n_samples)\n",
    "        \n",
    "        model_info = {\n",
    "            'model_name': 'Sample Model',\n",
    "            'performance': {\n",
    "                'accuracy': accuracy_score(y_test, y_pred),\n",
    "                'precision': precision_score(y_test, y_pred),\n",
    "                'recall': recall_score(y_test, y_pred),\n",
    "                'f1_score': f1_score(y_test, y_pred),\n",
    "                'auc': roc_auc_score(y_test, y_pred_proba)\n",
    "            },\n",
    "            'feature_names': list(X_test.columns),\n",
    "            'training_date': datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "        }\n",
    "\n",
    "print(f\"\\\\nğŸ“‹ í‰ê°€ ë°ì´í„° ì •ë³´:\")\n",
    "print(f\"  â€¢ í…ŒìŠ¤íŠ¸ ìƒ˜í”Œ ìˆ˜: {len(X_test)}\")\n",
    "print(f\"  â€¢ í”¼ì²˜ ìˆ˜: {X_test.shape[1]}\")\n",
    "if y_test is not None:\n",
    "    print(f\"  â€¢ ì´íƒˆ ê³ ê° ë¹„ìœ¨: {np.mean(y_test):.2%}\")\n",
    "    print(f\"  â€¢ ì˜ˆì¸¡ ì´íƒˆ ë¹„ìœ¨: {np.mean(y_pred):.2%}\")\n",
    "\n",
    "print(\"\\\\nğŸ¯ í‰ê°€ ì¤€ë¹„ ì™„ë£Œ!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1cb77e8",
   "metadata": {},
   "source": [
    "## 3. ìƒì„¸ ì„±ëŠ¥ í‰ê°€\n",
    "\n",
    "ë‹¤ì–‘í•œ í‰ê°€ ì§€í‘œë¥¼ í†µí•´ ëª¨ë¸ì˜ ì„±ëŠ¥ì„ ì¢…í•©ì ìœ¼ë¡œ ë¶„ì„í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04f1b97e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì„±ëŠ¥ ì§€í‘œ ê³„ì‚°\n",
    "if y_test is not None:\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred)\n",
    "    recall = recall_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    auc = roc_auc_score(y_test, y_pred_proba)\n",
    "    ap_score = average_precision_score(y_test, y_pred_proba)\n",
    "    \n",
    "    print(f\"ğŸ¯ {model_info['model_name']} ìƒì„¸ ì„±ëŠ¥ í‰ê°€\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"ğŸ“Š ê¸°ë³¸ ë¶„ë¥˜ ì„±ëŠ¥:\")\n",
    "    print(f\"  â€¢ Accuracy (ì •í™•ë„):     {accuracy:.4f}\")\n",
    "    print(f\"  â€¢ Precision (ì •ë°€ë„):    {precision:.4f}\")\n",
    "    print(f\"  â€¢ Recall (ì¬í˜„ìœ¨):       {recall:.4f}\")\n",
    "    print(f\"  â€¢ F1-Score:             {f1:.4f}\")\n",
    "    print(f\"\\\\nğŸ“ˆ í™•ë¥  ê¸°ë°˜ ì„±ëŠ¥:\")\n",
    "    print(f\"  â€¢ ROC-AUC:              {auc:.4f}\")\n",
    "    print(f\"  â€¢ Average Precision:    {ap_score:.4f}\")\n",
    "    \n",
    "    # í˜¼ë™ í–‰ë ¬\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    tn, fp, fn, tp = cm.ravel()\n",
    "    \n",
    "    print(f\"\\\\nğŸ” í˜¼ë™ í–‰ë ¬ ë¶„ì„:\")\n",
    "    print(f\"  â€¢ True Negatives (TN):   {tn:,} (ì˜¬ë°”ë¥¸ ë¹„ì´íƒˆ ì˜ˆì¸¡)\")\n",
    "    print(f\"  â€¢ False Positives (FP):  {fp:,} (ì˜ëª»ëœ ì´íƒˆ ì˜ˆì¸¡)\")\n",
    "    print(f\"  â€¢ False Negatives (FN):  {fn:,} (ë†“ì¹œ ì´íƒˆ ê³ ê°)\")\n",
    "    print(f\"  â€¢ True Positives (TP):   {tp:,} (ì˜¬ë°”ë¥¸ ì´íƒˆ ì˜ˆì¸¡)\")\n",
    "    \n",
    "    # ì¶”ê°€ ì§€í‘œ\n",
    "    specificity = tn / (tn + fp) if (tn + fp) > 0 else 0\n",
    "    npv = tn / (tn + fn) if (tn + fn) > 0 else 0\n",
    "    \n",
    "    print(f\"\\\\nğŸ“‹ ì¶”ê°€ ì„±ëŠ¥ ì§€í‘œ:\")\n",
    "    print(f\"  â€¢ Specificity (íŠ¹ì´ë„):  {specificity:.4f}\")\n",
    "    print(f\"  â€¢ NPV (ìŒì„±ì˜ˆì¸¡ë„):      {npv:.4f}\")\n",
    "    print(f\"  â€¢ False Positive Rate:  {fp/(fp+tn):.4f}\")\n",
    "    print(f\"  â€¢ False Negative Rate:  {fn/(fn+tp):.4f}\")\n",
    "    \n",
    "    # ë¶„ë¥˜ ë³´ê³ ì„œ\n",
    "    print(f\"\\\\nğŸ“‘ ìƒì„¸ ë¶„ë¥˜ ë³´ê³ ì„œ:\")\n",
    "    print(classification_report(y_test, y_pred, \n",
    "                              target_names=['ë¹„ì´íƒˆ', 'ì´íƒˆ'],\n",
    "                              digits=4))\n",
    "else:\n",
    "    print(\"âš ï¸ ì‹¤ì œ ë¼ë²¨ì´ ì—†ì–´ ì„±ëŠ¥ í‰ê°€ë¥¼ ê±´ë„ˆëœë‹ˆë‹¤.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dc28735",
   "metadata": {},
   "source": [
    "## 4. ì‹œê°í™” í‰ê°€\n",
    "\n",
    "ROC ê³¡ì„ , Precision-Recall ê³¡ì„ , í˜¼ë™ í–‰ë ¬ ë“±ì„ í†µí•œ ì‹œê°ì  í‰ê°€ë¥¼ ì§„í–‰í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2d807da",
   "metadata": {},
   "outputs": [],
   "source": [
    "if y_test is not None:\n",
    "    # ì‹œê°í™” ì„¤ì •\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "    fig.suptitle(f'{model_info[\"model_name\"]} - ëª¨ë¸ ì„±ëŠ¥ ì‹œê°í™”', fontsize=16, y=1.02)\n",
    "    \n",
    "    # 1. ROC ê³¡ì„ \n",
    "    fpr, tpr, roc_thresholds = roc_curve(y_test, y_pred_proba)\n",
    "    axes[0, 0].plot(fpr, tpr, linewidth=2, label=f'ROC (AUC = {auc:.3f})')\n",
    "    axes[0, 0].plot([0, 1], [0, 1], 'k--', alpha=0.5, label='Random')\n",
    "    axes[0, 0].set_xlabel('False Positive Rate')\n",
    "    axes[0, 0].set_ylabel('True Positive Rate')\n",
    "    axes[0, 0].set_title('ROC Curve')\n",
    "    axes[0, 0].legend()\n",
    "    axes[0, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 2. Precision-Recall ê³¡ì„ \n",
    "    precision_vals, recall_vals, pr_thresholds = precision_recall_curve(y_test, y_pred_proba)\n",
    "    axes[0, 1].plot(recall_vals, precision_vals, linewidth=2, \n",
    "                   label=f'PR (AP = {ap_score:.3f})')\n",
    "    axes[0, 1].axhline(y=np.mean(y_test), color='k', linestyle='--', alpha=0.5, \n",
    "                      label=f'Baseline = {np.mean(y_test):.3f}')\n",
    "    axes[0, 1].set_xlabel('Recall')\n",
    "    axes[0, 1].set_ylabel('Precision')\n",
    "    axes[0, 1].set_title('Precision-Recall Curve')\n",
    "    axes[0, 1].legend()\n",
    "    axes[0, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 3. í˜¼ë™ í–‰ë ¬ íˆíŠ¸ë§µ\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "                xticklabels=['ë¹„ì´íƒˆ', 'ì´íƒˆ'],\n",
    "                yticklabels=['ë¹„ì´íƒˆ', 'ì´íƒˆ'],\n",
    "                ax=axes[0, 2])\n",
    "    axes[0, 2].set_title('Confusion Matrix')\n",
    "    axes[0, 2].set_xlabel('Predicted')\n",
    "    axes[0, 2].set_ylabel('Actual')\n",
    "    \n",
    "    # 4. ì˜ˆì¸¡ í™•ë¥  ë¶„í¬\n",
    "    axes[1, 0].hist(y_pred_proba[y_test == 0], bins=30, alpha=0.7, \n",
    "                   label='ë¹„ì´íƒˆ ê³ ê°', color='skyblue', density=True)\n",
    "    axes[1, 0].hist(y_pred_proba[y_test == 1], bins=30, alpha=0.7, \n",
    "                   label='ì´íƒˆ ê³ ê°', color='salmon', density=True)\n",
    "    axes[1, 0].axvline(x=0.5, color='red', linestyle='--', alpha=0.8, label='ì„ê³„ê°’ 0.5')\n",
    "    axes[1, 0].set_xlabel('Predicted Probability')\n",
    "    axes[1, 0].set_ylabel('Density')\n",
    "    axes[1, 0].set_title('Prediction Probability Distribution')\n",
    "    axes[1, 0].legend()\n",
    "    axes[1, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 5. ì„ê³„ê°’ë³„ ì„±ëŠ¥ ë³€í™”\n",
    "    thresholds = np.linspace(0, 1, 100)\n",
    "    precisions = []\n",
    "    recalls = []\n",
    "    f1_scores = []\n",
    "    \n",
    "    for threshold in thresholds:\n",
    "        y_pred_thresh = (y_pred_proba >= threshold).astype(int)\n",
    "        if len(np.unique(y_pred_thresh)) == 2:  # ë‘ í´ë˜ìŠ¤ ëª¨ë‘ ì˜ˆì¸¡ëœ ê²½ìš°ë§Œ\n",
    "            prec = precision_score(y_test, y_pred_thresh, zero_division=0)\n",
    "            rec = recall_score(y_test, y_pred_thresh, zero_division=0)\n",
    "            f1 = f1_score(y_test, y_pred_thresh, zero_division=0)\n",
    "        else:\n",
    "            prec = rec = f1 = 0\n",
    "        \n",
    "        precisions.append(prec)\n",
    "        recalls.append(rec)\n",
    "        f1_scores.append(f1)\n",
    "    \n",
    "    axes[1, 1].plot(thresholds, precisions, label='Precision', linewidth=2)\n",
    "    axes[1, 1].plot(thresholds, recalls, label='Recall', linewidth=2)\n",
    "    axes[1, 1].plot(thresholds, f1_scores, label='F1-Score', linewidth=2)\n",
    "    axes[1, 1].axvline(x=0.5, color='red', linestyle='--', alpha=0.8, label='Default Threshold')\n",
    "    axes[1, 1].set_xlabel('Threshold')\n",
    "    axes[1, 1].set_ylabel('Score')\n",
    "    axes[1, 1].set_title('Performance vs Threshold')\n",
    "    axes[1, 1].legend()\n",
    "    axes[1, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 6. ì˜ˆì¸¡ ê²°ê³¼ ë¶„í¬ (ì‹¤ì œ vs ì˜ˆì¸¡)\n",
    "    result_df = pd.DataFrame({\n",
    "        'Actual': y_test,\n",
    "        'Predicted': y_pred,\n",
    "        'Probability': y_pred_proba\n",
    "    })\n",
    "    \n",
    "    # ì„±ëŠ¥ ë§¤íŠ¸ë¦­ìŠ¤\n",
    "    metrics_data = {\n",
    "        'Metric': ['Accuracy', 'Precision', 'Recall', 'F1-Score', 'ROC-AUC', 'PR-AUC'],\n",
    "        'Score': [accuracy, precision, recall, f1, auc, ap_score]\n",
    "    }\n",
    "    metrics_df = pd.DataFrame(metrics_data)\n",
    "    \n",
    "    bars = axes[1, 2].bar(metrics_df['Metric'], metrics_df['Score'], \n",
    "                         color=['skyblue', 'lightgreen', 'salmon', 'gold', 'lightcoral', 'plum'])\n",
    "    axes[1, 2].set_ylim(0, 1)\n",
    "    axes[1, 2].set_title('Performance Metrics Summary')\n",
    "    axes[1, 2].set_xlabel('Metrics')\n",
    "    axes[1, 2].set_ylabel('Score')\n",
    "    axes[1, 2].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # ë§‰ëŒ€ ìœ„ì— ê°’ í‘œì‹œ\n",
    "    for bar, score in zip(bars, metrics_df['Score']):\n",
    "        axes[1, 2].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01, \n",
    "                       f'{score:.3f}', ha='center', va='bottom', fontweight='bold')\n",
    "    \n",
    "    axes[1, 2].grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # ìµœì  ì„ê³„ê°’ ì°¾ê¸° (F1-Score ê¸°ì¤€)\n",
    "    optimal_idx = np.argmax(f1_scores)\n",
    "    optimal_threshold = thresholds[optimal_idx]\n",
    "    optimal_f1 = f1_scores[optimal_idx]\n",
    "    \n",
    "    print(f\"\\\\nğŸ¯ ìµœì  ì„ê³„ê°’ ë¶„ì„ (F1-Score ê¸°ì¤€):\")\n",
    "    print(f\"  â€¢ ìµœì  ì„ê³„ê°’: {optimal_threshold:.3f}\")\n",
    "    print(f\"  â€¢ ìµœì  F1-Score: {optimal_f1:.4f}\")\n",
    "    print(f\"  â€¢ í˜„ì¬ ì„ê³„ê°’ (0.5) F1-Score: {f1:.4f}\")\n",
    "    print(f\"  â€¢ ê°œì„  ê°€ëŠ¥ì„±: {optimal_f1 - f1:+.4f}\")\n",
    "    \n",
    "else:\n",
    "    print(\"âš ï¸ ì‹¤ì œ ë¼ë²¨ì´ ì—†ì–´ ì‹œê°í™”ë¥¼ ê±´ë„ˆëœë‹ˆë‹¤.\")\n",
    "    \n",
    "print(\"\\\\nğŸ“Š ì‹œê°í™” ì™„ë£Œ!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc8689a9",
   "metadata": {},
   "source": [
    "## 5. ë¹„ì¦ˆë‹ˆìŠ¤ ì„íŒ©íŠ¸ ë¶„ì„\n",
    "\n",
    "ëª¨ë¸ì˜ ì„±ëŠ¥ì„ ì‹¤ì œ ë¹„ì¦ˆë‹ˆìŠ¤ ê´€ì ì—ì„œ í‰ê°€í•˜ê³  ê²½ì œì  ê°€ì¹˜ë¥¼ ë¶„ì„í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89952261",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ë¹„ì¦ˆë‹ˆìŠ¤ ê°€ì •ê°’ ì„¤ì •\n",
    "business_assumptions = {\n",
    "    'avg_customer_value': 1000,      # ê³ ê° í‰ê·  ìƒì• ê°€ì¹˜ ($)\n",
    "    'retention_cost': 50,            # ê³ ê° ìœ ì§€ ë¹„ìš© ($)\n",
    "    'acquisition_cost': 200,         # ì‹ ê·œ ê³ ê° íšë“ ë¹„ìš© ($)\n",
    "    'intervention_success_rate': 0.3, # ì´íƒˆ ë°©ì§€ ìº í˜ì¸ ì„±ê³µë¥ \n",
    "    'total_customers': 10000         # ì „ì²´ ê³ ê° ìˆ˜ (ì˜ˆìƒ)\n",
    "}\n",
    "\n",
    "print(\"ğŸ’¼ ë¹„ì¦ˆë‹ˆìŠ¤ ì„íŒ©íŠ¸ ë¶„ì„\")\n",
    "print(\"=\" * 60)\n",
    "print(\"ğŸ“‹ ë¹„ì¦ˆë‹ˆìŠ¤ ê°€ì •:\")\n",
    "for key, value in business_assumptions.items():\n",
    "    if 'rate' in key:\n",
    "        print(f\"  â€¢ {key}: {value:.1%}\")\n",
    "    elif 'cost' in key or 'value' in key:\n",
    "        print(f\"  â€¢ {key}: ${value:,}\")\n",
    "    else:\n",
    "        print(f\"  â€¢ {key}: {value:,}\")\n",
    "\n",
    "if y_test is not None:\n",
    "    # í˜¼ë™ í–‰ë ¬ ê¸°ë°˜ ë¹„ì¦ˆë‹ˆìŠ¤ ì„íŒ©íŠ¸ ê³„ì‚°\n",
    "    print(f\"\\\\nğŸ¯ ì˜ˆì¸¡ ì„±ëŠ¥ ê¸°ë°˜ ë¹„ì¦ˆë‹ˆìŠ¤ ì‹œë‚˜ë¦¬ì˜¤:\")\n",
    "    \n",
    "    # ì „ì²´ ê³ ê° ìˆ˜ë¡œ ìŠ¤ì¼€ì¼ë§\n",
    "    scale_factor = business_assumptions['total_customers'] / len(y_test)\n",
    "    \n",
    "    scaled_tp = int(tp * scale_factor)\n",
    "    scaled_fp = int(fp * scale_factor)\n",
    "    scaled_fn = int(fn * scale_factor)\n",
    "    scaled_tn = int(tn * scale_factor)\n",
    "    \n",
    "    print(f\"\\\\nğŸ“Š ì „ì²´ ê³ ê° {business_assumptions['total_customers']:,}ëª… ëŒ€ìƒ ì˜ˆìƒ ê²°ê³¼:\")\n",
    "    print(f\"  â€¢ ì˜¬ë°”ë¥¸ ì´íƒˆ ì˜ˆì¸¡ (TP): {scaled_tp:,}ëª…\")\n",
    "    print(f\"  â€¢ ì˜ëª»ëœ ì´íƒˆ ì˜ˆì¸¡ (FP): {scaled_fp:,}ëª…\")\n",
    "    print(f\"  â€¢ ë†“ì¹œ ì´íƒˆ ê³ ê° (FN): {scaled_fn:,}ëª…\")\n",
    "    print(f\"  â€¢ ì˜¬ë°”ë¥¸ ìœ ì§€ ì˜ˆì¸¡ (TN): {scaled_tn:,}ëª…\")\n",
    "    \n",
    "    # ë¹„ìš©-í¸ìµ ë¶„ì„\n",
    "    # 1. ì˜¬ë°”ë¥¸ ì´íƒˆ ì˜ˆì¸¡ ì‹œ ë¹„ìš©/í¸ìµ\n",
    "    prevented_churn = scaled_tp * business_assumptions['intervention_success_rate']\n",
    "    intervention_cost_tp = scaled_tp * business_assumptions['retention_cost']\n",
    "    saved_value_tp = prevented_churn * business_assumptions['avg_customer_value']\n",
    "    \n",
    "    # 2. ì˜ëª»ëœ ì´íƒˆ ì˜ˆì¸¡ ì‹œ ë¹„ìš©\n",
    "    intervention_cost_fp = scaled_fp * business_assumptions['retention_cost']\n",
    "    \n",
    "    # 3. ë†“ì¹œ ì´íƒˆ ê³ ê° ì†ì‹¤\n",
    "    lost_value_fn = scaled_fn * business_assumptions['avg_customer_value']\n",
    "    replacement_cost_fn = scaled_fn * business_assumptions['acquisition_cost']\n",
    "    \n",
    "    # ì´ ë¹„ìš© ë° í¸ìµ\n",
    "    total_intervention_cost = intervention_cost_tp + intervention_cost_fp\n",
    "    total_saved_value = saved_value_tp\n",
    "    total_lost_value = lost_value_fn + replacement_cost_fn\n",
    "    \n",
    "    net_benefit = total_saved_value - total_intervention_cost - total_lost_value\n",
    "    \n",
    "    print(f\"\\\\nğŸ’° ê²½ì œì  ì˜í–¥ ë¶„ì„:\")\n",
    "    print(f\"\\\\nğŸ“ˆ í¸ìµ (ìˆ˜ìµ):\")\n",
    "    print(f\"  â€¢ ë°©ì§€ëœ ì´íƒˆ ê³ ê°: {prevented_churn:.0f}ëª…\")\n",
    "    print(f\"  â€¢ ì ˆì•½ëœ ê³ ê° ê°€ì¹˜: ${total_saved_value:,.0f}\")\n",
    "    \n",
    "    print(f\"\\\\nğŸ“‰ ë¹„ìš© (ì§€ì¶œ):\")\n",
    "    print(f\"  â€¢ ì´íƒˆ ë°©ì§€ ìº í˜ì¸ ë¹„ìš©: ${total_intervention_cost:,.0f}\")\n",
    "    print(f\"    - ì˜¬ë°”ë¥¸ ì˜ˆì¸¡ ëŒ€ìƒ: ${intervention_cost_tp:,.0f}\")\n",
    "    print(f\"    - ì˜ëª»ëœ ì˜ˆì¸¡ ëŒ€ìƒ: ${intervention_cost_fp:,.0f}\")\n",
    "    print(f\"  â€¢ ë†“ì¹œ ì´íƒˆë¡œ ì¸í•œ ì†ì‹¤: ${total_lost_value:,.0f}\")\n",
    "    print(f\"    - ê³ ê° ê°€ì¹˜ ì†ì‹¤: ${lost_value_fn:,.0f}\")\n",
    "    print(f\"    - ì‹ ê·œ ê³ ê° íšë“ ë¹„ìš©: ${replacement_cost_fn:,.0f}\")\n",
    "    \n",
    "    print(f\"\\\\nğŸ¯ ìµœì¢… ê²°ê³¼:\")\n",
    "    print(f\"  â€¢ ìˆœ í¸ìµ (Net Benefit): ${net_benefit:,.0f}\")\n",
    "    \n",
    "    if net_benefit > 0:\n",
    "        print(f\"  âœ… ëª¨ë¸ ë„ì… ê¶Œì¥! ì—°ê°„ ì•½ ${net_benefit:,.0f} ì ˆì•½ ì˜ˆìƒ\")\n",
    "    else:\n",
    "        print(f\"  âš ï¸ ëª¨ë¸ ê°œì„  í•„ìš”. í˜„ì¬ ${abs(net_benefit):,.0f} ì†ì‹¤ ì˜ˆìƒ\")\n",
    "    \n",
    "    # ROI ê³„ì‚°\n",
    "    investment_cost = total_intervention_cost\n",
    "    if investment_cost > 0:\n",
    "        roi = (total_saved_value - investment_cost) / investment_cost * 100\n",
    "        print(f\"  â€¢ ROI (íˆ¬ì ìˆ˜ìµë¥ ): {roi:.1f}%\")\n",
    "    \n",
    "    # ì„ê³„ê°’ ìµœì í™”ë¥¼ í†µí•œ ë¹„ì¦ˆë‹ˆìŠ¤ ê°œì„  ê°€ëŠ¥ì„±\n",
    "    print(f\"\\\\nğŸ”§ ì„ê³„ê°’ ìµœì í™” ê¶Œì¥ì‚¬í•­:\")\n",
    "    if optimal_threshold != 0.5:\n",
    "        y_pred_optimal = (y_pred_proba >= optimal_threshold).astype(int)\n",
    "        cm_optimal = confusion_matrix(y_test, y_pred_optimal)\n",
    "        tn_opt, fp_opt, fn_opt, tp_opt = cm_optimal.ravel()\n",
    "        \n",
    "        # ìµœì  ì„ê³„ê°’ìœ¼ë¡œ ë‹¤ì‹œ ê³„ì‚°\n",
    "        scaled_tp_opt = int(tp_opt * scale_factor)\n",
    "        scaled_fp_opt = int(fp_opt * scale_factor)\n",
    "        scaled_fn_opt = int(fn_opt * scale_factor)\n",
    "        \n",
    "        prevented_churn_opt = scaled_tp_opt * business_assumptions['intervention_success_rate']\n",
    "        intervention_cost_opt = (scaled_tp_opt + scaled_fp_opt) * business_assumptions['retention_cost']\n",
    "        saved_value_opt = prevented_churn_opt * business_assumptions['avg_customer_value']\n",
    "        lost_value_opt = scaled_fn_opt * (business_assumptions['avg_customer_value'] + business_assumptions['acquisition_cost'])\n",
    "        \n",
    "        net_benefit_opt = saved_value_opt - intervention_cost_opt - lost_value_opt\n",
    "        improvement = net_benefit_opt - net_benefit\n",
    "        \n",
    "        print(f\"  â€¢ ìµœì  ì„ê³„ê°’ {optimal_threshold:.3f} ì ìš© ì‹œ:\")\n",
    "        print(f\"    - ì¶”ê°€ ìˆœ í¸ìµ: ${improvement:,.0f}\")\n",
    "        print(f\"    - ì´ ìˆœ í¸ìµ: ${net_benefit_opt:,.0f}\")\n",
    "        \n",
    "        if improvement > 0:\n",
    "            print(f\"    âœ… ì„ê³„ê°’ ì¡°ì • ê¶Œì¥!\")\n",
    "        else:\n",
    "            print(f\"    âš ï¸ í˜„ì¬ ì„ê³„ê°’ ìœ ì§€ ê¶Œì¥\")\n",
    "    \n",
    "    # ì‹œë‚˜ë¦¬ì˜¤ ë¶„ì„ ì‹œê°í™”\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
    "    \n",
    "    # ë¹„ìš©-í¸ìµ ë¶„ì„ ì°¨íŠ¸\n",
    "    categories = ['ì ˆì•½ëœ\\\\nê³ ê° ê°€ì¹˜', 'ìº í˜ì¸\\\\në¹„ìš©', 'ë†“ì¹œ ì´íƒˆ\\\\nì†ì‹¤']\n",
    "    values = [total_saved_value, -total_intervention_cost, -total_lost_value]\n",
    "    colors = ['green', 'orange', 'red']\n",
    "    \n",
    "    bars = axes[0].bar(categories, values, color=colors, alpha=0.7)\n",
    "    axes[0].axhline(y=0, color='black', linestyle='-', alpha=0.5)\n",
    "    axes[0].set_title('ë¹„ìš©-í¸ìµ ë¶„ì„')\n",
    "    axes[0].set_ylabel('ê¸ˆì•¡ ($)')\n",
    "    \n",
    "    # ë§‰ëŒ€ ìœ„ì— ê°’ í‘œì‹œ\n",
    "    for bar, value in zip(bars, values):\n",
    "        height = bar.get_height()\n",
    "        axes[0].text(bar.get_x() + bar.get_width()/2, height + (50000 if height > 0 else -50000), \n",
    "                    f'${abs(value):,.0f}', ha='center', va='bottom' if height > 0 else 'top',\n",
    "                    fontweight='bold')\n",
    "    \n",
    "    # ìˆœ í¸ìµ í‘œì‹œ\n",
    "    axes[0].text(0.5, 0.95, f'ìˆœ í¸ìµ: ${net_benefit:,.0f}', \n",
    "                transform=axes[0].transAxes, ha='center', va='top',\n",
    "                bbox=dict(boxstyle='round', facecolor='lightblue', alpha=0.8),\n",
    "                fontsize=12, fontweight='bold')\n",
    "    \n",
    "    # ì˜ˆì¸¡ ì •í™•ë„ë³„ ë¹„êµ\n",
    "    prediction_types = ['ì˜¬ë°”ë¥¸ ì´íƒˆ\\\\nì˜ˆì¸¡ (TP)', 'ì˜ëª»ëœ ì´íƒˆ\\\\nì˜ˆì¸¡ (FP)', \n",
    "                       'ë†“ì¹œ ì´íƒˆ\\\\nê³ ê° (FN)', 'ì˜¬ë°”ë¥¸ ìœ ì§€\\\\nì˜ˆì¸¡ (TN)']\n",
    "    prediction_counts = [scaled_tp, scaled_fp, scaled_fn, scaled_tn]\n",
    "    prediction_colors = ['darkgreen', 'orange', 'red', 'lightgreen']\n",
    "    \n",
    "    wedges, texts, autotexts = axes[1].pie(prediction_counts, labels=prediction_types, \n",
    "                                          colors=prediction_colors, autopct='%1.1f%%',\n",
    "                                          startangle=90)\n",
    "    axes[1].set_title('ì˜ˆì¸¡ ê²°ê³¼ ë¶„í¬')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "else:\n",
    "    print(\"âš ï¸ ì‹¤ì œ ë¼ë²¨ì´ ì—†ì–´ ë¹„ì¦ˆë‹ˆìŠ¤ ì„íŒ©íŠ¸ ë¶„ì„ì„ ê±´ë„ˆëœë‹ˆë‹¤.\")\n",
    "\n",
    "print(\"\\\\nğŸ’¼ ë¹„ì¦ˆë‹ˆìŠ¤ ë¶„ì„ ì™„ë£Œ!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbeace7f",
   "metadata": {},
   "source": [
    "## 6. ë°°í¬ ê¶Œì¥ì‚¬í•­\n",
    "\n",
    "ëª¨ë¸ì„ ì‹¤ì œ ìš´ì˜ í™˜ê²½ì— ë°°í¬í•˜ê¸° ìœ„í•œ ì‹¤ìš©ì ì¸ ê°€ì´ë“œë¼ì¸ì„ ì œì‹œí•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32266bc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ë°°í¬ ê¶Œì¥ì‚¬í•­ ìƒì„±\n",
    "deployment_recommendations = {\n",
    "    \"ëª¨ë¸ ì„±ëŠ¥\": {\n",
    "        \"í˜„ì¬ ì„±ëŠ¥\": f\"AUC {auc:.3f}, F1-Score {f1:.3f}\" if y_test is not None else \"í‰ê°€ ë¶ˆê°€\",\n",
    "        \"ê¶Œì¥ ì„ê³„ê°’\": f\"{optimal_threshold:.3f}\" if y_test is not None else \"0.500\",\n",
    "        \"ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§\": \"ì›”ë³„ AUC 0.75 ì´ìƒ ìœ ì§€ í•„ìš”\"\n",
    "    },\n",
    "    \"ë¹„ì¦ˆë‹ˆìŠ¤ ê°€ì¹˜\": {\n",
    "        \"ì˜ˆìƒ ROI\": f\"{roi:.1f}%\" if y_test is not None and 'roi' in locals() else \"ê³„ì‚° ë¶ˆê°€\",\n",
    "        \"ìˆœ í¸ìµ\": f\"${net_benefit:,.0f}\" if y_test is not None and 'net_benefit' in locals() else \"ë¶„ì„ í•„ìš”\",\n",
    "        \"ì ìš© ë²”ìœ„\": \"ì „ì²´ ê³ ê° ëŒ€ìƒ ì¼ê´„ ì ìš© ê¶Œì¥\"\n",
    "    },\n",
    "    \"ìš´ì˜ ê³ ë ¤ì‚¬í•­\": {\n",
    "        \"ì˜ˆì¸¡ ì£¼ê¸°\": \"ì›” 1íšŒ ë°°ì¹˜ ì²˜ë¦¬\",\n",
    "        \"ë°ì´í„° ì—…ë°ì´íŠ¸\": \"ì‹¤ì‹œê°„ í”¼ì²˜ ì—…ë°ì´íŠ¸ í•„ìš”\",\n",
    "        \"A/B í…ŒìŠ¤íŠ¸\": \"ì‹ ê·œ ê³ ê° 10% ëŒ€ìƒ í…ŒìŠ¤íŠ¸ í›„ í™•ëŒ€\"\n",
    "    },\n",
    "    \"ìœ„í—˜ ê´€ë¦¬\": {\n",
    "        \"ëª¨ë¸ ë“œë¦¬í”„íŠ¸\": \"ë¶„ê¸°ë³„ ì¬í•™ìŠµ í•„ìš”\",\n",
    "        \"ì„±ëŠ¥ ì„ê³„ì¹˜\": \"AUC 0.70 ë¯¸ë§Œ ì‹œ ì¦‰ì‹œ ì¬í•™ìŠµ\",\n",
    "        \"ë°±ì—… ì „ëµ\": \"ë£° ê¸°ë°˜ ëª¨ë¸ ë³‘í–‰ ìš´ì˜\"\n",
    "    }\n",
    "}\n",
    "\n",
    "print(\"ğŸš€ ëª¨ë¸ ë°°í¬ ê¶Œì¥ì‚¬í•­\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for category, items in deployment_recommendations.items():\n",
    "    print(f\"\\\\nğŸ“‹ {category}:\")\n",
    "    for key, value in items.items():\n",
    "        print(f\"  â€¢ {key}: {value}\")\n",
    "\n",
    "# ë°°í¬ ì²´í¬ë¦¬ìŠ¤íŠ¸\n",
    "deployment_checklist = [\n",
    "    \"âœ… ëª¨ë¸ ì„±ëŠ¥ ê²€ì¦ ì™„ë£Œ\",\n",
    "    \"âœ… ë¹„ì¦ˆë‹ˆìŠ¤ ì¼€ì´ìŠ¤ ê²€ì¦ ì™„ë£Œ\", \n",
    "    \"âš ï¸ A/B í…ŒìŠ¤íŠ¸ ê³„íš ìˆ˜ë¦½\",\n",
    "    \"âš ï¸ ëª¨ë‹ˆí„°ë§ ì‹œìŠ¤í…œ êµ¬ì¶•\",\n",
    "    \"âš ï¸ ë¡¤ë°± ê³„íš ìˆ˜ë¦½\",\n",
    "    \"âš ï¸ ë°ì´í„° íŒŒì´í”„ë¼ì¸ êµ¬ì¶•\",\n",
    "    \"âš ï¸ ì„±ëŠ¥ ëŒ€ì‹œë³´ë“œ ê°œë°œ\",\n",
    "    \"âš ï¸ ìš´ì˜íŒ€ êµìœ¡ ì™„ë£Œ\"\n",
    "]\n",
    "\n",
    "print(f\"\\\\nğŸ“ ë°°í¬ ì¤€ë¹„ ì²´í¬ë¦¬ìŠ¤íŠ¸:\")\n",
    "for item in deployment_checklist:\n",
    "    print(f\"  {item}\")\n",
    "\n",
    "# ìµœì¢… ê¶Œì¥ì‚¬í•­ ìš”ì•½\n",
    "print(f\"\\\\nğŸ¯ ìµœì¢… ê¶Œì¥ì‚¬í•­:\")\n",
    "\n",
    "if y_test is not None:\n",
    "    if auc >= 0.8 and net_benefit > 0:\n",
    "        recommendation = \"ì¦‰ì‹œ ë°°í¬ ê¶Œì¥\"\n",
    "        urgency = \"ğŸŸ¢ HIGH\"\n",
    "    elif auc >= 0.7 and net_benefit > 0:\n",
    "        recommendation = \"ì¡°ê±´ë¶€ ë°°í¬ ê¶Œì¥ (A/B í…ŒìŠ¤íŠ¸ í›„)\"\n",
    "        urgency = \"ğŸŸ¡ MEDIUM\"\n",
    "    else:\n",
    "        recommendation = \"ëª¨ë¸ ê°œì„  í›„ ì¬í‰ê°€\"\n",
    "        urgency = \"ğŸ”´ LOW\"\n",
    "else:\n",
    "    recommendation = \"ì‹¤ì œ ë°ì´í„°ë¡œ ì¬í‰ê°€ í•„ìš”\"\n",
    "    urgency = \"âšª PENDING\"\n",
    "\n",
    "print(f\"  â€¢ ë°°í¬ ìš°ì„ ìˆœìœ„: {urgency}\")\n",
    "print(f\"  â€¢ ê¶Œì¥ì‚¬í•­: {recommendation}\")\n",
    "\n",
    "# ë‹¤ìŒ ë‹¨ê³„\n",
    "next_steps = [\n",
    "    \"1. ì‹¤ì œ í”„ë¡œë•ì…˜ ë°ì´í„°ë¡œ ëª¨ë¸ ì¬ê²€ì¦\",\n",
    "    \"2. A/B í…ŒìŠ¤íŠ¸ ì„¤ê³„ ë° ì‹¤í–‰ ê³„íš ìˆ˜ë¦½\",\n",
    "    \"3. ëª¨ë¸ ëª¨ë‹ˆí„°ë§ ë° ì•ŒëŒ ì‹œìŠ¤í…œ êµ¬ì¶•\",\n",
    "    \"4. ì´íƒˆ ë°©ì§€ ìº í˜ì¸ í”„ë¡œì„¸ìŠ¤ ì„¤ê³„\",\n",
    "    \"5. ì„±ê³¼ ì¸¡ì • KPI ë° ëŒ€ì‹œë³´ë“œ ê°œë°œ\"\n",
    "]\n",
    "\n",
    "print(f\"\\\\nğŸ“… ë‹¤ìŒ ë‹¨ê³„:\")\n",
    "for step in next_steps:\n",
    "    print(f\"  {step}\")\n",
    "\n",
    "# ì—°ë½ì²˜ ë° ë¬¸ì„œí™”\n",
    "print(f\"\\\\nğŸ“„ ê´€ë ¨ ë¬¸ì„œ ë° ë¦¬ì†ŒìŠ¤:\")\n",
    "print(f\"  â€¢ ëª¨ë¸ ì €ì¥ ìœ„ì¹˜: /results/models/\")\n",
    "print(f\"  â€¢ í•™ìŠµ ë…¸íŠ¸ë¶: 03_Modeling.ipynb\")\n",
    "print(f\"  â€¢ í”„ë¡œì íŠ¸ ë¬¸ì„œ: docs/\")\n",
    "print(f\"  â€¢ ê¸°ìˆ  ë¬¸ì˜: ë°ì´í„°íŒ€\")\n",
    "print(f\"  â€¢ ë¹„ì¦ˆë‹ˆìŠ¤ ë¬¸ì˜: ë§ˆì¼€íŒ…íŒ€\")\n",
    "\n",
    "print(f\"\\\\n\" + \"=\"*60)\n",
    "print(f\"ğŸ ê³ ê° ì´íƒˆ ì˜ˆì¸¡ ëª¨ë¸ í‰ê°€ ì™„ë£Œ!\")\n",
    "print(f\"ğŸ“Š í‰ê°€ ì™„ë£Œ ì‹œê°„: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print(f\"ğŸ’¡ ì¶”ê°€ ì§ˆë¬¸ì´ë‚˜ ê°œì„ ì‚¬í•­ì´ ìˆìœ¼ì‹œë©´ í”„ë¡œì íŠ¸ íŒ€ì— ë¬¸ì˜í•´ì£¼ì„¸ìš”.\")\n",
    "print(\"=\"*60)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

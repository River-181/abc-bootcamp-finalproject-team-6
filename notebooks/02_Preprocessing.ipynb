{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a6f2907b",
   "metadata": {},
   "source": [
    "# ê³ ê° ì´íƒˆ ì˜ˆì¸¡ - ë°ì´í„° ì „ì²˜ë¦¬\n",
    "\n",
    "ì´ ë…¸íŠ¸ë¶ì—ì„œëŠ” íƒìƒ‰ì  ë°ì´í„° ë¶„ì„(EDA) ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ ë¨¸ì‹ ëŸ¬ë‹ ëª¨ë¸ í•™ìŠµì„ ìœ„í•œ ë°ì´í„° ì „ì²˜ë¦¬ë¥¼ ìˆ˜í–‰í•©ë‹ˆë‹¤.\n",
    "\n",
    "## ì£¼ìš” ë‚´ìš©\n",
    "1. **ë°ì´í„° ë¡œë“œ ë° ê²€ì¦**: EDAì—ì„œ ë¶„ì„í•œ ë°ì´í„° ë¶ˆëŸ¬ì˜¤ê¸°\n",
    "2. **ë°ì´í„° ì •ì œ**: ê²°ì¸¡ì¹˜, ì´ìƒì¹˜, ì¤‘ë³µê°’ ì²˜ë¦¬\n",
    "3. **í”¼ì²˜ ì—”ì§€ë‹ˆì–´ë§**: ìƒˆë¡œìš´ ë³€ìˆ˜ ìƒì„± ë° ê¸°ì¡´ ë³€ìˆ˜ ë³€í™˜\n",
    "4. **ì¸ì½”ë”©**: ë²”ì£¼í˜• ë³€ìˆ˜ ì²˜ë¦¬\n",
    "5. **ìŠ¤ì¼€ì¼ë§**: ìˆ˜ì¹˜í˜• ë³€ìˆ˜ ì •ê·œí™”\n",
    "6. **ë°ì´í„° ë¶„í• **: í›ˆë ¨/ê²€ì¦/í…ŒìŠ¤íŠ¸ ì„¸íŠ¸ ë¶„ë¦¬\n",
    "7. **ì „ì²˜ë¦¬ëœ ë°ì´í„° ì €ì¥**: ëª¨ë¸ë§ì„ ìœ„í•œ ë°ì´í„° export\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e106412c",
   "metadata": {},
   "source": [
    "## 1. ë¼ì´ë¸ŒëŸ¬ë¦¬ ë° ì„¤ì •"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7d953f2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“š ì „ì²˜ë¦¬ ë¼ì´ë¸ŒëŸ¬ë¦¬ ë¡œë“œ ì™„ë£Œ!\n",
      "ğŸ• ì „ì²˜ë¦¬ ì‹œì‘ ì‹œê°„: 2025-08-03 18:53:31\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# ê¸°ë³¸ ë¼ì´ë¸ŒëŸ¬ë¦¬\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "from datetime import datetime\n",
    "import os\n",
    "\n",
    "# ì „ì²˜ë¦¬ ë¼ì´ë¸ŒëŸ¬ë¦¬\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder, OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer, KNNImputer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# ì¶”ê°€ ìœ í‹¸ë¦¬í‹°\n",
    "import joblib\n",
    "import pickle\n",
    "\n",
    "# ì„¤ì •\n",
    "warnings.filterwarnings('ignore')\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# í•œê¸€ í°íŠ¸ ì„¤ì •\n",
    "plt.rcParams['font.family'] = ['AppleGothic'] if os.name == 'posix' else ['Malgun Gothic']\n",
    "plt.rcParams['axes.unicode_minus'] = False\n",
    "\n",
    "# í‘œì‹œ ì˜µì…˜\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', None)\n",
    "\n",
    "# ëœë¤ ì‹œë“œ\n",
    "RANDOM_STATE = 42\n",
    "np.random.seed(RANDOM_STATE)\n",
    "\n",
    "print(\"ğŸ“š ì „ì²˜ë¦¬ ë¼ì´ë¸ŒëŸ¬ë¦¬ ë¡œë“œ ì™„ë£Œ!\")\n",
    "print(f\"ğŸ• ì „ì²˜ë¦¬ ì‹œì‘ ì‹œê°„: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32b95249",
   "metadata": {},
   "source": [
    "## 2. ë°ì´í„° ë¡œë“œ ë° ê²€ì¦\n",
    "\n",
    "EDAì—ì„œ ë¶„ì„í•œ ë°ì´í„°ë¥¼ ë¶ˆëŸ¬ì˜¤ê³  ê¸°ë³¸ì ì¸ ê²€ì¦ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f87725c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ë°ì´í„° íŒŒì¼ ê²½ë¡œ í™•ì¸\n",
    "raw_data_path = '../data/raw/customer_data.csv'\n",
    "processed_data_path = '../data/processed/'\n",
    "\n",
    "# í´ë” ìƒì„±\n",
    "os.makedirs(processed_data_path, exist_ok=True)\n",
    "\n",
    "# ë°ì´í„° ë¡œë“œ ì‹œë„\n",
    "if os.path.exists(raw_data_path):\n",
    "    print(f\"ğŸ“ ë°ì´í„° ë¡œë“œ: {raw_data_path}\")\n",
    "    df = pd.read_csv(raw_data_path)\n",
    "    print(\"âœ… ì‹¤ì œ ë°ì´í„° ë¡œë“œ ì„±ê³µ!\")\n",
    "else:\n",
    "    print(\"âš ï¸ ì‹¤ì œ ë°ì´í„° íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤. ìƒ˜í”Œ ë°ì´í„°ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.\")\n",
    "    \n",
    "    # ê³ ê° ì´íƒˆ ì˜ˆì¸¡ì„ ìœ„í•œ ìƒ˜í”Œ ë°ì´í„° ìƒì„±\n",
    "    np.random.seed(RANDOM_STATE)\n",
    "    n_samples = 5000\n",
    "    \n",
    "    # ê³ ê° ê¸°ë³¸ ì •ë³´\n",
    "    customer_data = {\n",
    "        'customer_id': [f'CUST_{i:05d}' for i in range(1, n_samples + 1)],\n",
    "        'age': np.random.normal(45, 15, n_samples).astype(int),\n",
    "        'gender': np.random.choice(['Male', 'Female'], n_samples, p=[0.52, 0.48]),\n",
    "        'tenure_months': np.random.exponential(24, n_samples).astype(int),\n",
    "        'monthly_charges': np.random.normal(65, 20, n_samples),\n",
    "        'total_charges': None,  # ê³„ì‚°ìœ¼ë¡œ ìƒì„±\n",
    "        'contract_type': np.random.choice(['Month-to-month', 'One year', 'Two year'], \n",
    "                                        n_samples, p=[0.55, 0.25, 0.20]),\n",
    "        'payment_method': np.random.choice(['Electronic check', 'Credit card', 'Bank transfer', 'Mailed check'],\n",
    "                                         n_samples, p=[0.35, 0.25, 0.25, 0.15]),\n",
    "        'internet_service': np.random.choice(['DSL', 'Fiber optic', 'No'], \n",
    "                                           n_samples, p=[0.35, 0.45, 0.20]),\n",
    "        'phone_service': np.random.choice(['Yes', 'No'], n_samples, p=[0.85, 0.15]),\n",
    "        'multiple_lines': np.random.choice(['Yes', 'No', 'No phone service'], \n",
    "                                         n_samples, p=[0.40, 0.45, 0.15]),\n",
    "        'online_security': np.random.choice(['Yes', 'No', 'No internet service'], \n",
    "                                          n_samples, p=[0.30, 0.50, 0.20]),\n",
    "        'online_backup': np.random.choice(['Yes', 'No', 'No internet service'], \n",
    "                                        n_samples, p=[0.35, 0.45, 0.20]),\n",
    "        'device_protection': np.random.choice(['Yes', 'No', 'No internet service'], \n",
    "                                            n_samples, p=[0.32, 0.48, 0.20]),\n",
    "        'tech_support': np.random.choice(['Yes', 'No', 'No internet service'], \n",
    "                                       n_samples, p=[0.28, 0.52, 0.20]),\n",
    "        'streaming_tv': np.random.choice(['Yes', 'No', 'No internet service'], \n",
    "                                       n_samples, p=[0.38, 0.42, 0.20]),\n",
    "        'streaming_movies': np.random.choice(['Yes', 'No', 'No internet service'], \n",
    "                                           n_samples, p=[0.36, 0.44, 0.20]),\n",
    "        'paperless_billing': np.random.choice(['Yes', 'No'], n_samples, p=[0.75, 0.25]),\n",
    "        'senior_citizen': np.random.choice([0, 1], n_samples, p=[0.84, 0.16])\n",
    "    }\n",
    "    \n",
    "    df = pd.DataFrame(customer_data)\n",
    "    \n",
    "    # total_charges ê³„ì‚° (tenure_months * monthly_charges + ë…¸ì´ì¦ˆ)\n",
    "    df['total_charges'] = (df['tenure_months'] * df['monthly_charges'] + \n",
    "                          np.random.normal(0, 100, n_samples)).round(2)\n",
    "    df['total_charges'] = np.maximum(df['total_charges'], df['monthly_charges'])  # ìµœì†Œê°’ ë³´ì •\n",
    "    \n",
    "    # ì´íƒˆ ì—¬ë¶€ ìƒì„± (ë‹¤ì–‘í•œ ìš”ì¸ì„ ê³ ë ¤í•œ í™•ë¥ ì  ìƒì„±)\n",
    "    churn_prob = 0.15  # ê¸°ë³¸ ì´íƒˆ í™•ë¥ \n",
    "    \n",
    "    # ê³„ì•½ ìœ í˜•ì— ë”°ë¥¸ ì´íƒˆ í™•ë¥  ì¡°ì •\n",
    "    contract_multiplier = df['contract_type'].map({\n",
    "        'Month-to-month': 2.5,\n",
    "        'One year': 1.0,\n",
    "        'Two year': 0.3\n",
    "    })\n",
    "    \n",
    "    # ê¸°íƒ€ ìš”ì¸ë“¤\n",
    "    age_factor = np.where(df['age'] > 65, 0.8, 1.0)  # ê³ ë ¹ì ì´íƒˆ ë‚®ìŒ\n",
    "    tenure_factor = np.where(df['tenure_months'] < 6, 2.0, \n",
    "                           np.where(df['tenure_months'] > 36, 0.5, 1.0))\n",
    "    charges_factor = np.where(df['monthly_charges'] > 80, 1.5, 0.8)\n",
    "    \n",
    "    # ìµœì¢… ì´íƒˆ í™•ë¥ \n",
    "    final_churn_prob = (churn_prob * contract_multiplier * age_factor * \n",
    "                       tenure_factor * charges_factor)\n",
    "    final_churn_prob = np.clip(final_churn_prob, 0, 0.8)  # ìµœëŒ€ 80% ì œí•œ\n",
    "    \n",
    "    df['churn'] = np.random.binomial(1, final_churn_prob)\n",
    "    \n",
    "    # ì¼ë¶€ ê²°ì¸¡ì¹˜ ì˜ë„ì  ìƒì„±\n",
    "    missing_indices = np.random.choice(n_samples, int(n_samples * 0.02), replace=False)\n",
    "    df.loc[missing_indices, 'total_charges'] = np.nan\n",
    "    \n",
    "    print(\"âœ… ìƒ˜í”Œ ë°ì´í„° ìƒì„± ì™„ë£Œ!\")\n",
    "\n",
    "# ë°ì´í„° ê¸°ë³¸ ì •ë³´ ì¶œë ¥\n",
    "print(f\"\\nğŸ“Š ë°ì´í„° ê¸°ë³¸ ì •ë³´:\")\n",
    "print(f\"  â€¢ ì „ì²´ ìƒ˜í”Œ ìˆ˜: {len(df):,}\")\n",
    "print(f\"  â€¢ ì „ì²´ í”¼ì²˜ ìˆ˜: {df.shape[1]}\")\n",
    "print(f\"  â€¢ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰: {df.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n",
    "\n",
    "# íƒ€ê²Ÿ ë³€ìˆ˜ ë¶„í¬\n",
    "if 'churn' in df.columns:\n",
    "    churn_rate = df['churn'].mean()\n",
    "    print(f\"  â€¢ ì´íƒˆë¥ : {churn_rate:.2%}\")\n",
    "    print(f\"  â€¢ ì´íƒˆ ê³ ê° ìˆ˜: {df['churn'].sum():,}\")\n",
    "    print(f\"  â€¢ ìœ ì§€ ê³ ê° ìˆ˜: {(df['churn'] == 0).sum():,}\")\n",
    "\n",
    "print(f\"\\nğŸ“‹ ë°ì´í„° íƒ€ì… ì •ë³´:\")\n",
    "print(df.dtypes.value_counts())\n",
    "\n",
    "print(f\"\\nğŸ” ë°ì´í„° ë¯¸ë¦¬ë³´ê¸°:\")\n",
    "display(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77335aaf",
   "metadata": {},
   "source": [
    "## 3. ë°ì´í„° ì •ì œ\n",
    "\n",
    "ê²°ì¸¡ì¹˜, ì´ìƒì¹˜, ì¤‘ë³µê°’ì„ ì²˜ë¦¬í•˜ê³  ë°ì´í„° í’ˆì§ˆì„ ê°œì„ í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f942733",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ë°ì´í„° ì •ì œ ì „ ë°±ì—…\n",
    "df_original = df.copy()\n",
    "\n",
    "print(\"ğŸ§¹ ë°ì´í„° ì •ì œ ì‹œì‘\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# 1. ê²°ì¸¡ì¹˜ ë¶„ì„\n",
    "print(\"ğŸ“Š ê²°ì¸¡ì¹˜ ë¶„ì„:\")\n",
    "missing_info = df.isnull().sum()\n",
    "missing_percent = (missing_info / len(df) * 100).round(2)\n",
    "missing_df = pd.DataFrame({\n",
    "    'Missing_Count': missing_info,\n",
    "    'Missing_Percent': missing_percent\n",
    "}).sort_values('Missing_Count', ascending=False)\n",
    "\n",
    "# ê²°ì¸¡ì¹˜ê°€ ìˆëŠ” ì»¬ëŸ¼ë§Œ í‘œì‹œ\n",
    "missing_cols = missing_df[missing_df['Missing_Count'] > 0]\n",
    "if len(missing_cols) > 0:\n",
    "    print(\"âŒ ê²°ì¸¡ì¹˜ê°€ ìˆëŠ” ì»¬ëŸ¼:\")\n",
    "    display(missing_cols)\n",
    "else:\n",
    "    print(\"âœ… ê²°ì¸¡ì¹˜ ì—†ìŒ\")\n",
    "\n",
    "# 2. ì¤‘ë³µ ë°ì´í„° í™•ì¸\n",
    "duplicates = df.duplicated().sum()\n",
    "print(f\"\\nğŸ” ì¤‘ë³µ ë°ì´í„°: {duplicates}ê°œ\")\n",
    "\n",
    "if duplicates > 0:\n",
    "    print(\"âŒ ì¤‘ë³µ ë°ì´í„° ì œê±° ì¤‘...\")\n",
    "    df = df.drop_duplicates()\n",
    "    print(f\"âœ… {duplicates}ê°œ ì¤‘ë³µ ë°ì´í„° ì œê±° ì™„ë£Œ\")\n",
    "\n",
    "# 3. ê²°ì¸¡ì¹˜ ì²˜ë¦¬\n",
    "if len(missing_cols) > 0:\n",
    "    print(f\"\\nğŸ”§ ê²°ì¸¡ì¹˜ ì²˜ë¦¬ ì‹œì‘...\")\n",
    "    \n",
    "    for col in missing_cols.index:\n",
    "        missing_count = missing_cols.loc[col, 'Missing_Count']\n",
    "        missing_pct = missing_cols.loc[col, 'Missing_Percent']\n",
    "        \n",
    "        print(f\"\\n  ğŸ“‹ {col} ì»¬ëŸ¼ ì²˜ë¦¬ ({missing_count}ê°œ, {missing_pct}%)\")\n",
    "        \n",
    "        if df[col].dtype in ['int64', 'float64']:  # ìˆ˜ì¹˜í˜• ë°ì´í„°\n",
    "            if missing_pct < 5:  # 5% ë¯¸ë§Œì´ë©´ ì¤‘ì•™ê°’ìœ¼ë¡œ ëŒ€ì²´\n",
    "                median_value = df[col].median()\n",
    "                df[col].fillna(median_value, inplace=True)\n",
    "                print(f\"    âœ… ì¤‘ì•™ê°’({median_value:.2f})ìœ¼ë¡œ ëŒ€ì²´\")\n",
    "            else:  # 5% ì´ìƒì´ë©´ KNN ëŒ€ì²´\n",
    "                # ìˆ˜ì¹˜í˜• ì»¬ëŸ¼ë“¤ë§Œ ì„ íƒí•´ì„œ KNN ì ìš©\n",
    "                numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "                if col in numeric_cols:\n",
    "                    numeric_cols.remove(col)\n",
    "                \n",
    "                if len(numeric_cols) > 0:\n",
    "                    imputer = KNNImputer(n_neighbors=5)\n",
    "                    df[col] = imputer.fit_transform(df[[col] + numeric_cols[:3]])[:, 0]\n",
    "                    print(f\"    âœ… KNN ëŒ€ì²´ ì™„ë£Œ\")\n",
    "                else:\n",
    "                    median_value = df[col].median()\n",
    "                    df[col].fillna(median_value, inplace=True)\n",
    "                    print(f\"    âœ… ì¤‘ì•™ê°’({median_value:.2f})ìœ¼ë¡œ ëŒ€ì²´\")\n",
    "        else:  # ë²”ì£¼í˜• ë°ì´í„°\n",
    "            mode_value = df[col].mode().iloc[0] if len(df[col].mode()) > 0 else 'Unknown'\n",
    "            df[col].fillna(mode_value, inplace=True)\n",
    "            print(f\"    âœ… ìµœë¹ˆê°’('{mode_value}')ìœ¼ë¡œ ëŒ€ì²´\")\n",
    "\n",
    "# 4. ë°ì´í„° íƒ€ì… ìµœì í™”\n",
    "print(f\"\\nğŸ¯ ë°ì´í„° íƒ€ì… ìµœì í™”:\")\n",
    "\n",
    "# ì •ìˆ˜í˜• ìµœì í™”\n",
    "for col in df.select_dtypes(include=['int64']).columns:\n",
    "    col_min, col_max = df[col].min(), df[col].max()\n",
    "    if col_min >= 0:  # ì–‘ìˆ˜ì¸ ê²½ìš°\n",
    "        if col_max < 255:\n",
    "            df[col] = df[col].astype('uint8')\n",
    "        elif col_max < 65535:\n",
    "            df[col] = df[col].astype('uint16')\n",
    "        elif col_max < 4294967295:\n",
    "            df[col] = df[col].astype('uint32')\n",
    "    else:  # ìŒìˆ˜ê°€ ìˆëŠ” ê²½ìš°\n",
    "        if col_min > -128 and col_max < 127:\n",
    "            df[col] = df[col].astype('int8')\n",
    "        elif col_min > -32768 and col_max < 32767:\n",
    "            df[col] = df[col].astype('int16')\n",
    "        elif col_min > -2147483648 and col_max < 2147483647:\n",
    "            df[col] = df[col].astype('int32')\n",
    "\n",
    "# ì‹¤ìˆ˜í˜• ìµœì í™”\n",
    "for col in df.select_dtypes(include=['float64']).columns:\n",
    "    df[col] = pd.to_numeric(df[col], downcast='float')\n",
    "\n",
    "# 5. ì´ìƒì¹˜ íƒì§€ ë° ì²˜ë¦¬\n",
    "print(f\"\\nğŸ” ì´ìƒì¹˜ íƒì§€ (ìˆ˜ì¹˜í˜• ë³€ìˆ˜):\")\n",
    "\n",
    "numeric_columns = df.select_dtypes(include=[np.number]).columns\n",
    "outlier_summary = {}\n",
    "\n",
    "for col in numeric_columns:\n",
    "    Q1 = df[col].quantile(0.25)\n",
    "    Q3 = df[col].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "    \n",
    "    outliers = ((df[col] < lower_bound) | (df[col] > upper_bound)).sum()\n",
    "    outlier_pct = (outliers / len(df) * 100).round(2)\n",
    "    \n",
    "    outlier_summary[col] = {\n",
    "        'count': outliers,\n",
    "        'percentage': outlier_pct,\n",
    "        'lower_bound': lower_bound,\n",
    "        'upper_bound': upper_bound\n",
    "    }\n",
    "    \n",
    "    if outliers > 0:\n",
    "        print(f\"  ğŸ“Š {col}: {outliers}ê°œ ({outlier_pct}%)\")\n",
    "        \n",
    "        # ì´ìƒì¹˜ê°€ 5% ë¯¸ë§Œì´ë©´ ìƒí•œ/í•˜í•œì„ ìœ¼ë¡œ í´ë¦¬í•‘\n",
    "        if outlier_pct < 5:\n",
    "            df[col] = df[col].clip(lower=lower_bound, upper=upper_bound)\n",
    "            print(f\"      âœ… ìƒí•œ/í•˜í•œì„ ìœ¼ë¡œ í´ë¦¬í•‘ ì ìš©\")\n",
    "\n",
    "# 6. ì •ì œ ê²°ê³¼ ìš”ì•½\n",
    "print(f\"\\nğŸ“‹ ë°ì´í„° ì •ì œ ê²°ê³¼:\")\n",
    "print(f\"  â€¢ ì²˜ë¦¬ ì „ ìƒ˜í”Œ ìˆ˜: {len(df_original):,}\")\n",
    "print(f\"  â€¢ ì²˜ë¦¬ í›„ ìƒ˜í”Œ ìˆ˜: {len(df):,}\")\n",
    "print(f\"  â€¢ ì œê±°ëœ ìƒ˜í”Œ ìˆ˜: {len(df_original) - len(df):,}\")\n",
    "\n",
    "# ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ë¹„êµ\n",
    "memory_before = df_original.memory_usage(deep=True).sum() / 1024**2\n",
    "memory_after = df.memory_usage(deep=True).sum() / 1024**2\n",
    "memory_reduction = ((memory_before - memory_after) / memory_before * 100)\n",
    "\n",
    "print(f\"  â€¢ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ (ì „): {memory_before:.2f} MB\")\n",
    "print(f\"  â€¢ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ (í›„): {memory_after:.2f} MB\")\n",
    "print(f\"  â€¢ ë©”ëª¨ë¦¬ ì ˆì•½: {memory_reduction:.1f}%\")\n",
    "\n",
    "# ìµœì¢… ê²°ì¸¡ì¹˜ í™•ì¸\n",
    "final_missing = df.isnull().sum().sum()\n",
    "print(f\"  â€¢ ë‚¨ì€ ê²°ì¸¡ì¹˜: {final_missing}ê°œ\")\n",
    "\n",
    "if final_missing == 0:\n",
    "    print(\"âœ… ëª¨ë“  ê²°ì¸¡ì¹˜ ì²˜ë¦¬ ì™„ë£Œ!\")\n",
    "else:\n",
    "    print(\"âš ï¸ ì¼ë¶€ ê²°ì¸¡ì¹˜ê°€ ë‚¨ì•„ìˆìŠµë‹ˆë‹¤.\")\n",
    "\n",
    "print(f\"\\nğŸ” ì •ì œëœ ë°ì´í„° ë¯¸ë¦¬ë³´ê¸°:\")\n",
    "display(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07ffe59e",
   "metadata": {},
   "source": [
    "## 4. í”¼ì²˜ ì—”ì§€ë‹ˆì–´ë§\n",
    "\n",
    "ê¸°ì¡´ ë³€ìˆ˜ë¥¼ í™œìš©í•˜ì—¬ ìƒˆë¡œìš´ ì˜ë¯¸ ìˆëŠ” ë³€ìˆ˜ë¥¼ ìƒì„±í•˜ê³  ê¸°ì¡´ ë³€ìˆ˜ë¥¼ ë³€í™˜í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e65af08",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ğŸ”§ í”¼ì²˜ ì—”ì§€ë‹ˆì–´ë§ ì‹œì‘\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# 1. ìƒˆë¡œìš´ ìˆ˜ì¹˜í˜• í”¼ì²˜ ìƒì„±\n",
    "print(\"ğŸ“Š ìˆ˜ì¹˜í˜• í”¼ì²˜ ìƒì„±:\")\n",
    "\n",
    "# ê³ ê° ê°€ì¹˜ ê´€ë ¨ í”¼ì²˜\n",
    "if 'monthly_charges' in df.columns and 'tenure_months' in df.columns:\n",
    "    # ì›” í‰ê·  ì‚¬ìš©ë£Œ ëŒ€ë¹„ ì´ ì‚¬ìš©ë£Œ ë¹„ìœ¨\n",
    "    df['charges_per_month'] = df['total_charges'] / (df['tenure_months'] + 1)  # +1ë¡œ 0 ë‚˜ëˆ„ê¸° ë°©ì§€\n",
    "    print(\"  âœ… charges_per_month: ì›”í‰ê·  ìš”ê¸ˆ\")\n",
    "    \n",
    "    # ê³ ê° ìƒì•  ê°€ì¹˜ (Customer Lifetime Value ì¶”ì •)\n",
    "    df['estimated_clv'] = df['monthly_charges'] * df['tenure_months'] * 1.2  # 20% ë§ˆì§„ ê³ ë ¤\n",
    "    print(\"  âœ… estimated_clv: ì¶”ì • ê³ ê° ìƒì•  ê°€ì¹˜\")\n",
    "\n",
    "# ê³ ê° ì„¸ê·¸ë¨¼íŠ¸ ê´€ë ¨ í”¼ì²˜\n",
    "if 'age' in df.columns:\n",
    "    # ì—°ë ¹ëŒ€ ê·¸ë£¹\n",
    "    df['age_group'] = pd.cut(df['age'], \n",
    "                            bins=[0, 25, 35, 45, 55, 65, 100], \n",
    "                            labels=['Under_25', '25-34', '35-44', '45-54', '55-64', 'Over_65'])\n",
    "    print(\"  âœ… age_group: ì—°ë ¹ëŒ€ ê·¸ë£¹\")\n",
    "\n",
    "if 'tenure_months' in df.columns:\n",
    "    # ê³ ê° ì¶©ì„±ë„ ì„¸ê·¸ë¨¼íŠ¸\n",
    "    df['loyalty_segment'] = pd.cut(df['tenure_months'],\n",
    "                                  bins=[0, 6, 12, 24, 60, 1000],\n",
    "                                  labels=['New', 'Growing', 'Established', 'Loyal', 'Champion'])\n",
    "    print(\"  âœ… loyalty_segment: ê³ ê° ì¶©ì„±ë„ ì„¸ê·¸ë¨¼íŠ¸\")\n",
    "\n",
    "# 2. ì„œë¹„ìŠ¤ ê´€ë ¨ í”¼ì²˜ ì—”ì§€ë‹ˆì–´ë§\n",
    "print(f\"\\nğŸ› ï¸ ì„œë¹„ìŠ¤ ê´€ë ¨ í”¼ì²˜:\")\n",
    "\n",
    "# ë¶€ê°€ ì„œë¹„ìŠ¤ ê°œìˆ˜ ì¹´ìš´íŠ¸\n",
    "service_cols = [col for col in df.columns if any(service in col.lower() \n",
    "                for service in ['security', 'backup', 'protection', 'support', \n",
    "                              'streaming', 'lines'])]\n",
    "\n",
    "if service_cols:\n",
    "    # ë¶€ê°€ ì„œë¹„ìŠ¤ ì¤‘ 'Yes'ì¸ ê²ƒì˜ ê°œìˆ˜\n",
    "    def count_services(row):\n",
    "        return sum(1 for col in service_cols if row[col] == 'Yes')\n",
    "    \n",
    "    df['total_services'] = df.apply(count_services, axis=1)\n",
    "    print(f\"  âœ… total_services: ì´ìš© ì¤‘ì¸ ë¶€ê°€ ì„œë¹„ìŠ¤ ìˆ˜ ({len(service_cols)}ê°œ ì¤‘)\")\n",
    "\n",
    "# ì¸í„°ë„· ì„œë¹„ìŠ¤ í’ˆì§ˆ ì ìˆ˜\n",
    "if 'internet_service' in df.columns:\n",
    "    internet_score_map = {'Fiber optic': 3, 'DSL': 2, 'No': 0}\n",
    "    df['internet_score'] = df['internet_service'].map(internet_score_map)\n",
    "    print(\"  âœ… internet_score: ì¸í„°ë„· ì„œë¹„ìŠ¤ í’ˆì§ˆ ì ìˆ˜\")\n",
    "\n",
    "# 3. ê³„ì•½ ê´€ë ¨ í”¼ì²˜\n",
    "print(f\"\\nğŸ“‹ ê³„ì•½ ê´€ë ¨ í”¼ì²˜:\")\n",
    "\n",
    "if 'contract_type' in df.columns:\n",
    "    # ê³„ì•½ ì•ˆì •ì„± ì ìˆ˜\n",
    "    contract_stability = {'Month-to-month': 1, 'One year': 2, 'Two year': 3}\n",
    "    df['contract_stability'] = df['contract_type'].map(contract_stability)\n",
    "    print(\"  âœ… contract_stability: ê³„ì•½ ì•ˆì •ì„± ì ìˆ˜\")\n",
    "\n",
    "if 'payment_method' in df.columns:\n",
    "    # ìë™ ê²°ì œ ì—¬ë¶€\n",
    "    auto_payment = ['Credit card', 'Bank transfer']\n",
    "    df['auto_payment'] = df['payment_method'].apply(lambda x: 1 if x in auto_payment else 0)\n",
    "    print(\"  âœ… auto_payment: ìë™ ê²°ì œ ì—¬ë¶€\")\n",
    "\n",
    "# 4. ê³ ê° ë¦¬ìŠ¤í¬ ì ìˆ˜ ê³„ì‚°\n",
    "print(f\"\\nâš ï¸ ë¦¬ìŠ¤í¬ ì ìˆ˜ ê³„ì‚°:\")\n",
    "\n",
    "risk_factors = []\n",
    "\n",
    "# ê³„ì•½ ìœ í˜• ë¦¬ìŠ¤í¬ (ì›” ë‹¨ìœ„ ê³„ì•½ì´ ê°€ì¥ ìœ„í—˜)\n",
    "if 'contract_type' in df.columns:\n",
    "    contract_risk = df['contract_type'].map({'Month-to-month': 3, 'One year': 2, 'Two year': 1})\n",
    "    risk_factors.append(contract_risk)\n",
    "\n",
    "# ì‹ ê·œ ê³ ê° ë¦¬ìŠ¤í¬ (tenureê°€ ì§§ì„ìˆ˜ë¡ ìœ„í—˜)\n",
    "if 'tenure_months' in df.columns:\n",
    "    tenure_risk = np.where(df['tenure_months'] < 6, 3,\n",
    "                          np.where(df['tenure_months'] < 12, 2, 1))\n",
    "    risk_factors.append(pd.Series(tenure_risk, index=df.index))\n",
    "\n",
    "# ê³ ìš”ê¸ˆ ê³ ê° ë¦¬ìŠ¤í¬ (ì›” ìš”ê¸ˆì´ ë†’ì„ìˆ˜ë¡ ì´íƒˆ ê°€ëŠ¥ì„± ì¦ê°€)\n",
    "if 'monthly_charges' in df.columns:\n",
    "    charges_percentile = df['monthly_charges'].rank(pct=True)\n",
    "    charges_risk = np.where(charges_percentile > 0.8, 3,\n",
    "                           np.where(charges_percentile > 0.6, 2, 1))\n",
    "    risk_factors.append(pd.Series(charges_risk, index=df.index))\n",
    "\n",
    "# ìˆ˜ë™ ê²°ì œ ë¦¬ìŠ¤í¬\n",
    "if 'auto_payment' in df.columns:\n",
    "    payment_risk = np.where(df['auto_payment'] == 0, 2, 1)\n",
    "    risk_factors.append(pd.Series(payment_risk, index=df.index))\n",
    "\n",
    "# ì¢…í•© ë¦¬ìŠ¤í¬ ì ìˆ˜\n",
    "if risk_factors:\n",
    "    df['risk_score'] = sum(risk_factors)\n",
    "    df['risk_level'] = pd.cut(df['risk_score'], \n",
    "                             bins=[0, 4, 7, 10, 20], \n",
    "                             labels=['Low', 'Medium', 'High', 'Critical'])\n",
    "    print(f\"  âœ… risk_score: ì¢…í•© ë¦¬ìŠ¤í¬ ì ìˆ˜ (4-{df['risk_score'].max()})\")\n",
    "    print(f\"  âœ… risk_level: ë¦¬ìŠ¤í¬ ë ˆë²¨ (Low/Medium/High/Critical)\")\n",
    "\n",
    "# 5. ìƒí˜¸ì‘ìš© í”¼ì²˜ (ì¤‘ìš”í•œ ë³€ìˆ˜ë“¤ ê°„ì˜ ì¡°í•©)\n",
    "print(f\"\\nğŸ”— ìƒí˜¸ì‘ìš© í”¼ì²˜:\")\n",
    "\n",
    "if 'monthly_charges' in df.columns and 'tenure_months' in df.columns:\n",
    "    # ìš”ê¸ˆ ëŒ€ë¹„ ì¶©ì„±ë„\n",
    "    df['charges_tenure_ratio'] = df['monthly_charges'] / (df['tenure_months'] + 1)\n",
    "    print(\"  âœ… charges_tenure_ratio: ìš”ê¸ˆ ëŒ€ë¹„ ì¶©ì„±ë„\")\n",
    "\n",
    "if 'total_services' in df.columns and 'monthly_charges' in df.columns:\n",
    "    # ì„œë¹„ìŠ¤ë‹¹ í‰ê·  ìš”ê¸ˆ\n",
    "    df['charges_per_service'] = df['monthly_charges'] / (df['total_services'] + 1)\n",
    "    print(\"  âœ… charges_per_service: ì„œë¹„ìŠ¤ë‹¹ í‰ê·  ìš”ê¸ˆ\")\n",
    "\n",
    "# 6. ë¡œê·¸ ë³€í™˜ (ì¹˜ìš°ì¹œ ë¶„í¬ì˜ ìˆ˜ì¹˜í˜• ë³€ìˆ˜)\n",
    "print(f\"\\nğŸ“ˆ ë¡œê·¸ ë³€í™˜:\")\n",
    "\n",
    "log_transform_cols = []\n",
    "for col in ['monthly_charges', 'total_charges', 'tenure_months']:\n",
    "    if col in df.columns:\n",
    "        # ì¹˜ìš°ì¹¨ ì •ë„ í™•ì¸ (ì™œë„)\n",
    "        skewness = df[col].skew()\n",
    "        if abs(skewness) > 1:  # ì¹˜ìš°ì¹¨ì´ ì‹¬í•œ ê²½ìš°\n",
    "            df[f'{col}_log'] = np.log1p(df[col])  # log1pëŠ” log(1+x)ë¡œ 0ê°’ ì²˜ë¦¬\n",
    "            log_transform_cols.append(col)\n",
    "            print(f\"  âœ… {col}_log: {col}ì˜ ë¡œê·¸ ë³€í™˜ (ì›ë³¸ ì™œë„: {skewness:.2f})\")\n",
    "\n",
    "# 7. ìƒì„±ëœ í”¼ì²˜ ìš”ì•½\n",
    "print(f\"\\nğŸ“Š í”¼ì²˜ ì—”ì§€ë‹ˆì–´ë§ ê²°ê³¼:\")\n",
    "new_features = [col for col in df.columns if col not in df_original.columns]\n",
    "print(f\"  â€¢ ìƒˆë¡œ ìƒì„±ëœ í”¼ì²˜ ìˆ˜: {len(new_features)}\")\n",
    "print(f\"  â€¢ ì „ì²´ í”¼ì²˜ ìˆ˜: {df.shape[1]} (ê¸°ì¡´ {df_original.shape[1]}ê°œ + ì‹ ê·œ {len(new_features)}ê°œ)\")\n",
    "\n",
    "if new_features:\n",
    "    print(f\"  â€¢ ìƒˆë¡œìš´ í”¼ì²˜ ëª©ë¡:\")\n",
    "    for i, feature in enumerate(new_features, 1):\n",
    "        print(f\"    {i:2d}. {feature}\")\n",
    "\n",
    "# ë°ì´í„° íƒ€ì…ë³„ í”¼ì²˜ ë¶„í¬\n",
    "print(f\"\\nğŸ“‹ í”¼ì²˜ íƒ€ì… ë¶„í¬:\")\n",
    "dtype_counts = df.dtypes.value_counts()\n",
    "for dtype, count in dtype_counts.items():\n",
    "    print(f\"  â€¢ {dtype}: {count}ê°œ\")\n",
    "\n",
    "print(f\"\\nâœ… í”¼ì²˜ ì—”ì§€ë‹ˆì–´ë§ ì™„ë£Œ!\")\n",
    "print(f\"ğŸ” ì—…ë°ì´íŠ¸ëœ ë°ì´í„° ë¯¸ë¦¬ë³´ê¸°:\")\n",
    "display(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f92e7d3",
   "metadata": {},
   "source": [
    "## 5. ì¸ì½”ë”©\n",
    "\n",
    "ë²”ì£¼í˜• ë³€ìˆ˜ë¥¼ ë¨¸ì‹ ëŸ¬ë‹ ëª¨ë¸ì´ ì²˜ë¦¬í•  ìˆ˜ ìˆëŠ” ìˆ˜ì¹˜í˜•ìœ¼ë¡œ ë³€í™˜í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4e64449",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ğŸ”¤ ë²”ì£¼í˜• ë³€ìˆ˜ ì¸ì½”ë”© ì‹œì‘\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# ì¸ì½”ë”© ì „ ë°ì´í„° ë°±ì—…\n",
    "df_before_encoding = df.copy()\n",
    "\n",
    "# 1. ë²”ì£¼í˜• ë³€ìˆ˜ ì‹ë³„\n",
    "categorical_columns = df.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "print(f\"ğŸ“‹ ë²”ì£¼í˜• ë³€ìˆ˜ ({len(categorical_columns)}ê°œ):\")\n",
    "for i, col in enumerate(categorical_columns, 1):\n",
    "    unique_values = df[col].nunique()\n",
    "    print(f\"  {i:2d}. {col}: {unique_values}ê°œ ê³ ìœ ê°’\")\n",
    "\n",
    "# customer_idëŠ” ì œì™¸ (ì‹ë³„ìì´ë¯€ë¡œ)\n",
    "if 'customer_id' in categorical_columns:\n",
    "    categorical_columns.remove('customer_id')\n",
    "    print(f\"  â„¹ï¸ customer_idëŠ” ì¸ì½”ë”©ì—ì„œ ì œì™¸\")\n",
    "\n",
    "# 2. ì´ì§„ ì¸ì½”ë”© (Binary/Boolean ë³€ìˆ˜ë“¤)\n",
    "print(f\"\\nğŸ”¢ ì´ì§„ ì¸ì½”ë”©:\")\n",
    "binary_columns = []\n",
    "\n",
    "for col in categorical_columns:\n",
    "    unique_vals = df[col].unique()\n",
    "    if len(unique_vals) == 2:\n",
    "        binary_columns.append(col)\n",
    "        # Yes/No, Male/Female ë“±ì„ 0/1ë¡œ ë³€í™˜\n",
    "        if 'Yes' in unique_vals and 'No' in unique_vals:\n",
    "            df[col] = df[col].map({'Yes': 1, 'No': 0})\n",
    "        elif 'Male' in unique_vals and 'Female' in unique_vals:\n",
    "            df[col] = df[col].map({'Male': 1, 'Female': 0})\n",
    "        else:\n",
    "            # ê¸°íƒ€ ì´ì§„ ë³€ìˆ˜ëŠ” LabelEncoder ì‚¬ìš©\n",
    "            le = LabelEncoder()\n",
    "            df[col] = le.fit_transform(df[col])\n",
    "        \n",
    "        print(f\"  âœ… {col}: {unique_vals} â†’ 0/1\")\n",
    "\n",
    "# 3. ì„œìˆ˜ ì¸ì½”ë”© (Ordinal Encoding) - ìˆœì„œê°€ ìˆëŠ” ë²”ì£¼í˜• ë³€ìˆ˜\n",
    "print(f\"\\nğŸ“Š ì„œìˆ˜ ì¸ì½”ë”©:\")\n",
    "\n",
    "ordinal_mappings = {}\n",
    "\n",
    "# ê³„ì•½ ì•ˆì •ì„± (ì´ë¯¸ ì²˜ë¦¬ë¨)\n",
    "if 'contract_type' in df.columns and 'contract_stability' not in df.columns:\n",
    "    contract_order = {'Month-to-month': 1, 'One year': 2, 'Two year': 3}\n",
    "    df['contract_stability'] = df['contract_type'].map(contract_order)\n",
    "    ordinal_mappings['contract_type'] = contract_order\n",
    "    print(f\"  âœ… contract_type â†’ contract_stability: {contract_order}\")\n",
    "\n",
    "# ì¸í„°ë„· ì„œë¹„ìŠ¤ í’ˆì§ˆ (ì´ë¯¸ ì²˜ë¦¬ë¨)\n",
    "if 'internet_service' in df.columns and 'internet_score' not in df.columns:\n",
    "    internet_order = {'No': 0, 'DSL': 1, 'Fiber optic': 2}\n",
    "    df['internet_score'] = df['internet_service'].map(internet_order)\n",
    "    ordinal_mappings['internet_service'] = internet_order\n",
    "    print(f\"  âœ… internet_service â†’ internet_score: {internet_order}\")\n",
    "\n",
    "# 4. ì›-í•« ì¸ì½”ë”© (Nominal ë³€ìˆ˜ë“¤)\n",
    "print(f\"\\nğŸ¯ ì›-í•« ì¸ì½”ë”©:\")\n",
    "\n",
    "# ì›-í•« ì¸ì½”ë”©í•  ë³€ìˆ˜ë“¤ (ì¹´ë””ë„ë¦¬í‹°ê°€ ë‚®ì€ ëª…ëª©í˜• ë³€ìˆ˜)\n",
    "onehot_columns = []\n",
    "for col in categorical_columns:\n",
    "    if col not in binary_columns:\n",
    "        unique_count = df[col].nunique()\n",
    "        if unique_count <= 10:  # 10ê°œ ì´í•˜ì˜ ì¹´í…Œê³ ë¦¬ë§Œ ì›-í•« ì¸ì½”ë”©\n",
    "            onehot_columns.append(col)\n",
    "\n",
    "print(f\"  ğŸ“‹ ì›-í•« ì¸ì½”ë”© ëŒ€ìƒ ({len(onehot_columns)}ê°œ):\")\n",
    "for col in onehot_columns:\n",
    "    print(f\"    â€¢ {col}: {df[col].nunique()}ê°œ ì¹´í…Œê³ ë¦¬\")\n",
    "\n",
    "# ì›-í•« ì¸ì½”ë”© ìˆ˜í–‰\n",
    "if onehot_columns:\n",
    "    # pandas get_dummies ì‚¬ìš©\n",
    "    df_encoded = pd.get_dummies(df, columns=onehot_columns, prefix=onehot_columns, \n",
    "                               prefix_sep='_', drop_first=False)\n",
    "    \n",
    "    # ì›ë˜ ì»¬ëŸ¼ë“¤ ì œê±°\n",
    "    df_encoded = df_encoded.drop(columns=onehot_columns)\n",
    "    \n",
    "    print(f\"  âœ… ì›-í•« ì¸ì½”ë”© ì™„ë£Œ\")\n",
    "    print(f\"    - ìƒì„±ëœ ë”ë¯¸ ë³€ìˆ˜: {len(df_encoded.columns) - len(df.columns)}ê°œ\")\n",
    "    \n",
    "    df = df_encoded.copy()\n",
    "else:\n",
    "    print(f\"  â„¹ï¸ ì›-í•« ì¸ì½”ë”©í•  ë³€ìˆ˜ê°€ ì—†ìŠµë‹ˆë‹¤.\")\n",
    "\n",
    "# 5. ê³ ì¹´ë””ë„ë¦¬í‹° ë³€ìˆ˜ ì²˜ë¦¬ (ì¹´í…Œê³ ë¦¬ê°€ ë§ì€ ë³€ìˆ˜)\n",
    "print(f\"\\nğŸ” ê³ ì¹´ë””ë„ë¦¬í‹° ë³€ìˆ˜ ì²˜ë¦¬:\")\n",
    "\n",
    "high_cardinality_cols = []\n",
    "for col in categorical_columns:\n",
    "    if col not in binary_columns and col not in onehot_columns:\n",
    "        unique_count = df[col].nunique()\n",
    "        if unique_count > 10:\n",
    "            high_cardinality_cols.append(col)\n",
    "\n",
    "if high_cardinality_cols:\n",
    "    print(f\"  ğŸ“‹ ê³ ì¹´ë””ë„ë¦¬í‹° ë³€ìˆ˜ ({len(high_cardinality_cols)}ê°œ):\")\n",
    "    for col in high_cardinality_cols:\n",
    "        unique_count = df[col].nunique()\n",
    "        print(f\"    â€¢ {col}: {unique_count}ê°œ ì¹´í…Œê³ ë¦¬\")\n",
    "        \n",
    "        # íƒ€ê²Ÿ ì¸ì½”ë”© ë˜ëŠ” ë¹ˆë„ ì¸ì½”ë”© ì ìš©\n",
    "        if 'churn' in df.columns:\n",
    "            # íƒ€ê²Ÿ ì¸ì½”ë”© (ì´íƒˆë¥  ê¸°ë°˜)\n",
    "            target_encoding = df.groupby(col)['churn'].mean()\n",
    "            df[f'{col}_target_encoded'] = df[col].map(target_encoding)\n",
    "            print(f\"      âœ… íƒ€ê²Ÿ ì¸ì½”ë”© ì ìš© â†’ {col}_target_encoded\")\n",
    "        \n",
    "        # ë¹ˆë„ ì¸ì½”ë”©\n",
    "        frequency_encoding = df[col].value_counts()\n",
    "        df[f'{col}_frequency'] = df[col].map(frequency_encoding)\n",
    "        print(f\"      âœ… ë¹ˆë„ ì¸ì½”ë”© ì ìš© â†’ {col}_frequency\")\n",
    "        \n",
    "        # ì›ë³¸ ì»¬ëŸ¼ ì œê±°\n",
    "        df = df.drop(columns=[col])\n",
    "else:\n",
    "    print(f\"  â„¹ï¸ ê³ ì¹´ë””ë„ë¦¬í‹° ë³€ìˆ˜ê°€ ì—†ìŠµë‹ˆë‹¤.\")\n",
    "\n",
    "# 6. ìµœì¢… ë²”ì£¼í˜• ë³€ìˆ˜ ì²˜ë¦¬ í™•ì¸\n",
    "remaining_categorical = df.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "if 'customer_id' in remaining_categorical:\n",
    "    remaining_categorical.remove('customer_id')\n",
    "\n",
    "if remaining_categorical:\n",
    "    print(f\"\\nâš ï¸ ì²˜ë¦¬ë˜ì§€ ì•Šì€ ë²”ì£¼í˜• ë³€ìˆ˜:\")\n",
    "    for col in remaining_categorical:\n",
    "        print(f\"  â€¢ {col}: {df[col].nunique()}ê°œ ê³ ìœ ê°’\")\n",
    "        # ë‚¨ì€ ë²”ì£¼í˜• ë³€ìˆ˜ëŠ” ë¼ë²¨ ì¸ì½”ë”©ìœ¼ë¡œ ì²˜ë¦¬\n",
    "        le = LabelEncoder()\n",
    "        df[col] = le.fit_transform(df[col].astype(str))\n",
    "        print(f\"    âœ… ë¼ë²¨ ì¸ì½”ë”© ì ìš©\")\n",
    "\n",
    "# 7. ì¸ì½”ë”© ê²°ê³¼ ìš”ì•½\n",
    "print(f\"\\nğŸ“Š ì¸ì½”ë”© ê²°ê³¼ ìš”ì•½:\")\n",
    "print(f\"  â€¢ ì¸ì½”ë”© ì „ ì»¬ëŸ¼ ìˆ˜: {df_before_encoding.shape[1]}\")\n",
    "print(f\"  â€¢ ì¸ì½”ë”© í›„ ì»¬ëŸ¼ ìˆ˜: {df.shape[1]}\")\n",
    "print(f\"  â€¢ ì¶”ê°€ëœ ì»¬ëŸ¼ ìˆ˜: {df.shape[1] - df_before_encoding.shape[1]}\")\n",
    "\n",
    "# ë°ì´í„° íƒ€ì… ë¶„í¬\n",
    "numeric_cols = len(df.select_dtypes(include=[np.number]).columns)\n",
    "categorical_cols = len(df.select_dtypes(include=['object', 'category']).columns)\n",
    "\n",
    "print(f\"  â€¢ ìˆ˜ì¹˜í˜• ë³€ìˆ˜: {numeric_cols}ê°œ\")\n",
    "print(f\"  â€¢ ë²”ì£¼í˜• ë³€ìˆ˜: {categorical_cols}ê°œ\")\n",
    "\n",
    "# íƒ€ê²Ÿ ë³€ìˆ˜ í™•ì¸\n",
    "if 'churn' in df.columns:\n",
    "    print(f\"  â€¢ íƒ€ê²Ÿ ë³€ìˆ˜ (churn): {df['churn'].dtype}\")\n",
    "    print(f\"    - í´ë˜ìŠ¤ ë¶„í¬: {df['churn'].value_counts().to_dict()}\")\n",
    "\n",
    "print(f\"\\nâœ… ëª¨ë“  ì¸ì½”ë”© ì™„ë£Œ!\")\n",
    "\n",
    "# ì¸ì½”ë”©ëœ ë°ì´í„° ë¯¸ë¦¬ë³´ê¸°\n",
    "print(f\"\\nğŸ” ì¸ì½”ë”©ëœ ë°ì´í„° ë¯¸ë¦¬ë³´ê¸°:\")\n",
    "display(df.head())\n",
    "\n",
    "# ì»¬ëŸ¼ëª… ì •ë¦¬ (ê³µë°±ì´ë‚˜ íŠ¹ìˆ˜ë¬¸ì ì œê±°)\n",
    "df.columns = df.columns.str.replace(' ', '_').str.replace('-', '_').str.lower()\n",
    "print(f\"\\nğŸ”§ ì»¬ëŸ¼ëª… ì •ê·œí™” ì™„ë£Œ (ì†Œë¬¸ì, ì–¸ë”ìŠ¤ì½”ì–´ ì‚¬ìš©)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4099269e",
   "metadata": {},
   "source": [
    "## 6. ë°ì´í„° ë¶„í•  ë° ì €ì¥\n",
    "\n",
    "ì „ì²˜ë¦¬ê°€ ì™„ë£Œëœ ë°ì´í„°ë¥¼ í›ˆë ¨/ê²€ì¦/í…ŒìŠ¤íŠ¸ ì„¸íŠ¸ë¡œ ë¶„í• í•˜ê³  ì €ì¥í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f057271",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ğŸ“Š ë°ì´í„° ë¶„í•  ë° ì €ì¥\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# 1. í”¼ì²˜ì™€ íƒ€ê²Ÿ ë¶„ë¦¬\n",
    "if 'churn' in df.columns:\n",
    "    X = df.drop(['churn'], axis=1)\n",
    "    y = df['churn']\n",
    "    \n",
    "    # customer_idê°€ ìˆìœ¼ë©´ ì œê±° (ëª¨ë¸ë§ì— ë¶ˆí•„ìš”)\n",
    "    if 'customer_id' in X.columns:\n",
    "        customer_ids = X['customer_id'].copy()\n",
    "        X = X.drop(['customer_id'], axis=1)\n",
    "    else:\n",
    "        customer_ids = None\n",
    "    \n",
    "    print(f\"âœ… í”¼ì²˜-íƒ€ê²Ÿ ë¶„ë¦¬ ì™„ë£Œ:\")\n",
    "    print(f\"  â€¢ í”¼ì²˜ ìˆ˜ (X): {X.shape[1]}\")\n",
    "    print(f\"  â€¢ íƒ€ê²Ÿ ë³€ìˆ˜ (y): {y.name}\")\n",
    "    print(f\"  â€¢ ì´ ìƒ˜í”Œ ìˆ˜: {len(X)}\")\n",
    "else:\n",
    "    print(\"âŒ íƒ€ê²Ÿ ë³€ìˆ˜ 'churn'ì´ ì—†ìŠµë‹ˆë‹¤. ì „ì²´ ë°ì´í„°ë¥¼ í”¼ì²˜ë¡œ ì²˜ë¦¬í•©ë‹ˆë‹¤.\")\n",
    "    X = df.copy()\n",
    "    if 'customer_id' in X.columns:\n",
    "        customer_ids = X['customer_id'].copy()\n",
    "        X = X.drop(['customer_id'], axis=1)\n",
    "    else:\n",
    "        customer_ids = None\n",
    "    y = None\n",
    "\n",
    "# 2. ë°ì´í„° ë¶„í•  (í›ˆë ¨:ê²€ì¦:í…ŒìŠ¤íŠ¸ = 70:15:15)\n",
    "if y is not None:\n",
    "    print(f\"\\nğŸ”„ ë°ì´í„° ë¶„í•  ì‹œì‘:\")\n",
    "    print(f\"  ë¶„í•  ë¹„ìœ¨: í›ˆë ¨(70%) : ê²€ì¦(15%) : í…ŒìŠ¤íŠ¸(15%)\")\n",
    "    \n",
    "    # ë¨¼ì € í›ˆë ¨+ê²€ì¦ vs í…ŒìŠ¤íŠ¸ë¡œ ë¶„í•  (85:15)\n",
    "    X_temp, X_test, y_temp, y_test = train_test_split(\n",
    "        X, y, test_size=0.15, random_state=RANDOM_STATE, stratify=y\n",
    "    )\n",
    "    \n",
    "    # ë‹¤ì‹œ í›ˆë ¨ vs ê²€ì¦ìœ¼ë¡œ ë¶„í•  (70:15 = 82.35:17.65)\n",
    "    X_train, X_val, y_train, y_val = train_test_split(\n",
    "        X_temp, y_temp, test_size=0.176, random_state=RANDOM_STATE, stratify=y_temp\n",
    "    )\n",
    "    \n",
    "    print(f\"âœ… ë°ì´í„° ë¶„í•  ì™„ë£Œ:\")\n",
    "    print(f\"  â€¢ í›ˆë ¨ ì„¸íŠ¸: {len(X_train):,}ê°œ ({len(X_train)/len(X)*100:.1f}%)\")\n",
    "    print(f\"  â€¢ ê²€ì¦ ì„¸íŠ¸: {len(X_val):,}ê°œ ({len(X_val)/len(X)*100:.1f}%)\")\n",
    "    print(f\"  â€¢ í…ŒìŠ¤íŠ¸ ì„¸íŠ¸: {len(X_test):,}ê°œ ({len(X_test)/len(X)*100:.1f}%)\")\n",
    "    \n",
    "    # ê° ì„¸íŠ¸ì˜ íƒ€ê²Ÿ ë¶„í¬ í™•ì¸\n",
    "    print(f\"\\nğŸ“Š ê° ì„¸íŠ¸ë³„ ì´íƒˆë¥ :\")\n",
    "    print(f\"  â€¢ ì „ì²´: {y.mean():.3f}\")\n",
    "    print(f\"  â€¢ í›ˆë ¨: {y_train.mean():.3f}\")\n",
    "    print(f\"  â€¢ ê²€ì¦: {y_val.mean():.3f}\")\n",
    "    print(f\"  â€¢ í…ŒìŠ¤íŠ¸: {y_test.mean():.3f}\")\n",
    "    \n",
    "    # customer_idë„ í•¨ê»˜ ë¶„í•  (ìˆëŠ” ê²½ìš°)\n",
    "    if customer_ids is not None:\n",
    "        ids_temp = customer_ids.iloc[X_temp.index]\n",
    "        ids_test = customer_ids.iloc[X_test.index]\n",
    "        ids_train = ids_temp.iloc[X_train.index]\n",
    "        ids_val = ids_temp.iloc[X_val.index]\n",
    "else:\n",
    "    print(\"âš ï¸ íƒ€ê²Ÿ ë³€ìˆ˜ê°€ ì—†ì–´ ë¶„í• ì„ ê±´ë„ˆëœë‹ˆë‹¤.\")\n",
    "    X_train, X_val, X_test = X, None, None\n",
    "    y_train, y_val, y_test = None, None, None\n",
    "\n",
    "# 3. ìŠ¤ì¼€ì¼ë§ (í›ˆë ¨ ì„¸íŠ¸ ê¸°ì¤€ìœ¼ë¡œ fit, ëª¨ë“  ì„¸íŠ¸ì— transform)\n",
    "print(f\"\\nâš–ï¸ í”¼ì²˜ ìŠ¤ì¼€ì¼ë§:\")\n",
    "\n",
    "# ìˆ˜ì¹˜í˜• ì»¬ëŸ¼ ì‹ë³„\n",
    "numeric_features = X_train.select_dtypes(include=[np.number]).columns.tolist()\n",
    "print(f\"  â€¢ ìŠ¤ì¼€ì¼ë§ ëŒ€ìƒ: {len(numeric_features)}ê°œ ìˆ˜ì¹˜í˜• í”¼ì²˜\")\n",
    "\n",
    "if len(numeric_features) > 0:\n",
    "    # StandardScaler ì ìš©\n",
    "    scaler = StandardScaler()\n",
    "    \n",
    "    # í›ˆë ¨ ì„¸íŠ¸ë¡œ scaler í•™ìŠµ\n",
    "    X_train_scaled = X_train.copy()\n",
    "    X_train_scaled[numeric_features] = scaler.fit_transform(X_train[numeric_features])\n",
    "    \n",
    "    if X_val is not None:\n",
    "        X_val_scaled = X_val.copy()\n",
    "        X_val_scaled[numeric_features] = scaler.transform(X_val[numeric_features])\n",
    "    else:\n",
    "        X_val_scaled = None\n",
    "    \n",
    "    if X_test is not None:\n",
    "        X_test_scaled = X_test.copy()\n",
    "        X_test_scaled[numeric_features] = scaler.transform(X_test[numeric_features])\n",
    "    else:\n",
    "        X_test_scaled = None\n",
    "    \n",
    "    print(f\"  âœ… StandardScaler ì ìš© ì™„ë£Œ\")\n",
    "    print(f\"    - í‰ê· : 0, í‘œì¤€í¸ì°¨: 1ë¡œ ì •ê·œí™”\")\n",
    "    \n",
    "    # ìŠ¤ì¼€ì¼ë§ ì „í›„ ë¹„êµ (ì²« ë²ˆì§¸ ìˆ˜ì¹˜í˜• í”¼ì²˜)\n",
    "    if len(numeric_features) > 0:\n",
    "        sample_feature = numeric_features[0]\n",
    "        print(f\"    - ì˜ˆì‹œ ({sample_feature}):\")\n",
    "        print(f\"      ì›ë³¸: í‰ê· ={X_train[sample_feature].mean():.2f}, í‘œì¤€í¸ì°¨={X_train[sample_feature].std():.2f}\")\n",
    "        print(f\"      ë³€í™˜: í‰ê· ={X_train_scaled[sample_feature].mean():.2f}, í‘œì¤€í¸ì°¨={X_train_scaled[sample_feature].std():.2f}\")\n",
    "else:\n",
    "    print(\"  â„¹ï¸ ìŠ¤ì¼€ì¼ë§í•  ìˆ˜ì¹˜í˜• í”¼ì²˜ê°€ ì—†ìŠµë‹ˆë‹¤.\")\n",
    "    X_train_scaled = X_train.copy()\n",
    "    X_val_scaled = X_val.copy() if X_val is not None else None\n",
    "    X_test_scaled = X_test.copy() if X_test is not None else None\n",
    "    scaler = None\n",
    "\n",
    "# 4. ì „ì²˜ë¦¬ëœ ë°ì´í„° ì €ì¥\n",
    "print(f\"\\nğŸ’¾ ì „ì²˜ë¦¬ëœ ë°ì´í„° ì €ì¥:\")\n",
    "\n",
    "# ì €ì¥ ê²½ë¡œ ì„¤ì •\n",
    "save_paths = {\n",
    "    'train': os.path.join(processed_data_path, 'train_data.csv'),\n",
    "    'val': os.path.join(processed_data_path, 'val_data.csv'),\n",
    "    'test': os.path.join(processed_data_path, 'test_data.csv'),\n",
    "    'train_scaled': os.path.join(processed_data_path, 'train_data_scaled.csv'),\n",
    "    'val_scaled': os.path.join(processed_data_path, 'val_data_scaled.csv'),\n",
    "    'test_scaled': os.path.join(processed_data_path, 'test_data_scaled.csv'),\n",
    "    'scaler': os.path.join(processed_data_path, 'scaler.pkl'),\n",
    "    'feature_names': os.path.join(processed_data_path, 'feature_names.pkl')\n",
    "}\n",
    "\n",
    "# í›ˆë ¨ ì„¸íŠ¸ ì €ì¥\n",
    "if y_train is not None:\n",
    "    train_df = X_train.copy()\n",
    "    train_df['churn'] = y_train\n",
    "    train_df.to_csv(save_paths['train'], index=False)\n",
    "    \n",
    "    train_scaled_df = X_train_scaled.copy()\n",
    "    train_scaled_df['churn'] = y_train\n",
    "    train_scaled_df.to_csv(save_paths['train_scaled'], index=False)\n",
    "    \n",
    "    print(f\"  âœ… í›ˆë ¨ ì„¸íŠ¸ ì €ì¥: {save_paths['train']}\")\n",
    "    print(f\"  âœ… í›ˆë ¨ ì„¸íŠ¸ (ìŠ¤ì¼€ì¼ë§) ì €ì¥: {save_paths['train_scaled']}\")\n",
    "\n",
    "# ê²€ì¦ ì„¸íŠ¸ ì €ì¥\n",
    "if y_val is not None:\n",
    "    val_df = X_val.copy()\n",
    "    val_df['churn'] = y_val\n",
    "    val_df.to_csv(save_paths['val'], index=False)\n",
    "    \n",
    "    val_scaled_df = X_val_scaled.copy()\n",
    "    val_scaled_df['churn'] = y_val\n",
    "    val_scaled_df.to_csv(save_paths['val_scaled'], index=False)\n",
    "    \n",
    "    print(f\"  âœ… ê²€ì¦ ì„¸íŠ¸ ì €ì¥: {save_paths['val']}\")\n",
    "    print(f\"  âœ… ê²€ì¦ ì„¸íŠ¸ (ìŠ¤ì¼€ì¼ë§) ì €ì¥: {save_paths['val_scaled']}\")\n",
    "\n",
    "# í…ŒìŠ¤íŠ¸ ì„¸íŠ¸ ì €ì¥\n",
    "if y_test is not None:\n",
    "    test_df = X_test.copy()\n",
    "    test_df['churn'] = y_test\n",
    "    test_df.to_csv(save_paths['test'], index=False)\n",
    "    \n",
    "    test_scaled_df = X_test_scaled.copy()\n",
    "    test_scaled_df['churn'] = y_test\n",
    "    test_scaled_df.to_csv(save_paths['test_scaled'], index=False)\n",
    "    \n",
    "    print(f\"  âœ… í…ŒìŠ¤íŠ¸ ì„¸íŠ¸ ì €ì¥: {save_paths['test']}\")\n",
    "    print(f\"  âœ… í…ŒìŠ¤íŠ¸ ì„¸íŠ¸ (ìŠ¤ì¼€ì¼ë§) ì €ì¥: {save_paths['test_scaled']}\")\n",
    "\n",
    "# ìŠ¤ì¼€ì¼ëŸ¬ ì €ì¥\n",
    "if scaler is not None:\n",
    "    with open(save_paths['scaler'], 'wb') as f:\n",
    "        pickle.dump(scaler, f)\n",
    "    print(f\"  âœ… ìŠ¤ì¼€ì¼ëŸ¬ ì €ì¥: {save_paths['scaler']}\")\n",
    "\n",
    "# í”¼ì²˜ ì´ë¦„ ì €ì¥\n",
    "feature_names = X_train.columns.tolist()\n",
    "with open(save_paths['feature_names'], 'wb') as f:\n",
    "    pickle.dump(feature_names, f)\n",
    "print(f\"  âœ… í”¼ì²˜ ì´ë¦„ ì €ì¥: {save_paths['feature_names']}\")\n",
    "\n",
    "# 5. ì „ì²˜ë¦¬ ìš”ì•½ ì €ì¥\n",
    "summary_path = os.path.join(processed_data_path, 'preprocessing_summary.txt')\n",
    "with open(summary_path, 'w', encoding='utf-8') as f:\n",
    "    f.write(\"ê³ ê° ì´íƒˆ ì˜ˆì¸¡ - ë°ì´í„° ì „ì²˜ë¦¬ ìš”ì•½\\\\n\")\n",
    "    f.write(\"=\" * 50 + \"\\\\n\\\\n\")\n",
    "    f.write(f\"ì „ì²˜ë¦¬ ì™„ë£Œ ì‹œê°„: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\\\n\\\\n\")\n",
    "    \n",
    "    f.write(f\"ë°ì´í„° ì •ë³´:\\\\n\")\n",
    "    f.write(f\"  â€¢ ì›ë³¸ ìƒ˜í”Œ ìˆ˜: {len(df_original):,}\\\\n\")\n",
    "    f.write(f\"  â€¢ ìµœì¢… ìƒ˜í”Œ ìˆ˜: {len(df):,}\\\\n\")\n",
    "    f.write(f\"  â€¢ ì´ í”¼ì²˜ ìˆ˜: {len(feature_names)}\\\\n\")\n",
    "    \n",
    "    if y is not None:\n",
    "        f.write(f\"  â€¢ ì´íƒˆë¥ : {y.mean():.3f}\\\\n\")\n",
    "        f.write(f\"\\\\në°ì´í„° ë¶„í• :\\\\n\")\n",
    "        f.write(f\"  â€¢ í›ˆë ¨ ì„¸íŠ¸: {len(X_train):,}ê°œ\\\\n\")\n",
    "        if X_val is not None:\n",
    "            f.write(f\"  â€¢ ê²€ì¦ ì„¸íŠ¸: {len(X_val):,}ê°œ\\\\n\")\n",
    "        if X_test is not None:\n",
    "            f.write(f\"  â€¢ í…ŒìŠ¤íŠ¸ ì„¸íŠ¸: {len(X_test):,}ê°œ\\\\n\")\n",
    "    \n",
    "    f.write(f\"\\\\nìƒì„±ëœ íŒŒì¼:\\\\n\")\n",
    "    for key, path in save_paths.items():\n",
    "        if os.path.exists(path):\n",
    "            f.write(f\"  â€¢ {key}: {os.path.basename(path)}\\\\n\")\n",
    "\n",
    "print(f\"  âœ… ì „ì²˜ë¦¬ ìš”ì•½ ì €ì¥: {summary_path}\")\n",
    "\n",
    "# 6. ìµœì¢… ê²°ê³¼ ìš”ì•½\n",
    "print(f\"\\\\nğŸ‰ ë°ì´í„° ì „ì²˜ë¦¬ ì™„ë£Œ!\")\n",
    "print(f\"=\" * 60)\n",
    "print(f\"ğŸ“Š ìµœì¢… ê²°ê³¼ ìš”ì•½:\")\n",
    "print(f\"  â€¢ ì²˜ë¦¬ëœ ì´ ìƒ˜í”Œ ìˆ˜: {len(df):,}\")\n",
    "print(f\"  â€¢ ìµœì¢… í”¼ì²˜ ìˆ˜: {len(feature_names)}\")\n",
    "print(f\"  â€¢ ì €ì¥ëœ íŒŒì¼ ìˆ˜: {len([p for p in save_paths.values() if os.path.exists(p)])}\")\n",
    "print(f\"  â€¢ ì €ì¥ ìœ„ì¹˜: {processed_data_path}\")\n",
    "\n",
    "if y is not None:\n",
    "    print(f\"\\\\nğŸ¯ ë‹¤ìŒ ë‹¨ê³„:\")\n",
    "    print(f\"  1. 03_Modeling.ipynbì—ì„œ ëª¨ë¸ í•™ìŠµ ì§„í–‰\")\n",
    "    print(f\"  2. ì €ì¥ëœ ì „ì²˜ë¦¬ ë°ì´í„° í™œìš©\")\n",
    "    print(f\"  3. ìŠ¤ì¼€ì¼ëŸ¬ë¥¼ ì‚¬ìš©í•œ ìƒˆ ë°ì´í„° ì „ì²˜ë¦¬ ê°€ëŠ¥\")\n",
    "\n",
    "print(f\"\\\\nâœ… ì „ì²˜ë¦¬ í”„ë¡œì„¸ìŠ¤ ì„±ê³µì ìœ¼ë¡œ ì™„ë£Œ!\")\n",
    "print(f\"ğŸ• ì™„ë£Œ ì‹œê°„: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print(\"=\" * 60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "abc-bootcamp-FP-2025",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

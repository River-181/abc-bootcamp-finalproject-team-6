{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a6f2907b",
   "metadata": {},
   "source": [
    "# 고객 이탈 예측 - 데이터 전처리\n",
    "\n",
    "이 노트북에서는 탐색적 데이터 분석(EDA) 결과를 바탕으로 머신러닝 모델 학습을 위한 데이터 전처리를 수행합니다.\n",
    "\n",
    "## 주요 내용\n",
    "1. **데이터 로드 및 검증**: EDA에서 분석한 데이터 불러오기\n",
    "2. **데이터 정제**: 결측치, 이상치, 중복값 처리\n",
    "3. **피처 엔지니어링**: 새로운 변수 생성 및 기존 변수 변환\n",
    "4. **인코딩**: 범주형 변수 처리\n",
    "5. **스케일링**: 수치형 변수 정규화\n",
    "6. **데이터 분할**: 훈련/검증/테스트 세트 분리\n",
    "7. **전처리된 데이터 저장**: 모델링을 위한 데이터 export\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e106412c",
   "metadata": {},
   "source": [
    "## 1. 라이브러리 및 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7d953f2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📚 전처리 라이브러리 로드 완료!\n",
      "🕐 전처리 시작 시간: 2025-08-03 18:53:31\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# 기본 라이브러리\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "from datetime import datetime\n",
    "import os\n",
    "\n",
    "# 전처리 라이브러리\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder, OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer, KNNImputer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# 추가 유틸리티\n",
    "import joblib\n",
    "import pickle\n",
    "\n",
    "# 설정\n",
    "warnings.filterwarnings('ignore')\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# 한글 폰트 설정\n",
    "plt.rcParams['font.family'] = ['AppleGothic'] if os.name == 'posix' else ['Malgun Gothic']\n",
    "plt.rcParams['axes.unicode_minus'] = False\n",
    "\n",
    "# 표시 옵션\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', None)\n",
    "\n",
    "# 랜덤 시드\n",
    "RANDOM_STATE = 42\n",
    "np.random.seed(RANDOM_STATE)\n",
    "\n",
    "print(\"📚 전처리 라이브러리 로드 완료!\")\n",
    "print(f\"🕐 전처리 시작 시간: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32b95249",
   "metadata": {},
   "source": [
    "## 2. 데이터 로드 및 검증\n",
    "\n",
    "EDA에서 분석한 데이터를 불러오고 기본적인 검증을 수행합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f87725c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 파일 경로 확인\n",
    "raw_data_path = '../data/raw/customer_data.csv'\n",
    "processed_data_path = '../data/processed/'\n",
    "\n",
    "# 폴더 생성\n",
    "os.makedirs(processed_data_path, exist_ok=True)\n",
    "\n",
    "# 데이터 로드 시도\n",
    "if os.path.exists(raw_data_path):\n",
    "    print(f\"📁 데이터 로드: {raw_data_path}\")\n",
    "    df = pd.read_csv(raw_data_path)\n",
    "    print(\"✅ 실제 데이터 로드 성공!\")\n",
    "else:\n",
    "    print(\"⚠️ 실제 데이터 파일이 없습니다. 샘플 데이터를 생성합니다.\")\n",
    "    \n",
    "    # 고객 이탈 예측을 위한 샘플 데이터 생성\n",
    "    np.random.seed(RANDOM_STATE)\n",
    "    n_samples = 5000\n",
    "    \n",
    "    # 고객 기본 정보\n",
    "    customer_data = {\n",
    "        'customer_id': [f'CUST_{i:05d}' for i in range(1, n_samples + 1)],\n",
    "        'age': np.random.normal(45, 15, n_samples).astype(int),\n",
    "        'gender': np.random.choice(['Male', 'Female'], n_samples, p=[0.52, 0.48]),\n",
    "        'tenure_months': np.random.exponential(24, n_samples).astype(int),\n",
    "        'monthly_charges': np.random.normal(65, 20, n_samples),\n",
    "        'total_charges': None,  # 계산으로 생성\n",
    "        'contract_type': np.random.choice(['Month-to-month', 'One year', 'Two year'], \n",
    "                                        n_samples, p=[0.55, 0.25, 0.20]),\n",
    "        'payment_method': np.random.choice(['Electronic check', 'Credit card', 'Bank transfer', 'Mailed check'],\n",
    "                                         n_samples, p=[0.35, 0.25, 0.25, 0.15]),\n",
    "        'internet_service': np.random.choice(['DSL', 'Fiber optic', 'No'], \n",
    "                                           n_samples, p=[0.35, 0.45, 0.20]),\n",
    "        'phone_service': np.random.choice(['Yes', 'No'], n_samples, p=[0.85, 0.15]),\n",
    "        'multiple_lines': np.random.choice(['Yes', 'No', 'No phone service'], \n",
    "                                         n_samples, p=[0.40, 0.45, 0.15]),\n",
    "        'online_security': np.random.choice(['Yes', 'No', 'No internet service'], \n",
    "                                          n_samples, p=[0.30, 0.50, 0.20]),\n",
    "        'online_backup': np.random.choice(['Yes', 'No', 'No internet service'], \n",
    "                                        n_samples, p=[0.35, 0.45, 0.20]),\n",
    "        'device_protection': np.random.choice(['Yes', 'No', 'No internet service'], \n",
    "                                            n_samples, p=[0.32, 0.48, 0.20]),\n",
    "        'tech_support': np.random.choice(['Yes', 'No', 'No internet service'], \n",
    "                                       n_samples, p=[0.28, 0.52, 0.20]),\n",
    "        'streaming_tv': np.random.choice(['Yes', 'No', 'No internet service'], \n",
    "                                       n_samples, p=[0.38, 0.42, 0.20]),\n",
    "        'streaming_movies': np.random.choice(['Yes', 'No', 'No internet service'], \n",
    "                                           n_samples, p=[0.36, 0.44, 0.20]),\n",
    "        'paperless_billing': np.random.choice(['Yes', 'No'], n_samples, p=[0.75, 0.25]),\n",
    "        'senior_citizen': np.random.choice([0, 1], n_samples, p=[0.84, 0.16])\n",
    "    }\n",
    "    \n",
    "    df = pd.DataFrame(customer_data)\n",
    "    \n",
    "    # total_charges 계산 (tenure_months * monthly_charges + 노이즈)\n",
    "    df['total_charges'] = (df['tenure_months'] * df['monthly_charges'] + \n",
    "                          np.random.normal(0, 100, n_samples)).round(2)\n",
    "    df['total_charges'] = np.maximum(df['total_charges'], df['monthly_charges'])  # 최소값 보정\n",
    "    \n",
    "    # 이탈 여부 생성 (다양한 요인을 고려한 확률적 생성)\n",
    "    churn_prob = 0.15  # 기본 이탈 확률\n",
    "    \n",
    "    # 계약 유형에 따른 이탈 확률 조정\n",
    "    contract_multiplier = df['contract_type'].map({\n",
    "        'Month-to-month': 2.5,\n",
    "        'One year': 1.0,\n",
    "        'Two year': 0.3\n",
    "    })\n",
    "    \n",
    "    # 기타 요인들\n",
    "    age_factor = np.where(df['age'] > 65, 0.8, 1.0)  # 고령자 이탈 낮음\n",
    "    tenure_factor = np.where(df['tenure_months'] < 6, 2.0, \n",
    "                           np.where(df['tenure_months'] > 36, 0.5, 1.0))\n",
    "    charges_factor = np.where(df['monthly_charges'] > 80, 1.5, 0.8)\n",
    "    \n",
    "    # 최종 이탈 확률\n",
    "    final_churn_prob = (churn_prob * contract_multiplier * age_factor * \n",
    "                       tenure_factor * charges_factor)\n",
    "    final_churn_prob = np.clip(final_churn_prob, 0, 0.8)  # 최대 80% 제한\n",
    "    \n",
    "    df['churn'] = np.random.binomial(1, final_churn_prob)\n",
    "    \n",
    "    # 일부 결측치 의도적 생성\n",
    "    missing_indices = np.random.choice(n_samples, int(n_samples * 0.02), replace=False)\n",
    "    df.loc[missing_indices, 'total_charges'] = np.nan\n",
    "    \n",
    "    print(\"✅ 샘플 데이터 생성 완료!\")\n",
    "\n",
    "# 데이터 기본 정보 출력\n",
    "print(f\"\\n📊 데이터 기본 정보:\")\n",
    "print(f\"  • 전체 샘플 수: {len(df):,}\")\n",
    "print(f\"  • 전체 피처 수: {df.shape[1]}\")\n",
    "print(f\"  • 메모리 사용량: {df.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n",
    "\n",
    "# 타겟 변수 분포\n",
    "if 'churn' in df.columns:\n",
    "    churn_rate = df['churn'].mean()\n",
    "    print(f\"  • 이탈률: {churn_rate:.2%}\")\n",
    "    print(f\"  • 이탈 고객 수: {df['churn'].sum():,}\")\n",
    "    print(f\"  • 유지 고객 수: {(df['churn'] == 0).sum():,}\")\n",
    "\n",
    "print(f\"\\n📋 데이터 타입 정보:\")\n",
    "print(df.dtypes.value_counts())\n",
    "\n",
    "print(f\"\\n🔍 데이터 미리보기:\")\n",
    "display(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77335aaf",
   "metadata": {},
   "source": [
    "## 3. 데이터 정제\n",
    "\n",
    "결측치, 이상치, 중복값을 처리하고 데이터 품질을 개선합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f942733",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 정제 전 백업\n",
    "df_original = df.copy()\n",
    "\n",
    "print(\"🧹 데이터 정제 시작\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# 1. 결측치 분석\n",
    "print(\"📊 결측치 분석:\")\n",
    "missing_info = df.isnull().sum()\n",
    "missing_percent = (missing_info / len(df) * 100).round(2)\n",
    "missing_df = pd.DataFrame({\n",
    "    'Missing_Count': missing_info,\n",
    "    'Missing_Percent': missing_percent\n",
    "}).sort_values('Missing_Count', ascending=False)\n",
    "\n",
    "# 결측치가 있는 컬럼만 표시\n",
    "missing_cols = missing_df[missing_df['Missing_Count'] > 0]\n",
    "if len(missing_cols) > 0:\n",
    "    print(\"❌ 결측치가 있는 컬럼:\")\n",
    "    display(missing_cols)\n",
    "else:\n",
    "    print(\"✅ 결측치 없음\")\n",
    "\n",
    "# 2. 중복 데이터 확인\n",
    "duplicates = df.duplicated().sum()\n",
    "print(f\"\\n🔍 중복 데이터: {duplicates}개\")\n",
    "\n",
    "if duplicates > 0:\n",
    "    print(\"❌ 중복 데이터 제거 중...\")\n",
    "    df = df.drop_duplicates()\n",
    "    print(f\"✅ {duplicates}개 중복 데이터 제거 완료\")\n",
    "\n",
    "# 3. 결측치 처리\n",
    "if len(missing_cols) > 0:\n",
    "    print(f\"\\n🔧 결측치 처리 시작...\")\n",
    "    \n",
    "    for col in missing_cols.index:\n",
    "        missing_count = missing_cols.loc[col, 'Missing_Count']\n",
    "        missing_pct = missing_cols.loc[col, 'Missing_Percent']\n",
    "        \n",
    "        print(f\"\\n  📋 {col} 컬럼 처리 ({missing_count}개, {missing_pct}%)\")\n",
    "        \n",
    "        if df[col].dtype in ['int64', 'float64']:  # 수치형 데이터\n",
    "            if missing_pct < 5:  # 5% 미만이면 중앙값으로 대체\n",
    "                median_value = df[col].median()\n",
    "                df[col].fillna(median_value, inplace=True)\n",
    "                print(f\"    ✅ 중앙값({median_value:.2f})으로 대체\")\n",
    "            else:  # 5% 이상이면 KNN 대체\n",
    "                # 수치형 컬럼들만 선택해서 KNN 적용\n",
    "                numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "                if col in numeric_cols:\n",
    "                    numeric_cols.remove(col)\n",
    "                \n",
    "                if len(numeric_cols) > 0:\n",
    "                    imputer = KNNImputer(n_neighbors=5)\n",
    "                    df[col] = imputer.fit_transform(df[[col] + numeric_cols[:3]])[:, 0]\n",
    "                    print(f\"    ✅ KNN 대체 완료\")\n",
    "                else:\n",
    "                    median_value = df[col].median()\n",
    "                    df[col].fillna(median_value, inplace=True)\n",
    "                    print(f\"    ✅ 중앙값({median_value:.2f})으로 대체\")\n",
    "        else:  # 범주형 데이터\n",
    "            mode_value = df[col].mode().iloc[0] if len(df[col].mode()) > 0 else 'Unknown'\n",
    "            df[col].fillna(mode_value, inplace=True)\n",
    "            print(f\"    ✅ 최빈값('{mode_value}')으로 대체\")\n",
    "\n",
    "# 4. 데이터 타입 최적화\n",
    "print(f\"\\n🎯 데이터 타입 최적화:\")\n",
    "\n",
    "# 정수형 최적화\n",
    "for col in df.select_dtypes(include=['int64']).columns:\n",
    "    col_min, col_max = df[col].min(), df[col].max()\n",
    "    if col_min >= 0:  # 양수인 경우\n",
    "        if col_max < 255:\n",
    "            df[col] = df[col].astype('uint8')\n",
    "        elif col_max < 65535:\n",
    "            df[col] = df[col].astype('uint16')\n",
    "        elif col_max < 4294967295:\n",
    "            df[col] = df[col].astype('uint32')\n",
    "    else:  # 음수가 있는 경우\n",
    "        if col_min > -128 and col_max < 127:\n",
    "            df[col] = df[col].astype('int8')\n",
    "        elif col_min > -32768 and col_max < 32767:\n",
    "            df[col] = df[col].astype('int16')\n",
    "        elif col_min > -2147483648 and col_max < 2147483647:\n",
    "            df[col] = df[col].astype('int32')\n",
    "\n",
    "# 실수형 최적화\n",
    "for col in df.select_dtypes(include=['float64']).columns:\n",
    "    df[col] = pd.to_numeric(df[col], downcast='float')\n",
    "\n",
    "# 5. 이상치 탐지 및 처리\n",
    "print(f\"\\n🔎 이상치 탐지 (수치형 변수):\")\n",
    "\n",
    "numeric_columns = df.select_dtypes(include=[np.number]).columns\n",
    "outlier_summary = {}\n",
    "\n",
    "for col in numeric_columns:\n",
    "    Q1 = df[col].quantile(0.25)\n",
    "    Q3 = df[col].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "    \n",
    "    outliers = ((df[col] < lower_bound) | (df[col] > upper_bound)).sum()\n",
    "    outlier_pct = (outliers / len(df) * 100).round(2)\n",
    "    \n",
    "    outlier_summary[col] = {\n",
    "        'count': outliers,\n",
    "        'percentage': outlier_pct,\n",
    "        'lower_bound': lower_bound,\n",
    "        'upper_bound': upper_bound\n",
    "    }\n",
    "    \n",
    "    if outliers > 0:\n",
    "        print(f\"  📊 {col}: {outliers}개 ({outlier_pct}%)\")\n",
    "        \n",
    "        # 이상치가 5% 미만이면 상한/하한선으로 클리핑\n",
    "        if outlier_pct < 5:\n",
    "            df[col] = df[col].clip(lower=lower_bound, upper=upper_bound)\n",
    "            print(f\"      ✅ 상한/하한선으로 클리핑 적용\")\n",
    "\n",
    "# 6. 정제 결과 요약\n",
    "print(f\"\\n📋 데이터 정제 결과:\")\n",
    "print(f\"  • 처리 전 샘플 수: {len(df_original):,}\")\n",
    "print(f\"  • 처리 후 샘플 수: {len(df):,}\")\n",
    "print(f\"  • 제거된 샘플 수: {len(df_original) - len(df):,}\")\n",
    "\n",
    "# 메모리 사용량 비교\n",
    "memory_before = df_original.memory_usage(deep=True).sum() / 1024**2\n",
    "memory_after = df.memory_usage(deep=True).sum() / 1024**2\n",
    "memory_reduction = ((memory_before - memory_after) / memory_before * 100)\n",
    "\n",
    "print(f\"  • 메모리 사용량 (전): {memory_before:.2f} MB\")\n",
    "print(f\"  • 메모리 사용량 (후): {memory_after:.2f} MB\")\n",
    "print(f\"  • 메모리 절약: {memory_reduction:.1f}%\")\n",
    "\n",
    "# 최종 결측치 확인\n",
    "final_missing = df.isnull().sum().sum()\n",
    "print(f\"  • 남은 결측치: {final_missing}개\")\n",
    "\n",
    "if final_missing == 0:\n",
    "    print(\"✅ 모든 결측치 처리 완료!\")\n",
    "else:\n",
    "    print(\"⚠️ 일부 결측치가 남아있습니다.\")\n",
    "\n",
    "print(f\"\\n🔍 정제된 데이터 미리보기:\")\n",
    "display(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07ffe59e",
   "metadata": {},
   "source": [
    "## 4. 피처 엔지니어링\n",
    "\n",
    "기존 변수를 활용하여 새로운 의미 있는 변수를 생성하고 기존 변수를 변환합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e65af08",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"🔧 피처 엔지니어링 시작\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# 1. 새로운 수치형 피처 생성\n",
    "print(\"📊 수치형 피처 생성:\")\n",
    "\n",
    "# 고객 가치 관련 피처\n",
    "if 'monthly_charges' in df.columns and 'tenure_months' in df.columns:\n",
    "    # 월 평균 사용료 대비 총 사용료 비율\n",
    "    df['charges_per_month'] = df['total_charges'] / (df['tenure_months'] + 1)  # +1로 0 나누기 방지\n",
    "    print(\"  ✅ charges_per_month: 월평균 요금\")\n",
    "    \n",
    "    # 고객 생애 가치 (Customer Lifetime Value 추정)\n",
    "    df['estimated_clv'] = df['monthly_charges'] * df['tenure_months'] * 1.2  # 20% 마진 고려\n",
    "    print(\"  ✅ estimated_clv: 추정 고객 생애 가치\")\n",
    "\n",
    "# 고객 세그먼트 관련 피처\n",
    "if 'age' in df.columns:\n",
    "    # 연령대 그룹\n",
    "    df['age_group'] = pd.cut(df['age'], \n",
    "                            bins=[0, 25, 35, 45, 55, 65, 100], \n",
    "                            labels=['Under_25', '25-34', '35-44', '45-54', '55-64', 'Over_65'])\n",
    "    print(\"  ✅ age_group: 연령대 그룹\")\n",
    "\n",
    "if 'tenure_months' in df.columns:\n",
    "    # 고객 충성도 세그먼트\n",
    "    df['loyalty_segment'] = pd.cut(df['tenure_months'],\n",
    "                                  bins=[0, 6, 12, 24, 60, 1000],\n",
    "                                  labels=['New', 'Growing', 'Established', 'Loyal', 'Champion'])\n",
    "    print(\"  ✅ loyalty_segment: 고객 충성도 세그먼트\")\n",
    "\n",
    "# 2. 서비스 관련 피처 엔지니어링\n",
    "print(f\"\\n🛠️ 서비스 관련 피처:\")\n",
    "\n",
    "# 부가 서비스 개수 카운트\n",
    "service_cols = [col for col in df.columns if any(service in col.lower() \n",
    "                for service in ['security', 'backup', 'protection', 'support', \n",
    "                              'streaming', 'lines'])]\n",
    "\n",
    "if service_cols:\n",
    "    # 부가 서비스 중 'Yes'인 것의 개수\n",
    "    def count_services(row):\n",
    "        return sum(1 for col in service_cols if row[col] == 'Yes')\n",
    "    \n",
    "    df['total_services'] = df.apply(count_services, axis=1)\n",
    "    print(f\"  ✅ total_services: 이용 중인 부가 서비스 수 ({len(service_cols)}개 중)\")\n",
    "\n",
    "# 인터넷 서비스 품질 점수\n",
    "if 'internet_service' in df.columns:\n",
    "    internet_score_map = {'Fiber optic': 3, 'DSL': 2, 'No': 0}\n",
    "    df['internet_score'] = df['internet_service'].map(internet_score_map)\n",
    "    print(\"  ✅ internet_score: 인터넷 서비스 품질 점수\")\n",
    "\n",
    "# 3. 계약 관련 피처\n",
    "print(f\"\\n📋 계약 관련 피처:\")\n",
    "\n",
    "if 'contract_type' in df.columns:\n",
    "    # 계약 안정성 점수\n",
    "    contract_stability = {'Month-to-month': 1, 'One year': 2, 'Two year': 3}\n",
    "    df['contract_stability'] = df['contract_type'].map(contract_stability)\n",
    "    print(\"  ✅ contract_stability: 계약 안정성 점수\")\n",
    "\n",
    "if 'payment_method' in df.columns:\n",
    "    # 자동 결제 여부\n",
    "    auto_payment = ['Credit card', 'Bank transfer']\n",
    "    df['auto_payment'] = df['payment_method'].apply(lambda x: 1 if x in auto_payment else 0)\n",
    "    print(\"  ✅ auto_payment: 자동 결제 여부\")\n",
    "\n",
    "# 4. 고객 리스크 점수 계산\n",
    "print(f\"\\n⚠️ 리스크 점수 계산:\")\n",
    "\n",
    "risk_factors = []\n",
    "\n",
    "# 계약 유형 리스크 (월 단위 계약이 가장 위험)\n",
    "if 'contract_type' in df.columns:\n",
    "    contract_risk = df['contract_type'].map({'Month-to-month': 3, 'One year': 2, 'Two year': 1})\n",
    "    risk_factors.append(contract_risk)\n",
    "\n",
    "# 신규 고객 리스크 (tenure가 짧을수록 위험)\n",
    "if 'tenure_months' in df.columns:\n",
    "    tenure_risk = np.where(df['tenure_months'] < 6, 3,\n",
    "                          np.where(df['tenure_months'] < 12, 2, 1))\n",
    "    risk_factors.append(pd.Series(tenure_risk, index=df.index))\n",
    "\n",
    "# 고요금 고객 리스크 (월 요금이 높을수록 이탈 가능성 증가)\n",
    "if 'monthly_charges' in df.columns:\n",
    "    charges_percentile = df['monthly_charges'].rank(pct=True)\n",
    "    charges_risk = np.where(charges_percentile > 0.8, 3,\n",
    "                           np.where(charges_percentile > 0.6, 2, 1))\n",
    "    risk_factors.append(pd.Series(charges_risk, index=df.index))\n",
    "\n",
    "# 수동 결제 리스크\n",
    "if 'auto_payment' in df.columns:\n",
    "    payment_risk = np.where(df['auto_payment'] == 0, 2, 1)\n",
    "    risk_factors.append(pd.Series(payment_risk, index=df.index))\n",
    "\n",
    "# 종합 리스크 점수\n",
    "if risk_factors:\n",
    "    df['risk_score'] = sum(risk_factors)\n",
    "    df['risk_level'] = pd.cut(df['risk_score'], \n",
    "                             bins=[0, 4, 7, 10, 20], \n",
    "                             labels=['Low', 'Medium', 'High', 'Critical'])\n",
    "    print(f\"  ✅ risk_score: 종합 리스크 점수 (4-{df['risk_score'].max()})\")\n",
    "    print(f\"  ✅ risk_level: 리스크 레벨 (Low/Medium/High/Critical)\")\n",
    "\n",
    "# 5. 상호작용 피처 (중요한 변수들 간의 조합)\n",
    "print(f\"\\n🔗 상호작용 피처:\")\n",
    "\n",
    "if 'monthly_charges' in df.columns and 'tenure_months' in df.columns:\n",
    "    # 요금 대비 충성도\n",
    "    df['charges_tenure_ratio'] = df['monthly_charges'] / (df['tenure_months'] + 1)\n",
    "    print(\"  ✅ charges_tenure_ratio: 요금 대비 충성도\")\n",
    "\n",
    "if 'total_services' in df.columns and 'monthly_charges' in df.columns:\n",
    "    # 서비스당 평균 요금\n",
    "    df['charges_per_service'] = df['monthly_charges'] / (df['total_services'] + 1)\n",
    "    print(\"  ✅ charges_per_service: 서비스당 평균 요금\")\n",
    "\n",
    "# 6. 로그 변환 (치우친 분포의 수치형 변수)\n",
    "print(f\"\\n📈 로그 변환:\")\n",
    "\n",
    "log_transform_cols = []\n",
    "for col in ['monthly_charges', 'total_charges', 'tenure_months']:\n",
    "    if col in df.columns:\n",
    "        # 치우침 정도 확인 (왜도)\n",
    "        skewness = df[col].skew()\n",
    "        if abs(skewness) > 1:  # 치우침이 심한 경우\n",
    "            df[f'{col}_log'] = np.log1p(df[col])  # log1p는 log(1+x)로 0값 처리\n",
    "            log_transform_cols.append(col)\n",
    "            print(f\"  ✅ {col}_log: {col}의 로그 변환 (원본 왜도: {skewness:.2f})\")\n",
    "\n",
    "# 7. 생성된 피처 요약\n",
    "print(f\"\\n📊 피처 엔지니어링 결과:\")\n",
    "new_features = [col for col in df.columns if col not in df_original.columns]\n",
    "print(f\"  • 새로 생성된 피처 수: {len(new_features)}\")\n",
    "print(f\"  • 전체 피처 수: {df.shape[1]} (기존 {df_original.shape[1]}개 + 신규 {len(new_features)}개)\")\n",
    "\n",
    "if new_features:\n",
    "    print(f\"  • 새로운 피처 목록:\")\n",
    "    for i, feature in enumerate(new_features, 1):\n",
    "        print(f\"    {i:2d}. {feature}\")\n",
    "\n",
    "# 데이터 타입별 피처 분포\n",
    "print(f\"\\n📋 피처 타입 분포:\")\n",
    "dtype_counts = df.dtypes.value_counts()\n",
    "for dtype, count in dtype_counts.items():\n",
    "    print(f\"  • {dtype}: {count}개\")\n",
    "\n",
    "print(f\"\\n✅ 피처 엔지니어링 완료!\")\n",
    "print(f\"🔍 업데이트된 데이터 미리보기:\")\n",
    "display(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f92e7d3",
   "metadata": {},
   "source": [
    "## 5. 인코딩\n",
    "\n",
    "범주형 변수를 머신러닝 모델이 처리할 수 있는 수치형으로 변환합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4e64449",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"🔤 범주형 변수 인코딩 시작\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# 인코딩 전 데이터 백업\n",
    "df_before_encoding = df.copy()\n",
    "\n",
    "# 1. 범주형 변수 식별\n",
    "categorical_columns = df.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "print(f\"📋 범주형 변수 ({len(categorical_columns)}개):\")\n",
    "for i, col in enumerate(categorical_columns, 1):\n",
    "    unique_values = df[col].nunique()\n",
    "    print(f\"  {i:2d}. {col}: {unique_values}개 고유값\")\n",
    "\n",
    "# customer_id는 제외 (식별자이므로)\n",
    "if 'customer_id' in categorical_columns:\n",
    "    categorical_columns.remove('customer_id')\n",
    "    print(f\"  ℹ️ customer_id는 인코딩에서 제외\")\n",
    "\n",
    "# 2. 이진 인코딩 (Binary/Boolean 변수들)\n",
    "print(f\"\\n🔢 이진 인코딩:\")\n",
    "binary_columns = []\n",
    "\n",
    "for col in categorical_columns:\n",
    "    unique_vals = df[col].unique()\n",
    "    if len(unique_vals) == 2:\n",
    "        binary_columns.append(col)\n",
    "        # Yes/No, Male/Female 등을 0/1로 변환\n",
    "        if 'Yes' in unique_vals and 'No' in unique_vals:\n",
    "            df[col] = df[col].map({'Yes': 1, 'No': 0})\n",
    "        elif 'Male' in unique_vals and 'Female' in unique_vals:\n",
    "            df[col] = df[col].map({'Male': 1, 'Female': 0})\n",
    "        else:\n",
    "            # 기타 이진 변수는 LabelEncoder 사용\n",
    "            le = LabelEncoder()\n",
    "            df[col] = le.fit_transform(df[col])\n",
    "        \n",
    "        print(f\"  ✅ {col}: {unique_vals} → 0/1\")\n",
    "\n",
    "# 3. 서수 인코딩 (Ordinal Encoding) - 순서가 있는 범주형 변수\n",
    "print(f\"\\n📊 서수 인코딩:\")\n",
    "\n",
    "ordinal_mappings = {}\n",
    "\n",
    "# 계약 안정성 (이미 처리됨)\n",
    "if 'contract_type' in df.columns and 'contract_stability' not in df.columns:\n",
    "    contract_order = {'Month-to-month': 1, 'One year': 2, 'Two year': 3}\n",
    "    df['contract_stability'] = df['contract_type'].map(contract_order)\n",
    "    ordinal_mappings['contract_type'] = contract_order\n",
    "    print(f\"  ✅ contract_type → contract_stability: {contract_order}\")\n",
    "\n",
    "# 인터넷 서비스 품질 (이미 처리됨)\n",
    "if 'internet_service' in df.columns and 'internet_score' not in df.columns:\n",
    "    internet_order = {'No': 0, 'DSL': 1, 'Fiber optic': 2}\n",
    "    df['internet_score'] = df['internet_service'].map(internet_order)\n",
    "    ordinal_mappings['internet_service'] = internet_order\n",
    "    print(f\"  ✅ internet_service → internet_score: {internet_order}\")\n",
    "\n",
    "# 4. 원-핫 인코딩 (Nominal 변수들)\n",
    "print(f\"\\n🎯 원-핫 인코딩:\")\n",
    "\n",
    "# 원-핫 인코딩할 변수들 (카디널리티가 낮은 명목형 변수)\n",
    "onehot_columns = []\n",
    "for col in categorical_columns:\n",
    "    if col not in binary_columns:\n",
    "        unique_count = df[col].nunique()\n",
    "        if unique_count <= 10:  # 10개 이하의 카테고리만 원-핫 인코딩\n",
    "            onehot_columns.append(col)\n",
    "\n",
    "print(f\"  📋 원-핫 인코딩 대상 ({len(onehot_columns)}개):\")\n",
    "for col in onehot_columns:\n",
    "    print(f\"    • {col}: {df[col].nunique()}개 카테고리\")\n",
    "\n",
    "# 원-핫 인코딩 수행\n",
    "if onehot_columns:\n",
    "    # pandas get_dummies 사용\n",
    "    df_encoded = pd.get_dummies(df, columns=onehot_columns, prefix=onehot_columns, \n",
    "                               prefix_sep='_', drop_first=False)\n",
    "    \n",
    "    # 원래 컬럼들 제거\n",
    "    df_encoded = df_encoded.drop(columns=onehot_columns)\n",
    "    \n",
    "    print(f\"  ✅ 원-핫 인코딩 완료\")\n",
    "    print(f\"    - 생성된 더미 변수: {len(df_encoded.columns) - len(df.columns)}개\")\n",
    "    \n",
    "    df = df_encoded.copy()\n",
    "else:\n",
    "    print(f\"  ℹ️ 원-핫 인코딩할 변수가 없습니다.\")\n",
    "\n",
    "# 5. 고카디널리티 변수 처리 (카테고리가 많은 변수)\n",
    "print(f\"\\n🔍 고카디널리티 변수 처리:\")\n",
    "\n",
    "high_cardinality_cols = []\n",
    "for col in categorical_columns:\n",
    "    if col not in binary_columns and col not in onehot_columns:\n",
    "        unique_count = df[col].nunique()\n",
    "        if unique_count > 10:\n",
    "            high_cardinality_cols.append(col)\n",
    "\n",
    "if high_cardinality_cols:\n",
    "    print(f\"  📋 고카디널리티 변수 ({len(high_cardinality_cols)}개):\")\n",
    "    for col in high_cardinality_cols:\n",
    "        unique_count = df[col].nunique()\n",
    "        print(f\"    • {col}: {unique_count}개 카테고리\")\n",
    "        \n",
    "        # 타겟 인코딩 또는 빈도 인코딩 적용\n",
    "        if 'churn' in df.columns:\n",
    "            # 타겟 인코딩 (이탈률 기반)\n",
    "            target_encoding = df.groupby(col)['churn'].mean()\n",
    "            df[f'{col}_target_encoded'] = df[col].map(target_encoding)\n",
    "            print(f\"      ✅ 타겟 인코딩 적용 → {col}_target_encoded\")\n",
    "        \n",
    "        # 빈도 인코딩\n",
    "        frequency_encoding = df[col].value_counts()\n",
    "        df[f'{col}_frequency'] = df[col].map(frequency_encoding)\n",
    "        print(f\"      ✅ 빈도 인코딩 적용 → {col}_frequency\")\n",
    "        \n",
    "        # 원본 컬럼 제거\n",
    "        df = df.drop(columns=[col])\n",
    "else:\n",
    "    print(f\"  ℹ️ 고카디널리티 변수가 없습니다.\")\n",
    "\n",
    "# 6. 최종 범주형 변수 처리 확인\n",
    "remaining_categorical = df.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "if 'customer_id' in remaining_categorical:\n",
    "    remaining_categorical.remove('customer_id')\n",
    "\n",
    "if remaining_categorical:\n",
    "    print(f\"\\n⚠️ 처리되지 않은 범주형 변수:\")\n",
    "    for col in remaining_categorical:\n",
    "        print(f\"  • {col}: {df[col].nunique()}개 고유값\")\n",
    "        # 남은 범주형 변수는 라벨 인코딩으로 처리\n",
    "        le = LabelEncoder()\n",
    "        df[col] = le.fit_transform(df[col].astype(str))\n",
    "        print(f\"    ✅ 라벨 인코딩 적용\")\n",
    "\n",
    "# 7. 인코딩 결과 요약\n",
    "print(f\"\\n📊 인코딩 결과 요약:\")\n",
    "print(f\"  • 인코딩 전 컬럼 수: {df_before_encoding.shape[1]}\")\n",
    "print(f\"  • 인코딩 후 컬럼 수: {df.shape[1]}\")\n",
    "print(f\"  • 추가된 컬럼 수: {df.shape[1] - df_before_encoding.shape[1]}\")\n",
    "\n",
    "# 데이터 타입 분포\n",
    "numeric_cols = len(df.select_dtypes(include=[np.number]).columns)\n",
    "categorical_cols = len(df.select_dtypes(include=['object', 'category']).columns)\n",
    "\n",
    "print(f\"  • 수치형 변수: {numeric_cols}개\")\n",
    "print(f\"  • 범주형 변수: {categorical_cols}개\")\n",
    "\n",
    "# 타겟 변수 확인\n",
    "if 'churn' in df.columns:\n",
    "    print(f\"  • 타겟 변수 (churn): {df['churn'].dtype}\")\n",
    "    print(f\"    - 클래스 분포: {df['churn'].value_counts().to_dict()}\")\n",
    "\n",
    "print(f\"\\n✅ 모든 인코딩 완료!\")\n",
    "\n",
    "# 인코딩된 데이터 미리보기\n",
    "print(f\"\\n🔍 인코딩된 데이터 미리보기:\")\n",
    "display(df.head())\n",
    "\n",
    "# 컬럼명 정리 (공백이나 특수문자 제거)\n",
    "df.columns = df.columns.str.replace(' ', '_').str.replace('-', '_').str.lower()\n",
    "print(f\"\\n🔧 컬럼명 정규화 완료 (소문자, 언더스코어 사용)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4099269e",
   "metadata": {},
   "source": [
    "## 6. 데이터 분할 및 저장\n",
    "\n",
    "전처리가 완료된 데이터를 훈련/검증/테스트 세트로 분할하고 저장합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f057271",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"📊 데이터 분할 및 저장\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# 1. 피처와 타겟 분리\n",
    "if 'churn' in df.columns:\n",
    "    X = df.drop(['churn'], axis=1)\n",
    "    y = df['churn']\n",
    "    \n",
    "    # customer_id가 있으면 제거 (모델링에 불필요)\n",
    "    if 'customer_id' in X.columns:\n",
    "        customer_ids = X['customer_id'].copy()\n",
    "        X = X.drop(['customer_id'], axis=1)\n",
    "    else:\n",
    "        customer_ids = None\n",
    "    \n",
    "    print(f\"✅ 피처-타겟 분리 완료:\")\n",
    "    print(f\"  • 피처 수 (X): {X.shape[1]}\")\n",
    "    print(f\"  • 타겟 변수 (y): {y.name}\")\n",
    "    print(f\"  • 총 샘플 수: {len(X)}\")\n",
    "else:\n",
    "    print(\"❌ 타겟 변수 'churn'이 없습니다. 전체 데이터를 피처로 처리합니다.\")\n",
    "    X = df.copy()\n",
    "    if 'customer_id' in X.columns:\n",
    "        customer_ids = X['customer_id'].copy()\n",
    "        X = X.drop(['customer_id'], axis=1)\n",
    "    else:\n",
    "        customer_ids = None\n",
    "    y = None\n",
    "\n",
    "# 2. 데이터 분할 (훈련:검증:테스트 = 70:15:15)\n",
    "if y is not None:\n",
    "    print(f\"\\n🔄 데이터 분할 시작:\")\n",
    "    print(f\"  분할 비율: 훈련(70%) : 검증(15%) : 테스트(15%)\")\n",
    "    \n",
    "    # 먼저 훈련+검증 vs 테스트로 분할 (85:15)\n",
    "    X_temp, X_test, y_temp, y_test = train_test_split(\n",
    "        X, y, test_size=0.15, random_state=RANDOM_STATE, stratify=y\n",
    "    )\n",
    "    \n",
    "    # 다시 훈련 vs 검증으로 분할 (70:15 = 82.35:17.65)\n",
    "    X_train, X_val, y_train, y_val = train_test_split(\n",
    "        X_temp, y_temp, test_size=0.176, random_state=RANDOM_STATE, stratify=y_temp\n",
    "    )\n",
    "    \n",
    "    print(f\"✅ 데이터 분할 완료:\")\n",
    "    print(f\"  • 훈련 세트: {len(X_train):,}개 ({len(X_train)/len(X)*100:.1f}%)\")\n",
    "    print(f\"  • 검증 세트: {len(X_val):,}개 ({len(X_val)/len(X)*100:.1f}%)\")\n",
    "    print(f\"  • 테스트 세트: {len(X_test):,}개 ({len(X_test)/len(X)*100:.1f}%)\")\n",
    "    \n",
    "    # 각 세트의 타겟 분포 확인\n",
    "    print(f\"\\n📊 각 세트별 이탈률:\")\n",
    "    print(f\"  • 전체: {y.mean():.3f}\")\n",
    "    print(f\"  • 훈련: {y_train.mean():.3f}\")\n",
    "    print(f\"  • 검증: {y_val.mean():.3f}\")\n",
    "    print(f\"  • 테스트: {y_test.mean():.3f}\")\n",
    "    \n",
    "    # customer_id도 함께 분할 (있는 경우)\n",
    "    if customer_ids is not None:\n",
    "        ids_temp = customer_ids.iloc[X_temp.index]\n",
    "        ids_test = customer_ids.iloc[X_test.index]\n",
    "        ids_train = ids_temp.iloc[X_train.index]\n",
    "        ids_val = ids_temp.iloc[X_val.index]\n",
    "else:\n",
    "    print(\"⚠️ 타겟 변수가 없어 분할을 건너뜁니다.\")\n",
    "    X_train, X_val, X_test = X, None, None\n",
    "    y_train, y_val, y_test = None, None, None\n",
    "\n",
    "# 3. 스케일링 (훈련 세트 기준으로 fit, 모든 세트에 transform)\n",
    "print(f\"\\n⚖️ 피처 스케일링:\")\n",
    "\n",
    "# 수치형 컬럼 식별\n",
    "numeric_features = X_train.select_dtypes(include=[np.number]).columns.tolist()\n",
    "print(f\"  • 스케일링 대상: {len(numeric_features)}개 수치형 피처\")\n",
    "\n",
    "if len(numeric_features) > 0:\n",
    "    # StandardScaler 적용\n",
    "    scaler = StandardScaler()\n",
    "    \n",
    "    # 훈련 세트로 scaler 학습\n",
    "    X_train_scaled = X_train.copy()\n",
    "    X_train_scaled[numeric_features] = scaler.fit_transform(X_train[numeric_features])\n",
    "    \n",
    "    if X_val is not None:\n",
    "        X_val_scaled = X_val.copy()\n",
    "        X_val_scaled[numeric_features] = scaler.transform(X_val[numeric_features])\n",
    "    else:\n",
    "        X_val_scaled = None\n",
    "    \n",
    "    if X_test is not None:\n",
    "        X_test_scaled = X_test.copy()\n",
    "        X_test_scaled[numeric_features] = scaler.transform(X_test[numeric_features])\n",
    "    else:\n",
    "        X_test_scaled = None\n",
    "    \n",
    "    print(f\"  ✅ StandardScaler 적용 완료\")\n",
    "    print(f\"    - 평균: 0, 표준편차: 1로 정규화\")\n",
    "    \n",
    "    # 스케일링 전후 비교 (첫 번째 수치형 피처)\n",
    "    if len(numeric_features) > 0:\n",
    "        sample_feature = numeric_features[0]\n",
    "        print(f\"    - 예시 ({sample_feature}):\")\n",
    "        print(f\"      원본: 평균={X_train[sample_feature].mean():.2f}, 표준편차={X_train[sample_feature].std():.2f}\")\n",
    "        print(f\"      변환: 평균={X_train_scaled[sample_feature].mean():.2f}, 표준편차={X_train_scaled[sample_feature].std():.2f}\")\n",
    "else:\n",
    "    print(\"  ℹ️ 스케일링할 수치형 피처가 없습니다.\")\n",
    "    X_train_scaled = X_train.copy()\n",
    "    X_val_scaled = X_val.copy() if X_val is not None else None\n",
    "    X_test_scaled = X_test.copy() if X_test is not None else None\n",
    "    scaler = None\n",
    "\n",
    "# 4. 전처리된 데이터 저장\n",
    "print(f\"\\n💾 전처리된 데이터 저장:\")\n",
    "\n",
    "# 저장 경로 설정\n",
    "save_paths = {\n",
    "    'train': os.path.join(processed_data_path, 'train_data.csv'),\n",
    "    'val': os.path.join(processed_data_path, 'val_data.csv'),\n",
    "    'test': os.path.join(processed_data_path, 'test_data.csv'),\n",
    "    'train_scaled': os.path.join(processed_data_path, 'train_data_scaled.csv'),\n",
    "    'val_scaled': os.path.join(processed_data_path, 'val_data_scaled.csv'),\n",
    "    'test_scaled': os.path.join(processed_data_path, 'test_data_scaled.csv'),\n",
    "    'scaler': os.path.join(processed_data_path, 'scaler.pkl'),\n",
    "    'feature_names': os.path.join(processed_data_path, 'feature_names.pkl')\n",
    "}\n",
    "\n",
    "# 훈련 세트 저장\n",
    "if y_train is not None:\n",
    "    train_df = X_train.copy()\n",
    "    train_df['churn'] = y_train\n",
    "    train_df.to_csv(save_paths['train'], index=False)\n",
    "    \n",
    "    train_scaled_df = X_train_scaled.copy()\n",
    "    train_scaled_df['churn'] = y_train\n",
    "    train_scaled_df.to_csv(save_paths['train_scaled'], index=False)\n",
    "    \n",
    "    print(f\"  ✅ 훈련 세트 저장: {save_paths['train']}\")\n",
    "    print(f\"  ✅ 훈련 세트 (스케일링) 저장: {save_paths['train_scaled']}\")\n",
    "\n",
    "# 검증 세트 저장\n",
    "if y_val is not None:\n",
    "    val_df = X_val.copy()\n",
    "    val_df['churn'] = y_val\n",
    "    val_df.to_csv(save_paths['val'], index=False)\n",
    "    \n",
    "    val_scaled_df = X_val_scaled.copy()\n",
    "    val_scaled_df['churn'] = y_val\n",
    "    val_scaled_df.to_csv(save_paths['val_scaled'], index=False)\n",
    "    \n",
    "    print(f\"  ✅ 검증 세트 저장: {save_paths['val']}\")\n",
    "    print(f\"  ✅ 검증 세트 (스케일링) 저장: {save_paths['val_scaled']}\")\n",
    "\n",
    "# 테스트 세트 저장\n",
    "if y_test is not None:\n",
    "    test_df = X_test.copy()\n",
    "    test_df['churn'] = y_test\n",
    "    test_df.to_csv(save_paths['test'], index=False)\n",
    "    \n",
    "    test_scaled_df = X_test_scaled.copy()\n",
    "    test_scaled_df['churn'] = y_test\n",
    "    test_scaled_df.to_csv(save_paths['test_scaled'], index=False)\n",
    "    \n",
    "    print(f\"  ✅ 테스트 세트 저장: {save_paths['test']}\")\n",
    "    print(f\"  ✅ 테스트 세트 (스케일링) 저장: {save_paths['test_scaled']}\")\n",
    "\n",
    "# 스케일러 저장\n",
    "if scaler is not None:\n",
    "    with open(save_paths['scaler'], 'wb') as f:\n",
    "        pickle.dump(scaler, f)\n",
    "    print(f\"  ✅ 스케일러 저장: {save_paths['scaler']}\")\n",
    "\n",
    "# 피처 이름 저장\n",
    "feature_names = X_train.columns.tolist()\n",
    "with open(save_paths['feature_names'], 'wb') as f:\n",
    "    pickle.dump(feature_names, f)\n",
    "print(f\"  ✅ 피처 이름 저장: {save_paths['feature_names']}\")\n",
    "\n",
    "# 5. 전처리 요약 저장\n",
    "summary_path = os.path.join(processed_data_path, 'preprocessing_summary.txt')\n",
    "with open(summary_path, 'w', encoding='utf-8') as f:\n",
    "    f.write(\"고객 이탈 예측 - 데이터 전처리 요약\\\\n\")\n",
    "    f.write(\"=\" * 50 + \"\\\\n\\\\n\")\n",
    "    f.write(f\"전처리 완료 시간: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\\\n\\\\n\")\n",
    "    \n",
    "    f.write(f\"데이터 정보:\\\\n\")\n",
    "    f.write(f\"  • 원본 샘플 수: {len(df_original):,}\\\\n\")\n",
    "    f.write(f\"  • 최종 샘플 수: {len(df):,}\\\\n\")\n",
    "    f.write(f\"  • 총 피처 수: {len(feature_names)}\\\\n\")\n",
    "    \n",
    "    if y is not None:\n",
    "        f.write(f\"  • 이탈률: {y.mean():.3f}\\\\n\")\n",
    "        f.write(f\"\\\\n데이터 분할:\\\\n\")\n",
    "        f.write(f\"  • 훈련 세트: {len(X_train):,}개\\\\n\")\n",
    "        if X_val is not None:\n",
    "            f.write(f\"  • 검증 세트: {len(X_val):,}개\\\\n\")\n",
    "        if X_test is not None:\n",
    "            f.write(f\"  • 테스트 세트: {len(X_test):,}개\\\\n\")\n",
    "    \n",
    "    f.write(f\"\\\\n생성된 파일:\\\\n\")\n",
    "    for key, path in save_paths.items():\n",
    "        if os.path.exists(path):\n",
    "            f.write(f\"  • {key}: {os.path.basename(path)}\\\\n\")\n",
    "\n",
    "print(f\"  ✅ 전처리 요약 저장: {summary_path}\")\n",
    "\n",
    "# 6. 최종 결과 요약\n",
    "print(f\"\\\\n🎉 데이터 전처리 완료!\")\n",
    "print(f\"=\" * 60)\n",
    "print(f\"📊 최종 결과 요약:\")\n",
    "print(f\"  • 처리된 총 샘플 수: {len(df):,}\")\n",
    "print(f\"  • 최종 피처 수: {len(feature_names)}\")\n",
    "print(f\"  • 저장된 파일 수: {len([p for p in save_paths.values() if os.path.exists(p)])}\")\n",
    "print(f\"  • 저장 위치: {processed_data_path}\")\n",
    "\n",
    "if y is not None:\n",
    "    print(f\"\\\\n🎯 다음 단계:\")\n",
    "    print(f\"  1. 03_Modeling.ipynb에서 모델 학습 진행\")\n",
    "    print(f\"  2. 저장된 전처리 데이터 활용\")\n",
    "    print(f\"  3. 스케일러를 사용한 새 데이터 전처리 가능\")\n",
    "\n",
    "print(f\"\\\\n✅ 전처리 프로세스 성공적으로 완료!\")\n",
    "print(f\"🕐 완료 시간: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print(\"=\" * 60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "abc-bootcamp-FP-2025",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

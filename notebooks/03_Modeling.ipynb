{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cf93245c",
   "metadata": {},
   "source": [
    "# 03. ë¨¸ì‹ ëŸ¬ë‹ ëª¨ë¸ë§\n",
    "\n",
    "ì´ ë…¸íŠ¸ë¶ì—ì„œëŠ” ì „ì²˜ë¦¬ëœ ë°ì´í„°ë¥¼ ì‚¬ìš©í•˜ì—¬ ë‹¤ì–‘í•œ ë¨¸ì‹ ëŸ¬ë‹ ëª¨ë¸ì„ êµ¬ì¶•í•˜ê³  í•™ìŠµì‹œí‚µë‹ˆë‹¤.\n",
    "\n",
    "## ğŸ¯ ëª©í‘œ\n",
    "- ê³ ê° ì´íƒˆ ì˜ˆì¸¡ ëª¨ë¸ ê°œë°œ\n",
    "- ì—¬ëŸ¬ ì•Œê³ ë¦¬ì¦˜ ë¹„êµ ë° ì„±ëŠ¥ í‰ê°€\n",
    "- ìµœì  í•˜ì´í¼íŒŒë¼ë¯¸í„° íƒìƒ‰\n",
    "- ëª¨ë¸ í•´ì„ ë° í”¼ì²˜ ì¤‘ìš”ë„ ë¶„ì„\n",
    "- ìµœì¢… ëª¨ë¸ ì €ì¥\n",
    "\n",
    "## ğŸ“‹ ëª¨ë¸ë§ íŒŒì´í”„ë¼ì¸\n",
    "1. ë°ì´í„° ë¡œë“œ ë° í™•ì¸\n",
    "2. í”¼ì²˜ ì—”ì§€ë‹ˆì–´ë§\n",
    "3. í•™ìŠµ/ê²€ì¦/í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¶„í• \n",
    "4. ë² ì´ìŠ¤ë¼ì¸ ëª¨ë¸ êµ¬ì¶•\n",
    "5. ë‹¤ì–‘í•œ ì•Œê³ ë¦¬ì¦˜ ì‹œë„\n",
    "6. ëª¨ë¸ ì„±ëŠ¥ ë¹„êµ\n",
    "7. í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹\n",
    "8. ìµœì¢… ëª¨ë¸ ì„ íƒ ë° ì €ì¥"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35d2d6cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# í•„ìˆ˜ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "import joblib\n",
    "import json\n",
    "\n",
    "# ë¨¸ì‹ ëŸ¬ë‹ ë¼ì´ë¸ŒëŸ¬ë¦¬\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV, StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, VotingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "# ëª¨ë¸ í‰ê°€\n",
    "from sklearn.metrics import (classification_report, confusion_matrix, roc_auc_score, \n",
    "                           roc_curve, precision_recall_curve, f1_score, accuracy_score)\n",
    "\n",
    "# ì‹œê°í™”\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "# ì„¤ì •\n",
    "warnings.filterwarnings('ignore')\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# ëœë¤ ì‹œë“œ ê³ ì •\n",
    "RANDOM_STATE = 42\n",
    "np.random.seed(RANDOM_STATE)\n",
    "\n",
    "print(\"âœ… ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸ ì™„ë£Œ!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be20b37a",
   "metadata": {},
   "source": [
    "## 1. ë°ì´í„° ë¡œë“œ ë° í™•ì¸\n",
    "\n",
    "ì „ì²˜ë¦¬ëœ ë°ì´í„°ë¥¼ ë¡œë“œí•˜ê³  ëª¨ë¸ë§ì„ ìœ„í•œ ì¤€ë¹„ë¥¼ í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81e16f91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì „ì²˜ë¦¬ëœ ë°ì´í„° ë¡œë“œ\n",
    "data_path = Path(\"../data/processed\")\n",
    "\n",
    "# ì „ì²˜ë¦¬ëœ ë°ì´í„°ê°€ ìˆëŠ”ì§€ í™•ì¸\n",
    "if (data_path / \"train_features.csv\").exists():\n",
    "    print(\"ğŸ“Š ì „ì²˜ë¦¬ëœ ë°ì´í„° ë¡œë“œ ì¤‘...\")\n",
    "    X_train = pd.read_csv(data_path / \"train_features.csv\")\n",
    "    X_test = pd.read_csv(data_path / \"test_features.csv\")\n",
    "    y_train = pd.read_csv(data_path / \"train_target.csv\").values.ravel()\n",
    "    y_test = pd.read_csv(data_path / \"test_target.csv\").values.ravel()\n",
    "    \n",
    "    print(f\"âœ… ì „ì²˜ë¦¬ëœ ë°ì´í„° ë¡œë“œ ì™„ë£Œ!\")\n",
    "    print(f\"â€¢ í•™ìŠµ ë°ì´í„°: {X_train.shape}\")\n",
    "    print(f\"â€¢ í…ŒìŠ¤íŠ¸ ë°ì´í„°: {X_test.shape}\")\n",
    "    print(f\"â€¢ íƒ€ê²Ÿ ë¶„í¬ (í•™ìŠµ): {np.unique(y_train, return_counts=True)}\")\n",
    "    \n",
    "else:\n",
    "    # ì „ì²˜ë¦¬ëœ ë°ì´í„°ê°€ ì—†ìœ¼ë©´ EDA ë°ì´í„°ë¥¼ ì‚¬ìš©\n",
    "    print(\"âš ï¸ ì „ì²˜ë¦¬ëœ ë°ì´í„°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. EDA ë°ì´í„°ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.\")\n",
    "    print(\"ğŸ’¡ 02_Preprocessing.ipynbë¥¼ ë¨¼ì € ì‹¤í–‰í•˜ëŠ” ê²ƒì„ ê¶Œì¥í•©ë‹ˆë‹¤.\")\n",
    "    \n",
    "    # EDAì—ì„œ ìƒì„±í•œ ë°ì´í„° ë¡œë“œ\n",
    "    if (data_path / \"customer_data_with_eda_features.csv\").exists():\n",
    "        df = pd.read_csv(data_path / \"customer_data_with_eda_features.csv\")\n",
    "        print(f\"ğŸ“Š EDA ë°ì´í„° ë¡œë“œ: {df.shape}\")\n",
    "    else:\n",
    "        # ìƒ˜í”Œ ë°ì´í„° ì¬ìƒì„±\n",
    "        print(\"ğŸ”„ ìƒ˜í”Œ ë°ì´í„° ìƒì„± ì¤‘...\")\n",
    "        np.random.seed(RANDOM_STATE)\n",
    "        n_samples = 1000\n",
    "        \n",
    "        sample_data = {\n",
    "            'customer_id': range(1, n_samples + 1),\n",
    "            'age': np.random.normal(35, 12, n_samples).astype(int),\n",
    "            'income': np.random.lognormal(10.5, 0.5, n_samples).astype(int),\n",
    "            'spending_score': np.random.beta(2, 5, n_samples) * 100,\n",
    "            'tenure_months': np.random.exponential(24, n_samples).astype(int),\n",
    "            'num_purchases': np.random.poisson(15, n_samples),\n",
    "            'category': np.random.choice(['Premium', 'Standard', 'Basic'], n_samples, p=[0.2, 0.5, 0.3]),\n",
    "            'region': np.random.choice(['North', 'South', 'East', 'West'], n_samples),\n",
    "            'is_churned': np.random.choice([0, 1], n_samples, p=[0.8, 0.2])\n",
    "        }\n",
    "        \n",
    "        df = pd.DataFrame(sample_data)\n",
    "        df['age'] = np.clip(df['age'], 18, 80)\n",
    "        df['income'] = np.clip(df['income'], 20000, 200000)\n",
    "        \n",
    "        print(f\"âœ… ìƒ˜í”Œ ë°ì´í„° ìƒì„± ì™„ë£Œ: {df.shape}\")\n",
    "\n",
    "print(f\"\\nğŸ“‹ ë°ì´í„° ê¸°ë³¸ ì •ë³´:\")\n",
    "if 'df' in locals():\n",
    "    print(df.info())\n",
    "    display(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcc883d0",
   "metadata": {},
   "source": [
    "## 2. í”¼ì²˜ ì—”ì§€ë‹ˆì–´ë§ ë° ë°ì´í„° ì¤€ë¹„\n",
    "\n",
    "ëª¨ë¸ë§ì„ ìœ„í•œ í”¼ì²˜ ì—”ì§€ë‹ˆì–´ë§ê³¼ ë°ì´í„° ì „ì²˜ë¦¬ë¥¼ ìˆ˜í–‰í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99732f39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì „ì²˜ë¦¬ëœ ë°ì´í„°ê°€ ì—†ëŠ” ê²½ìš°ë¥¼ ìœ„í•œ í”¼ì²˜ ì—”ì§€ë‹ˆì–´ë§\n",
    "if 'df' in locals():\n",
    "    print(\"ğŸ”§ í”¼ì²˜ ì—”ì§€ë‹ˆì–´ë§ ìˆ˜í–‰ ì¤‘...\")\n",
    "    \n",
    "    # 1. ê²°ì¸¡ê°’ ì²˜ë¦¬\n",
    "    df['income'].fillna(df['income'].median(), inplace=True)\n",
    "    \n",
    "    # 2. ìƒˆë¡œìš´ í”¼ì²˜ ìƒì„±\n",
    "    df['income_per_age'] = df['income'] / df['age']\n",
    "    df['spending_per_tenure'] = df['spending_score'] / (df['tenure_months'] + 1)\n",
    "    df['purchases_per_month'] = df['num_purchases'] / (df['tenure_months'] + 1)\n",
    "    \n",
    "    # 3. ë‚˜ì´ ê·¸ë£¹ ìƒì„±\n",
    "    df['age_group'] = pd.cut(df['age'], \n",
    "                            bins=[0, 25, 35, 45, 55, 100], \n",
    "                            labels=['18-25', '26-35', '36-45', '46-55', '56+'])\n",
    "    \n",
    "    # 4. ì†Œë“ êµ¬ê°„ ìƒì„±\n",
    "    df['income_level'] = pd.cut(df['income'], \n",
    "                               bins=[0, 40000, 70000, 100000, float('inf')], \n",
    "                               labels=['Low', 'Medium', 'High', 'Premium'])\n",
    "    \n",
    "    print(\"âœ… í”¼ì²˜ ì—”ì§€ë‹ˆì–´ë§ ì™„ë£Œ!\")\n",
    "    print(f\"â€¢ ìƒˆë¡œìš´ í”¼ì²˜ ê°œìˆ˜: 5ê°œ\")\n",
    "    print(f\"â€¢ ìµœì¢… ë°ì´í„° í¬ê¸°: {df.shape}\")\n",
    "    \n",
    "    # ë²”ì£¼í˜• ë³€ìˆ˜ ì¸ì½”ë”©\n",
    "    print(\"\\nğŸ·ï¸ ë²”ì£¼í˜• ë³€ìˆ˜ ì¸ì½”ë”© ì¤‘...\")\n",
    "    categorical_columns = ['category', 'region', 'age_group', 'income_level']\n",
    "    \n",
    "    # LabelEncoder ì‚¬ìš©\n",
    "    label_encoders = {}\n",
    "    for col in categorical_columns:\n",
    "        le = LabelEncoder()\n",
    "        df[f'{col}_encoded'] = le.fit_transform(df[col].astype(str))\n",
    "        label_encoders[col] = le\n",
    "    \n",
    "    # ì›-í•« ì¸ì½”ë”©\n",
    "    df_encoded = pd.get_dummies(df, columns=categorical_columns, prefix=categorical_columns)\n",
    "    \n",
    "    print(f\"âœ… ì¸ì½”ë”© ì™„ë£Œ! ìµœì¢… í”¼ì²˜ ìˆ˜: {df_encoded.shape[1]}\")\n",
    "    \n",
    "    # íƒ€ê²Ÿ ë³€ìˆ˜ì™€ í”¼ì²˜ ë¶„ë¦¬\n",
    "    target_col = 'is_churned'\n",
    "    feature_cols = [col for col in df_encoded.columns if col not in [\n",
    "        'customer_id', target_col, 'category', 'region', 'age_group', 'income_level'\n",
    "    ]]\n",
    "    \n",
    "    X = df_encoded[feature_cols]\n",
    "    y = df_encoded[target_col]\n",
    "    \n",
    "    print(f\"\\nğŸ“Š ìµœì¢… ë°ì´í„° ì¤€ë¹„:\")\n",
    "    print(f\"â€¢ í”¼ì²˜ ìˆ˜: {X.shape[1]}\")\n",
    "    print(f\"â€¢ ìƒ˜í”Œ ìˆ˜: {X.shape[0]}\")\n",
    "    print(f\"â€¢ íƒ€ê²Ÿ ë¶„í¬: {y.value_counts().to_dict()}\")\n",
    "    \n",
    "    # í•™ìŠµ/í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¶„í• \n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.2, random_state=RANDOM_STATE, stratify=y\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nğŸ”„ ë°ì´í„° ë¶„í•  ì™„ë£Œ:\")\n",
    "    print(f\"â€¢ í•™ìŠµ ë°ì´í„°: {X_train.shape}\")\n",
    "    print(f\"â€¢ í…ŒìŠ¤íŠ¸ ë°ì´í„°: {X_test.shape}\")\n",
    "\n",
    "# í”¼ì²˜ ìŠ¤ì¼€ì¼ë§\n",
    "print(\"\\nâš–ï¸ í”¼ì²˜ ìŠ¤ì¼€ì¼ë§ ìˆ˜í–‰ ì¤‘...\")\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "print(\"âœ… ìŠ¤ì¼€ì¼ë§ ì™„ë£Œ!\")\n",
    "print(f\"â€¢ í•™ìŠµ ë°ì´í„° í‰ê· : {X_train_scaled.mean():.4f}\")\n",
    "print(f\"â€¢ í•™ìŠµ ë°ì´í„° í‘œì¤€í¸ì°¨: {X_train_scaled.std():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d85533a",
   "metadata": {},
   "source": [
    "## 3. ë² ì´ìŠ¤ë¼ì¸ ëª¨ë¸ êµ¬ì¶•\n",
    "\n",
    "ê°„ë‹¨í•œ ë² ì´ìŠ¤ë¼ì¸ ëª¨ë¸ì„ êµ¬ì¶•í•˜ì—¬ ê¸°ì¤€ ì„±ëŠ¥ì„ ì„¤ì •í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe24da77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ë² ì´ìŠ¤ë¼ì¸ ëª¨ë¸ - ë‹¤ìˆ˜ í´ë˜ìŠ¤ ì˜ˆì¸¡\n",
    "from collections import Counter\n",
    "\n",
    "print(\"ğŸ“Š ë² ì´ìŠ¤ë¼ì¸ ëª¨ë¸ í‰ê°€\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# 1. ë‹¤ìˆ˜ í´ë˜ìŠ¤ ì˜ˆì¸¡ (Majority Class Baseline)\n",
    "majority_class = Counter(y_train).most_common(1)[0][0]\n",
    "y_pred_baseline = np.full(len(y_test), majority_class)\n",
    "\n",
    "baseline_accuracy = accuracy_score(y_test, y_pred_baseline)\n",
    "print(f\"ğŸ¯ ë‹¤ìˆ˜ í´ë˜ìŠ¤ ë² ì´ìŠ¤ë¼ì¸ ì •í™•ë„: {baseline_accuracy:.4f}\")\n",
    "\n",
    "# 2. ë¡œì§€ìŠ¤í‹± íšŒê·€ ë² ì´ìŠ¤ë¼ì¸\n",
    "print(\"\\nğŸ¤– ë¡œì§€ìŠ¤í‹± íšŒê·€ ë² ì´ìŠ¤ë¼ì¸\")\n",
    "lr_baseline = LogisticRegression(random_state=RANDOM_STATE)\n",
    "lr_baseline.fit(X_train_scaled, y_train)\n",
    "\n",
    "y_pred_lr = lr_baseline.predict(X_test_scaled)\n",
    "y_pred_proba_lr = lr_baseline.predict_proba(X_test_scaled)[:, 1]\n",
    "\n",
    "lr_accuracy = accuracy_score(y_test, y_pred_lr)\n",
    "lr_f1 = f1_score(y_test, y_pred_lr)\n",
    "lr_auc = roc_auc_score(y_test, y_pred_proba_lr)\n",
    "\n",
    "print(f\"â€¢ ì •í™•ë„: {lr_accuracy:.4f}\")\n",
    "print(f\"â€¢ F1-Score: {lr_f1:.4f}\")\n",
    "print(f\"â€¢ AUC-ROC: {lr_auc:.4f}\")\n",
    "\n",
    "# ë² ì´ìŠ¤ë¼ì¸ ì„±ëŠ¥ ì €ì¥\n",
    "baseline_results = {\n",
    "    'majority_class_accuracy': baseline_accuracy,\n",
    "    'logistic_regression': {\n",
    "        'accuracy': lr_accuracy,\n",
    "        'f1_score': lr_f1,\n",
    "        'auc_roc': lr_auc\n",
    "    }\n",
    "}\n",
    "\n",
    "print(f\"\\nâœ… ë² ì´ìŠ¤ë¼ì¸ ëª¨ë¸ í‰ê°€ ì™„ë£Œ!\")\n",
    "print(f\"ğŸ’¡ ëª©í‘œ: AUC-ROC > {lr_auc:.4f}, F1-Score > {lr_f1:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32bdb0e6",
   "metadata": {},
   "source": [
    "## 4. ë‹¤ì–‘í•œ ì•Œê³ ë¦¬ì¦˜ ë¹„êµ\n",
    "\n",
    "ì—¬ëŸ¬ ë¨¸ì‹ ëŸ¬ë‹ ì•Œê³ ë¦¬ì¦˜ì„ ì‹œë„í•˜ê³  ì„±ëŠ¥ì„ ë¹„êµí•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5067d282",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ë‹¤ì–‘í•œ ëª¨ë¸ ì •ì˜\n",
    "models = {\n",
    "    'Logistic Regression': LogisticRegression(random_state=RANDOM_STATE),\n",
    "    'Random Forest': RandomForestClassifier(n_estimators=100, random_state=RANDOM_STATE),\n",
    "    'Gradient Boosting': GradientBoostingClassifier(random_state=RANDOM_STATE),\n",
    "    'Decision Tree': DecisionTreeClassifier(random_state=RANDOM_STATE),\n",
    "    'KNN': KNeighborsClassifier(),\n",
    "    'SVM': SVC(probability=True, random_state=RANDOM_STATE),\n",
    "    'Naive Bayes': GaussianNB()\n",
    "}\n",
    "\n",
    "print(\"ğŸ¤– ë‹¤ì–‘í•œ ëª¨ë¸ ì„±ëŠ¥ ë¹„êµ\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# êµì°¨ ê²€ì¦ì„ ìœ„í•œ ì„¤ì •\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=RANDOM_STATE)\n",
    "results = {}\n",
    "\n",
    "# ê° ëª¨ë¸ í‰ê°€\n",
    "for name, model in models.items():\n",
    "    print(f\"\\nğŸ“Š {name} í‰ê°€ ì¤‘...\")\n",
    "    \n",
    "    # êµì°¨ ê²€ì¦\n",
    "    cv_scores = cross_val_score(model, X_train_scaled, y_train, cv=cv, scoring='roc_auc')\n",
    "    \n",
    "    # ëª¨ë¸ í•™ìŠµ ë° ì˜ˆì¸¡\n",
    "    model.fit(X_train_scaled, y_train)\n",
    "    y_pred = model.predict(X_test_scaled)\n",
    "    y_pred_proba = model.predict_proba(X_test_scaled)[:, 1] if hasattr(model, \"predict_proba\") else None\n",
    "    \n",
    "    # ì„±ëŠ¥ ì§€í‘œ ê³„ì‚°\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    auc = roc_auc_score(y_test, y_pred_proba) if y_pred_proba is not None else None\n",
    "    \n",
    "    results[name] = {\n",
    "        'model': model,\n",
    "        'cv_auc_mean': cv_scores.mean(),\n",
    "        'cv_auc_std': cv_scores.std(),\n",
    "        'test_accuracy': accuracy,\n",
    "        'test_f1': f1,\n",
    "        'test_auc': auc,\n",
    "        'y_pred': y_pred,\n",
    "        'y_pred_proba': y_pred_proba\n",
    "    }\n",
    "    \n",
    "    print(f\"  â€¢ CV AUC: {cv_scores.mean():.4f} (Â±{cv_scores.std():.4f})\")\n",
    "    print(f\"  â€¢ Test Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"  â€¢ Test F1-Score: {f1:.4f}\")\n",
    "    if auc is not None:\n",
    "        print(f\"  â€¢ Test AUC: {auc:.4f}\")\n",
    "\n",
    "print(\"\\nâœ… ëª¨ë¸ ë¹„êµ ì™„ë£Œ!\")\n",
    "\n",
    "# ê²°ê³¼ ì •ë¦¬\n",
    "comparison_df = pd.DataFrame({\n",
    "    'Model': list(results.keys()),\n",
    "    'CV_AUC_Mean': [results[model]['cv_auc_mean'] for model in results],\n",
    "    'CV_AUC_Std': [results[model]['cv_auc_std'] for model in results],\n",
    "    'Test_Accuracy': [results[model]['test_accuracy'] for model in results],\n",
    "    'Test_F1': [results[model]['test_f1'] for model in results],\n",
    "    'Test_AUC': [results[model]['test_auc'] if results[model]['test_auc'] is not None else np.nan for model in results]\n",
    "})\n",
    "\n",
    "comparison_df = comparison_df.sort_values('CV_AUC_Mean', ascending=False)\n",
    "print(f\"\\nğŸ“Š ëª¨ë¸ ì„±ëŠ¥ ìˆœìœ„:\")\n",
    "display(comparison_df.round(4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "213dea07",
   "metadata": {},
   "source": [
    "## 5. í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹\n",
    "\n",
    "ìƒìœ„ ì„±ëŠ¥ ëª¨ë¸ë“¤ì— ëŒ€í•´ GridSearchCVë¥¼ ì‚¬ìš©í•˜ì—¬ ìµœì  í•˜ì´í¼íŒŒë¼ë¯¸í„°ë¥¼ ì°¾ì•„ë³´ê² ìŠµë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7728a3c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ìƒìœ„ 3ê°œ ëª¨ë¸ ì„ íƒ\n",
    "top_models = comparison_df.head(3)['Model'].tolist()\n",
    "print(f\"ğŸ¯ í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ ëŒ€ìƒ ëª¨ë¸: {top_models}\")\n",
    "\n",
    "# í•˜ì´í¼íŒŒë¼ë¯¸í„° ê·¸ë¦¬ë“œ ì •ì˜\n",
    "param_grids = {\n",
    "    'Random Forest': {\n",
    "        'n_estimators': [100, 200, 300],\n",
    "        'max_depth': [None, 10, 20, 30],\n",
    "        'min_samples_split': [2, 5, 10],\n",
    "        'min_samples_leaf': [1, 2, 4]\n",
    "    },\n",
    "    'Gradient Boosting': {\n",
    "        'n_estimators': [100, 200, 300],\n",
    "        'max_depth': [3, 5, 7],\n",
    "        'learning_rate': [0.01, 0.1, 0.2],\n",
    "        'subsample': [0.8, 0.9, 1.0]\n",
    "    },\n",
    "    'Logistic Regression': {\n",
    "        'C': [0.001, 0.01, 0.1, 1, 10, 100],\n",
    "        'penalty': ['l1', 'l2'],\n",
    "        'solver': ['liblinear', 'saga']\n",
    "    },\n",
    "    'Decision Tree': {\n",
    "        'max_depth': [None, 10, 20, 30, 40],\n",
    "        'min_samples_split': [2, 5, 10, 20],\n",
    "        'min_samples_leaf': [1, 2, 5, 10],\n",
    "        'criterion': ['gini', 'entropy']\n",
    "    },\n",
    "    'SVM': {\n",
    "        'C': [0.1, 1, 10, 100],\n",
    "        'kernel': ['rbf', 'linear'],\n",
    "        'gamma': ['scale', 'auto', 0.001, 0.01]\n",
    "    }\n",
    "}\n",
    "\n",
    "# íŠœë‹ëœ ëª¨ë¸ ì €ì¥\n",
    "best_models = {}\n",
    "tuning_results = {}\n",
    "\n",
    "print(\"\\nğŸ”§ í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ ì‹œì‘...\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for model_name in top_models:\n",
    "    if model_name in param_grids:\n",
    "        print(f\"\\nğŸ›ï¸ {model_name} íŠœë‹ ì¤‘...\")\n",
    "        \n",
    "        # ì›ë³¸ ëª¨ë¸ ê°€ì ¸ì˜¤ê¸°\n",
    "        original_model = models[model_name]\n",
    "        \n",
    "        # GridSearch ì„¤ì •\n",
    "        grid_search = GridSearchCV(\n",
    "            original_model,\n",
    "            param_grids[model_name],\n",
    "            cv=3,  # ì‹œê°„ ì ˆì•½ì„ ìœ„í•´ 3-fold\n",
    "            scoring='roc_auc',\n",
    "            n_jobs=-1,\n",
    "            verbose=0\n",
    "        )\n",
    "        \n",
    "        # íŠœë‹ ì‹¤í–‰\n",
    "        start_time = time.time()\n",
    "        grid_search.fit(X_train_scaled, y_train)\n",
    "        end_time = time.time()\n",
    "        \n",
    "        # ìµœì  ëª¨ë¸ë¡œ ì˜ˆì¸¡\n",
    "        best_model = grid_search.best_estimator_\n",
    "        y_pred_tuned = best_model.predict(X_test_scaled)\n",
    "        y_pred_proba_tuned = best_model.predict_proba(X_test_scaled)[:, 1]\n",
    "        \n",
    "        # ì„±ëŠ¥ ê³„ì‚°\n",
    "        tuned_accuracy = accuracy_score(y_test, y_pred_tuned)\n",
    "        tuned_f1 = f1_score(y_test, y_pred_tuned)\n",
    "        tuned_auc = roc_auc_score(y_test, y_pred_proba_tuned)\n",
    "        \n",
    "        # ê²°ê³¼ ì €ì¥\n",
    "        best_models[model_name] = best_model\n",
    "        tuning_results[model_name] = {\n",
    "            'best_params': grid_search.best_params_,\n",
    "            'best_cv_score': grid_search.best_score_,\n",
    "            'test_accuracy': tuned_accuracy,\n",
    "            'test_f1': tuned_f1,\n",
    "            'test_auc': tuned_auc,\n",
    "            'tuning_time': end_time - start_time\n",
    "        }\n",
    "        \n",
    "        # ê°œì„  ì •ë„ ê³„ì‚°\n",
    "        original_auc = results[model_name]['test_auc']\n",
    "        improvement = tuned_auc - original_auc if original_auc else 0\n",
    "        \n",
    "        print(f\"  âœ… ì™„ë£Œ! ì†Œìš”ì‹œê°„: {end_time - start_time:.2f}ì´ˆ\")\n",
    "        print(f\"  â€¢ ìµœì  íŒŒë¼ë¯¸í„°: {grid_search.best_params_}\")\n",
    "        print(f\"  â€¢ CV Score: {grid_search.best_score_:.4f}\")\n",
    "        print(f\"  â€¢ Test AUC: {tuned_auc:.4f} (ê°œì„ : {improvement:+.4f})\")\n",
    "        print(f\"  â€¢ Test Accuracy: {tuned_accuracy:.4f}\")\n",
    "        print(f\"  â€¢ Test F1-Score: {tuned_f1:.4f}\")\n",
    "\n",
    "print(\"\\nâœ… í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ ì™„ë£Œ!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c64d397e",
   "metadata": {},
   "source": [
    "## 6. ëª¨ë¸ ì„±ëŠ¥ ì‹œê°í™”\n",
    "\n",
    "ë‹¤ì–‘í•œ ì‹œê°í™”ë¥¼ í†µí•´ ëª¨ë¸ë“¤ì˜ ì„±ëŠ¥ì„ ë¹„êµí•´ë³´ê² ìŠµë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "889eb9b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. ROC ê³¡ì„  ë¹„êµ\n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "# ì²« ë²ˆì§¸ ì„œë¸Œí”Œë¡¯: ëª¨ë“  ëª¨ë¸ ROC ê³¡ì„ \n",
    "plt.subplot(1, 3, 1)\n",
    "for name, result in results.items():\n",
    "    if result['y_pred_proba'] is not None:\n",
    "        fpr, tpr, _ = roc_curve(y_test, result['y_pred_proba'])\n",
    "        auc_score = result['test_auc']\n",
    "        plt.plot(fpr, tpr, label=f'{name} (AUC = {auc_score:.3f})')\n",
    "\n",
    "plt.plot([0, 1], [0, 1], 'k--', label='Random (AUC = 0.500)')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curves - Original Models')\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# ë‘ ë²ˆì§¸ ì„œë¸Œí”Œë¡¯: ì„±ëŠ¥ ì§€í‘œ ë¹„êµ (ë§‰ëŒ€ ê·¸ë˜í”„)\n",
    "plt.subplot(1, 3, 2)\n",
    "metrics = ['test_accuracy', 'test_f1', 'test_auc']\n",
    "metric_names = ['Accuracy', 'F1-Score', 'AUC']\n",
    "colors = ['skyblue', 'lightgreen', 'salmon']\n",
    "\n",
    "x = np.arange(len(comparison_df))\n",
    "width = 0.25\n",
    "\n",
    "for i, (metric, name, color) in enumerate(zip(metrics, metric_names, colors)):\n",
    "    values = [results[model][metric] if results[model][metric] is not None else 0 \n",
    "              for model in comparison_df['Model']]\n",
    "    plt.bar(x + i*width, values, width, label=name, color=color, alpha=0.8)\n",
    "\n",
    "plt.xlabel('Models')\n",
    "plt.ylabel('Score')\n",
    "plt.title('Performance Metrics Comparison')\n",
    "plt.xticks(x + width, comparison_df['Model'], rotation=45, ha='right')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# ì„¸ ë²ˆì§¸ ì„œë¸Œí”Œë¡¯: êµì°¨ê²€ì¦ ì ìˆ˜ ë¶„í¬\n",
    "plt.subplot(1, 3, 3)\n",
    "cv_means = [results[model]['cv_auc_mean'] for model in comparison_df['Model']]\n",
    "cv_stds = [results[model]['cv_auc_std'] for model in comparison_df['Model']]\n",
    "\n",
    "plt.errorbar(range(len(cv_means)), cv_means, yerr=cv_stds, \n",
    "             fmt='o', capsize=5, capthick=2, elinewidth=2, markerfacecolor='orange', \n",
    "             markeredgecolor='red', markeredgewidth=2, markersize=8)\n",
    "plt.xlabel('Models')\n",
    "plt.ylabel('Cross-Validation AUC')\n",
    "plt.title('CV Performance with Error Bars')\n",
    "plt.xticks(range(len(comparison_df)), comparison_df['Model'], rotation=45, ha='right')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 2. í˜¼ë™ í–‰ë ¬ íˆíŠ¸ë§µ (ìƒìœ„ 3ê°œ ëª¨ë¸)\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "fig.suptitle('Confusion Matrices - Top 3 Models', fontsize=16, y=1.02)\n",
    "\n",
    "for i, model_name in enumerate(top_models[:3]):\n",
    "    if model_name in results:\n",
    "        y_pred = results[model_name]['y_pred']\n",
    "        cm = confusion_matrix(y_test, y_pred)\n",
    "        \n",
    "        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "                   xticklabels=['No Churn', 'Churn'],\n",
    "                   yticklabels=['No Churn', 'Churn'],\n",
    "                   ax=axes[i])\n",
    "        axes[i].set_title(f'{model_name}')\n",
    "        axes[i].set_xlabel('Predicted')\n",
    "        axes[i].set_ylabel('Actual')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 3. í”¼ì²˜ ì¤‘ìš”ë„ ì‹œê°í™” (íŠ¸ë¦¬ ê¸°ë°˜ ëª¨ë¸ë“¤)\n",
    "tree_models = ['Random Forest', 'Gradient Boosting', 'Decision Tree']\n",
    "available_tree_models = [model for model in tree_models if model in results]\n",
    "\n",
    "if available_tree_models:\n",
    "    fig, axes = plt.subplots(len(available_tree_models), 1, figsize=(12, 6*len(available_tree_models)))\n",
    "    if len(available_tree_models) == 1:\n",
    "        axes = [axes]\n",
    "    \n",
    "    for i, model_name in enumerate(available_tree_models):\n",
    "        model = results[model_name]['model']\n",
    "        if hasattr(model, 'feature_importances_'):\n",
    "            importance_df = pd.DataFrame({\n",
    "                'feature': feature_names,\n",
    "                'importance': model.feature_importances_\n",
    "            }).sort_values('importance', ascending=True)\n",
    "            \n",
    "            # ìƒìœ„ 10ê°œ í”¼ì²˜ë§Œ í‘œì‹œ\n",
    "            top_features = importance_df.tail(10)\n",
    "            \n",
    "            axes[i].barh(range(len(top_features)), top_features['importance'])\n",
    "            axes[i].set_yticks(range(len(top_features)))\n",
    "            axes[i].set_yticklabels(top_features['feature'])\n",
    "            axes[i].set_xlabel('Feature Importance')\n",
    "            axes[i].set_title(f'{model_name} - Top 10 Feature Importances')\n",
    "            axes[i].grid(True, alpha=0.3, axis='x')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "print(\"ğŸ“Š ì‹œê°í™” ì™„ë£Œ!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6e5515b",
   "metadata": {},
   "source": [
    "## 7. ìµœì¢… ëª¨ë¸ ì„ íƒ ë° ì €ì¥\n",
    "\n",
    "ì„±ëŠ¥ í‰ê°€ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ ìµœì ì˜ ëª¨ë¸ì„ ì„ íƒí•˜ê³  ì €ì¥í•˜ê² ìŠµë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46d464eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ìµœê³  ì„±ëŠ¥ ëª¨ë¸ ì„ íƒ\n",
    "best_model_name = comparison_df.iloc[0]['Model']\n",
    "best_auc_score = comparison_df.iloc[0]['CV_AUC_Mean']\n",
    "\n",
    "print(f\"ğŸ† ìµœê³  ì„±ëŠ¥ ëª¨ë¸: {best_model_name}\")\n",
    "print(f\"ğŸ“Š êµì°¨ê²€ì¦ AUC: {best_auc_score:.4f}\")\n",
    "\n",
    "# í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ëœ ëª¨ë¸ì´ ìˆë‹¤ë©´ ì‚¬ìš©\n",
    "if best_model_name in best_models:\n",
    "    final_model = best_models[best_model_name]\n",
    "    tuned_results = tuning_results[best_model_name]\n",
    "    print(f\"ğŸ›ï¸ í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ ì ìš©ë¨\")\n",
    "    print(f\"  â€¢ ìµœì  íŒŒë¼ë¯¸í„°: {tuned_results['best_params']}\")\n",
    "    print(f\"  â€¢ íŠœë‹ëœ Test AUC: {tuned_results['test_auc']:.4f}\")\n",
    "else:\n",
    "    final_model = results[best_model_name]['model']\n",
    "    print(f\"âš™ï¸ ê¸°ë³¸ íŒŒë¼ë¯¸í„° ì‚¬ìš©\")\n",
    "\n",
    "# ìµœì¢… ì„±ëŠ¥ í‰ê°€\n",
    "y_final_pred = final_model.predict(X_test_scaled)\n",
    "y_final_proba = final_model.predict_proba(X_test_scaled)[:, 1]\n",
    "\n",
    "final_accuracy = accuracy_score(y_test, y_final_pred)\n",
    "final_precision = precision_score(y_test, y_final_pred)\n",
    "final_recall = recall_score(y_test, y_final_pred)\n",
    "final_f1 = f1_score(y_test, y_final_pred)\n",
    "final_auc = roc_auc_score(y_test, y_final_proba)\n",
    "\n",
    "print(f\"\\nğŸ“ˆ ìµœì¢… ëª¨ë¸ ì„±ëŠ¥:\")\n",
    "print(f\"  â€¢ Accuracy: {final_accuracy:.4f}\")\n",
    "print(f\"  â€¢ Precision: {final_precision:.4f}\")\n",
    "print(f\"  â€¢ Recall: {final_recall:.4f}\")\n",
    "print(f\"  â€¢ F1-Score: {final_f1:.4f}\")\n",
    "print(f\"  â€¢ AUC: {final_auc:.4f}\")\n",
    "\n",
    "# ëª¨ë¸ê³¼ ì „ì²˜ë¦¬ê¸° ì €ì¥\n",
    "import os\n",
    "import pickle\n",
    "from datetime import datetime\n",
    "\n",
    "# ê²°ê³¼ í´ë” ìƒì„±\n",
    "models_dir = '../results/models'\n",
    "os.makedirs(models_dir, exist_ok=True)\n",
    "\n",
    "# í˜„ì¬ ì‹œê°„ìœ¼ë¡œ íŒŒì¼ëª… ìƒì„±\n",
    "timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "model_filename = f'best_model_{best_model_name.lower().replace(\" \", \"_\")}_{timestamp}.pkl'\n",
    "scaler_filename = f'scaler_{timestamp}.pkl'\n",
    "results_filename = f'model_results_{timestamp}.pkl'\n",
    "\n",
    "# ëª¨ë¸ ì €ì¥\n",
    "model_path = os.path.join(models_dir, model_filename)\n",
    "with open(model_path, 'wb') as f:\n",
    "    pickle.dump(final_model, f)\n",
    "\n",
    "# ìŠ¤ì¼€ì¼ëŸ¬ ì €ì¥\n",
    "scaler_path = os.path.join(models_dir, scaler_filename)\n",
    "with open(scaler_path, 'wb') as f:\n",
    "    pickle.dump(scaler, f)\n",
    "\n",
    "# ê²°ê³¼ ì •ë³´ ì €ì¥\n",
    "model_info = {\n",
    "    'model_name': best_model_name,\n",
    "    'model_path': model_path,\n",
    "    'scaler_path': scaler_path,\n",
    "    'feature_names': feature_names,\n",
    "    'performance': {\n",
    "        'accuracy': final_accuracy,\n",
    "        'precision': final_precision,\n",
    "        'recall': final_recall,\n",
    "        'f1_score': final_f1,\n",
    "        'auc': final_auc\n",
    "    },\n",
    "    'hyperparameters': tuned_results['best_params'] if best_model_name in best_models else 'default',\n",
    "    'training_date': timestamp,\n",
    "    'model_comparison': comparison_df.to_dict('records')\n",
    "}\n",
    "\n",
    "results_path = os.path.join(models_dir, results_filename)\n",
    "with open(results_path, 'wb') as f:\n",
    "    pickle.dump(model_info, f)\n",
    "\n",
    "print(f\"\\nğŸ’¾ ëª¨ë¸ ì €ì¥ ì™„ë£Œ:\")\n",
    "print(f\"  â€¢ ëª¨ë¸: {model_path}\")\n",
    "print(f\"  â€¢ ìŠ¤ì¼€ì¼ëŸ¬: {scaler_path}\")\n",
    "print(f\"  â€¢ ê²°ê³¼ ì •ë³´: {results_path}\")\n",
    "\n",
    "# ê°„ë‹¨í•œ ëª¨ë¸ ìš”ì•½ ì €ì¥ (ì½ê¸° ì‰¬ìš´ í…ìŠ¤íŠ¸ í˜•íƒœ)\n",
    "summary_filename = f'model_summary_{timestamp}.txt'\n",
    "summary_path = os.path.join(models_dir, summary_filename)\n",
    "\n",
    "with open(summary_path, 'w', encoding='utf-8') as f:\n",
    "    f.write(f\"ê³ ê° ì´íƒˆ ì˜ˆì¸¡ ëª¨ë¸ - ìµœì¢… ê²°ê³¼ ë³´ê³ ì„œ\\n\")\n",
    "    f.write(f\"=\" * 50 + \"\\n\\n\")\n",
    "    f.write(f\"ëª¨ë¸ëª…: {best_model_name}\\n\")\n",
    "    f.write(f\"í•™ìŠµì¼ì‹œ: {timestamp}\\n\\n\")\n",
    "    f.write(f\"ì„±ëŠ¥ ì§€í‘œ:\\n\")\n",
    "    f.write(f\"  - Accuracy: {final_accuracy:.4f}\\n\")\n",
    "    f.write(f\"  - Precision: {final_precision:.4f}\\n\")\n",
    "    f.write(f\"  - Recall: {final_recall:.4f}\\n\")\n",
    "    f.write(f\"  - F1-Score: {final_f1:.4f}\\n\")\n",
    "    f.write(f\"  - AUC: {final_auc:.4f}\\n\\n\")\n",
    "    \n",
    "    if best_model_name in best_models:\n",
    "        f.write(f\"ìµœì í™”ëœ í•˜ì´í¼íŒŒë¼ë¯¸í„°:\\n\")\n",
    "        for param, value in tuned_results['best_params'].items():\n",
    "            f.write(f\"  - {param}: {value}\\n\")\n",
    "    \n",
    "    f.write(f\"\\nëª¨ë¸ ë¹„êµ ê²°ê³¼:\\n\")\n",
    "    for idx, row in comparison_df.iterrows():\n",
    "        f.write(f\"  {idx+1}. {row['Model']}: AUC {row['CV_AUC_Mean']:.4f}\\n\")\n",
    "\n",
    "print(f\"  â€¢ ìš”ì•½ ë³´ê³ ì„œ: {summary_path}\")\n",
    "print(f\"\\nâœ… ëª¨ë¸ë§ í”„ë¡œì„¸ìŠ¤ ì™„ë£Œ! ë‹¤ìŒ ë‹¨ê³„: 04_Evaluation.ipynbì—ì„œ ìƒì„¸ í‰ê°€ ì§„í–‰\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

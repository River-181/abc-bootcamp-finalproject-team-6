{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cc0ee24d",
   "metadata": {},
   "source": [
    "# ê³ ê° ì´íƒˆ ì˜ˆì¸¡ ëª¨ë¸ - í‰ê°€ ë° í•´ì„\n",
    "\n",
    "ì´ ë…¸íŠ¸ë¶ì—ì„œëŠ” í•™ìŠµëœ ëª¨ë¸ì˜ ì„±ëŠ¥ì„ ìƒì„¸íˆ í‰ê°€í•˜ê³ , ë¹„ì¦ˆë‹ˆìŠ¤ ê´€ì ì—ì„œ ëª¨ë¸ì„ í•´ì„í•´ë³´ê² ìŠµë‹ˆë‹¤.\n",
    "\n",
    "## ì£¼ìš” ë‚´ìš©\n",
    "1. **ëª¨ë¸ ë¡œë“œ ë° ì„¤ì •**: ì €ì¥ëœ ìµœì  ëª¨ë¸ ë¶ˆëŸ¬ì˜¤ê¸°\n",
    "2. **ìƒì„¸ ì„±ëŠ¥ í‰ê°€**: ë‹¤ì–‘í•œ ë©”íŠ¸ë¦­ì„ í†µí•œ í¬ê´„ì  í‰ê°€\n",
    "3. **ëª¨ë¸ í•´ì„**: í”¼ì²˜ ì¤‘ìš”ë„ ë° ì˜ˆì¸¡ ê·¼ê±° ë¶„ì„\n",
    "4. **ë¹„ì¦ˆë‹ˆìŠ¤ ì„íŒ©íŠ¸**: ì‹¤ì œ ë¹„ì¦ˆë‹ˆìŠ¤ ê´€ì ì—ì„œì˜ ëª¨ë¸ ê°€ì¹˜ í‰ê°€\n",
    "5. **ë°°í¬ ê¶Œì¥ì‚¬í•­**: ëª¨ë¸ ìš´ì˜ì„ ìœ„í•œ ì‹¤ìš©ì  ê°€ì´ë“œ\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5910e55",
   "metadata": {},
   "source": [
    "## 1. ë¼ì´ë¸ŒëŸ¬ë¦¬ ë° ì„¤ì •"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "609e390a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“š ë¼ì´ë¸ŒëŸ¬ë¦¬ ë¡œë“œ ì™„ë£Œ!\n",
      "ğŸ• í‰ê°€ ì‹œì‘ ì‹œê°„: 2025-08-03 19:23:05\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# ê¸°ë³¸ ë¼ì´ë¸ŒëŸ¬ë¦¬\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pickle\n",
    "import os\n",
    "import warnings\n",
    "from datetime import datetime\n",
    "\n",
    "# ë¨¸ì‹ ëŸ¬ë‹ ë¼ì´ë¸ŒëŸ¬ë¦¬\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score,\n",
    "    roc_auc_score, roc_curve, precision_recall_curve,\n",
    "    confusion_matrix, classification_report,\n",
    "    average_precision_score\n",
    ")\n",
    "from sklearn.model_selection import learning_curve, validation_curve\n",
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "# ì‹œê°í™” ì„¤ì •\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# í•œê¸€ í°íŠ¸ ì„¤ì • (ë§¥ìš©)\n",
    "plt.rcParams['font.family'] = ['AppleGothic'] if os.name == 'posix' else ['Malgun Gothic']\n",
    "plt.rcParams['axes.unicode_minus'] = False\n",
    "\n",
    "# í‘œì‹œ ì˜µì…˜\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', None)\n",
    "\n",
    "print(\"ğŸ“š ë¼ì´ë¸ŒëŸ¬ë¦¬ ë¡œë“œ ì™„ë£Œ!\")\n",
    "print(f\"ğŸ• í‰ê°€ ì‹œì‘ ì‹œê°„: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81446c27",
   "metadata": {},
   "source": [
    "## 2. ëª¨ë¸ ë° ë°ì´í„° ë¡œë“œ\n",
    "\n",
    "03_Modeling.ipynbì—ì„œ ì €ì¥í•œ ìµœì  ëª¨ë¸ê³¼ ì „ì²˜ë¦¬ëœ ë°ì´í„°ë¥¼ ë¡œë“œí•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e9f0bdc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”„ ëª¨ë¸ ë° ë°ì´í„° ë¡œë”© ì‹œì‘...\n",
      "ğŸ“ ëª¨ë¸ íŒŒì¼ ë°œê²¬: best_model_gradient_boosting_20250803_191441.pkl\n",
      "âœ… ëª¨ë¸ ë¡œë“œ ì™„ë£Œ: Gradient Boosting\n",
      "ğŸ”„ í…ŒìŠ¤íŠ¸ ë°ì´í„° ì¤€ë¹„ ì¤‘...\n",
      "ğŸ“Š ìŠ¤ì¼€ì¼ë§ëœ í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¡œë“œ ì¤‘...\n",
      "âš ï¸ ëˆ„ë½ê°’ 1ê°œ ë°œê²¬ - ì²˜ë¦¬ ì¤‘...\n",
      "âœ… ëˆ„ë½ê°’ ì²˜ë¦¬ ì™„ë£Œ\n",
      "ğŸ“Š ë°ì´í„° ë¡œë“œ ì™„ë£Œ - ìƒ˜í”Œ ìˆ˜: 750, í”¼ì²˜ ìˆ˜: 65\n",
      "ğŸ¯ ì˜ˆì¸¡ ìˆ˜í–‰ ì¤‘...\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "X has 65 features, but GradientBoostingClassifier is expecting 28 features as input.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 114\u001b[39m\n\u001b[32m    112\u001b[39m \u001b[38;5;66;03m# ì˜ˆì¸¡ ìˆ˜í–‰\u001b[39;00m\n\u001b[32m    113\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mğŸ¯ ì˜ˆì¸¡ ìˆ˜í–‰ ì¤‘...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m114\u001b[39m y_pred = \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_test\u001b[49m\u001b[43m.\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    116\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(model, \u001b[33m'\u001b[39m\u001b[33mpredict_proba\u001b[39m\u001b[33m'\u001b[39m):\n\u001b[32m    117\u001b[39m     y_pred_proba = model.predict_proba(X_test.values)[:, \u001b[32m1\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/abc-bootcamp-FP-2025/lib/python3.12/site-packages/sklearn/ensemble/_gb.py:1627\u001b[39m, in \u001b[36mGradientBoostingClassifier.predict\u001b[39m\u001b[34m(self, X)\u001b[39m\n\u001b[32m   1612\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mpredict\u001b[39m(\u001b[38;5;28mself\u001b[39m, X):\n\u001b[32m   1613\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Predict class for X.\u001b[39;00m\n\u001b[32m   1614\u001b[39m \n\u001b[32m   1615\u001b[39m \u001b[33;03m    Parameters\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1625\u001b[39m \u001b[33;03m        The predicted values.\u001b[39;00m\n\u001b[32m   1626\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1627\u001b[39m     raw_predictions = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdecision_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1628\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m raw_predictions.ndim == \u001b[32m1\u001b[39m:  \u001b[38;5;66;03m# decision_function already squeezed it\u001b[39;00m\n\u001b[32m   1629\u001b[39m         encoded_classes = (raw_predictions >= \u001b[32m0\u001b[39m).astype(\u001b[38;5;28mint\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/abc-bootcamp-FP-2025/lib/python3.12/site-packages/sklearn/ensemble/_gb.py:1580\u001b[39m, in \u001b[36mGradientBoostingClassifier.decision_function\u001b[39m\u001b[34m(self, X)\u001b[39m\n\u001b[32m   1561\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecision_function\u001b[39m(\u001b[38;5;28mself\u001b[39m, X):\n\u001b[32m   1562\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Compute the decision function of ``X``.\u001b[39;00m\n\u001b[32m   1563\u001b[39m \n\u001b[32m   1564\u001b[39m \u001b[33;03m    Parameters\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1578\u001b[39m \u001b[33;03m        array of shape (n_samples,).\u001b[39;00m\n\u001b[32m   1579\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1580\u001b[39m     X = \u001b[43mvalidate_data\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1581\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mDTYPE\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43morder\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mC\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcsr\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreset\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\n\u001b[32m   1582\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1583\u001b[39m     raw_predictions = \u001b[38;5;28mself\u001b[39m._raw_predict(X)\n\u001b[32m   1584\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m raw_predictions.shape[\u001b[32m1\u001b[39m] == \u001b[32m1\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/abc-bootcamp-FP-2025/lib/python3.12/site-packages/sklearn/utils/validation.py:2975\u001b[39m, in \u001b[36mvalidate_data\u001b[39m\u001b[34m(_estimator, X, y, reset, validate_separately, skip_check_array, **check_params)\u001b[39m\n\u001b[32m   2972\u001b[39m     out = X, y\n\u001b[32m   2974\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m check_params.get(\u001b[33m\"\u001b[39m\u001b[33mensure_2d\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[32m-> \u001b[39m\u001b[32m2975\u001b[39m     \u001b[43m_check_n_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_estimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreset\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2977\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/abc-bootcamp-FP-2025/lib/python3.12/site-packages/sklearn/utils/validation.py:2839\u001b[39m, in \u001b[36m_check_n_features\u001b[39m\u001b[34m(estimator, X, reset)\u001b[39m\n\u001b[32m   2836\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[32m   2838\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m n_features != estimator.n_features_in_:\n\u001b[32m-> \u001b[39m\u001b[32m2839\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   2840\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mX has \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mn_features\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m features, but \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mestimator.\u001b[34m__class__\u001b[39m.\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   2841\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mis expecting \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mestimator.n_features_in_\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m features as input.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   2842\u001b[39m     )\n",
      "\u001b[31mValueError\u001b[39m: X has 65 features, but GradientBoostingClassifier is expecting 28 features as input."
     ]
    }
   ],
   "source": [
    "# ì €ì¥ëœ ëª¨ë¸ íŒŒì¼ ì°¾ê¸°\n",
    "models_dir = '../results/models'\n",
    "data_dir = '../data/processed'\n",
    "\n",
    "print(\"ğŸ”„ ëª¨ë¸ ë° ë°ì´í„° ë¡œë”© ì‹œì‘...\")\n",
    "\n",
    "# ëª¨ë¸ íŒŒì¼ í™•ì¸\n",
    "if os.path.exists(models_dir):\n",
    "    model_files = [f for f in os.listdir(models_dir) if f.startswith('best_model_') and f.endswith('.pkl')]\n",
    "else:\n",
    "    model_files = []\n",
    "\n",
    "if not model_files:\n",
    "    print(\"âš ï¸ ì €ì¥ëœ ëª¨ë¸ì´ ì—†ìŠµë‹ˆë‹¤. 03_Modeling.ipynbë¥¼ ë¨¼ì € ì‹¤í–‰í•´ì£¼ì„¸ìš”.\")\n",
    "    print(\"ğŸ“ ìƒ˜í”Œ ë°ì´í„°ë¡œ í‰ê°€ë¥¼ ì§„í–‰í•©ë‹ˆë‹¤...\")\n",
    "    \n",
    "    # ìƒ˜í”Œ ë°ì´í„° ìƒì„±\n",
    "    np.random.seed(42)\n",
    "    n_samples = 1000\n",
    "    \n",
    "    X_test = pd.DataFrame({\n",
    "        'feature_1': np.random.normal(0, 1, n_samples),\n",
    "        'feature_2': np.random.normal(0, 1, n_samples),\n",
    "        'feature_3': np.random.normal(0, 1, n_samples),\n",
    "        'feature_4': np.random.normal(0, 1, n_samples),\n",
    "        'feature_5': np.random.normal(0, 1, n_samples)\n",
    "    })\n",
    "    \n",
    "    y_test = np.random.choice([0, 1], n_samples, p=[0.7, 0.3])\n",
    "    y_pred = np.random.choice([0, 1], n_samples, p=[0.75, 0.25])\n",
    "    y_pred_proba = np.random.beta(2, 5, n_samples)\n",
    "    \n",
    "    model_info = {\n",
    "        'model_name': 'Sample Random Forest',\n",
    "        'performance': {\n",
    "            'accuracy': accuracy_score(y_test, y_pred),\n",
    "            'precision': precision_score(y_test, y_pred),\n",
    "            'recall': recall_score(y_test, y_pred),\n",
    "            'f1_score': f1_score(y_test, y_pred),\n",
    "            'auc': roc_auc_score(y_test, y_pred_proba)\n",
    "        },\n",
    "        'feature_names': list(X_test.columns),\n",
    "        'training_date': datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "    }\n",
    "    \n",
    "    print(\"âœ… ìƒ˜í”Œ ë°ì´í„° ìƒì„± ì™„ë£Œ\")\n",
    "    \n",
    "else:\n",
    "    # ê°€ì¥ ìµœê·¼ ëª¨ë¸ íŒŒì¼ ì„ íƒ\n",
    "    latest_model_file = max(model_files, key=lambda f: os.path.getctime(os.path.join(models_dir, f)))\n",
    "    print(f\"ğŸ“ ëª¨ë¸ íŒŒì¼ ë°œê²¬: {latest_model_file}\")\n",
    "    \n",
    "    # ëª¨ë¸ ë¡œë“œ\n",
    "    model_path = os.path.join(models_dir, latest_model_file)\n",
    "    with open(model_path, 'rb') as f:\n",
    "        model = pickle.load(f)\n",
    "    \n",
    "    # ëª¨ë¸ ì´ë¦„ ì¶”ì¶œ\n",
    "    model_type = latest_model_file.split('_')[2]\n",
    "    model_name_mapping = {\n",
    "        'gradient': 'Gradient Boosting',\n",
    "        'random': 'Random Forest', \n",
    "        'xgb': 'XGBoost',\n",
    "        'logistic': 'Logistic Regression',\n",
    "        'svm': 'SVM',\n",
    "        'decision': 'Decision Tree',\n",
    "        'neural': 'Neural Network'\n",
    "    }\n",
    "    model_name = model_name_mapping.get(model_type, model_type.replace('_', ' ').title())\n",
    "    \n",
    "    # íƒ€ì„ìŠ¤íƒ¬í”„ ì¶”ì¶œ\n",
    "    timestamp = latest_model_file.split('_')[-1].replace('.pkl', '')\n",
    "    \n",
    "    # ëª¨ë¸ ì •ë³´ ì„¤ì •\n",
    "    model_info = {\n",
    "        'model_name': model_name,\n",
    "        'performance': {},\n",
    "        'feature_names': [],\n",
    "        'training_date': timestamp\n",
    "    }\n",
    "    \n",
    "    print(f\"âœ… ëª¨ë¸ ë¡œë“œ ì™„ë£Œ: {model_name}\")\n",
    "    \n",
    "    # í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¡œë“œ\n",
    "    print(\"ğŸ”„ í…ŒìŠ¤íŠ¸ ë°ì´í„° ì¤€ë¹„ ì¤‘...\")\n",
    "    \n",
    "    test_data_scaled_path = os.path.join(data_dir, 'test_data_scaled.csv')\n",
    "    \n",
    "    if os.path.exists(test_data_scaled_path):\n",
    "        print(\"ğŸ“Š ìŠ¤ì¼€ì¼ë§ëœ í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¡œë“œ ì¤‘...\")\n",
    "        test_data = pd.read_csv(test_data_scaled_path)\n",
    "        \n",
    "        # NaN ê°’ ì²˜ë¦¬\n",
    "        if test_data.isnull().sum().sum() > 0:\n",
    "            print(f\"âš ï¸ ëˆ„ë½ê°’ {test_data.isnull().sum().sum()}ê°œ ë°œê²¬ - ì²˜ë¦¬ ì¤‘...\")\n",
    "            test_data = test_data.fillna(test_data.mean())\n",
    "            print(\"âœ… ëˆ„ë½ê°’ ì²˜ë¦¬ ì™„ë£Œ\")\n",
    "        \n",
    "        # í”¼ì²˜ì™€ íƒ€ê²Ÿ ë¶„ë¦¬\n",
    "        if 'churn' in test_data.columns:\n",
    "            X_test = test_data.drop(['churn'], axis=1)\n",
    "            y_test = test_data['churn'].values\n",
    "        else:\n",
    "            X_test = test_data\n",
    "            y_test = None\n",
    "        \n",
    "        # ëª¨ë¸ ì •ë³´ ì—…ë°ì´íŠ¸\n",
    "        model_info['feature_names'] = list(X_test.columns)\n",
    "        \n",
    "        print(f\"ğŸ“Š ë°ì´í„° ë¡œë“œ ì™„ë£Œ - ìƒ˜í”Œ ìˆ˜: {len(X_test)}, í”¼ì²˜ ìˆ˜: {X_test.shape[1]}\")\n",
    "        \n",
    "        # ì˜ˆì¸¡ ìˆ˜í–‰\n",
    "        print(\"ğŸ¯ ì˜ˆì¸¡ ìˆ˜í–‰ ì¤‘...\")\n",
    "        y_pred = model.predict(X_test.values)\n",
    "        \n",
    "        if hasattr(model, 'predict_proba'):\n",
    "            y_pred_proba = model.predict_proba(X_test.values)[:, 1]\n",
    "        else:\n",
    "            # í™•ë¥  ì˜ˆì¸¡ì´ ë¶ˆê°€ëŠ¥í•œ ê²½ìš°\n",
    "            if hasattr(model, 'decision_function'):\n",
    "                decision_scores = model.decision_function(X_test.values)\n",
    "                y_pred_proba = 1 / (1 + np.exp(-decision_scores))\n",
    "            else:\n",
    "                y_pred_proba = np.random.random(len(X_test))\n",
    "                print(\"âš ï¸ í™•ë¥  ì˜ˆì¸¡ ë¶ˆê°€ - ëœë¤ í™•ë¥  ì‚¬ìš©\")\n",
    "        \n",
    "        print(\"âœ… ì˜ˆì¸¡ ì™„ë£Œ\")\n",
    "        \n",
    "    else:\n",
    "        print(\"âš ï¸ í…ŒìŠ¤íŠ¸ ë°ì´í„°ê°€ ì—†ì–´ ìƒ˜í”Œ ë°ì´í„° ìƒì„±...\")\n",
    "        np.random.seed(42)\n",
    "        n_samples = 500\n",
    "        n_features = 10\n",
    "        \n",
    "        X_test = pd.DataFrame(\n",
    "            np.random.normal(0, 1, (n_samples, n_features)),\n",
    "            columns=[f'feature_{i}' for i in range(n_features)]\n",
    "        )\n",
    "        y_test = np.random.choice([0, 1], n_samples, p=[0.7, 0.3])\n",
    "        y_pred = model.predict(X_test.values)\n",
    "        \n",
    "        if hasattr(model, 'predict_proba'):\n",
    "            y_pred_proba = model.predict_proba(X_test.values)[:, 1]\n",
    "        else:\n",
    "            y_pred_proba = np.random.random(n_samples)\n",
    "        \n",
    "        model_info['feature_names'] = list(X_test.columns)\n",
    "\n",
    "# ìµœì¢… ì •ë³´ ì¶œë ¥\n",
    "print(f\"\\nğŸ“‹ í‰ê°€ ë°ì´í„° ì •ë³´:\")\n",
    "print(f\"  â€¢ ëª¨ë¸: {model_info['model_name']}\")\n",
    "print(f\"  â€¢ í…ŒìŠ¤íŠ¸ ìƒ˜í”Œ ìˆ˜: {len(X_test)}\")\n",
    "print(f\"  â€¢ í”¼ì²˜ ìˆ˜: {X_test.shape[1]}\")\n",
    "\n",
    "if y_test is not None:\n",
    "    print(f\"  â€¢ ì´íƒˆ ê³ ê° ë¹„ìœ¨: {np.mean(y_test):.2%}\")\n",
    "    print(f\"  â€¢ ì˜ˆì¸¡ ì´íƒˆ ë¹„ìœ¨: {np.mean(y_pred):.2%}\")\n",
    "else:\n",
    "    print(\"  â€¢ ì‹¤ì œ ë¼ë²¨ ì—†ìŒ (ì˜ˆì¸¡ë§Œ ìˆ˜í–‰)\")\n",
    "\n",
    "print(\"\\nğŸ¯ í‰ê°€ ì¤€ë¹„ ì™„ë£Œ!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1cb77e8",
   "metadata": {},
   "source": [
    "## 3. ìƒì„¸ ì„±ëŠ¥ í‰ê°€\n",
    "\n",
    "ë‹¤ì–‘í•œ í‰ê°€ ì§€í‘œë¥¼ í†µí•´ ëª¨ë¸ì˜ ì„±ëŠ¥ì„ ì¢…í•©ì ìœ¼ë¡œ ë¶„ì„í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04f1b97e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì„±ëŠ¥ ì§€í‘œ ê³„ì‚°\n",
    "if y_test is not None:\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred)\n",
    "    recall = recall_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    auc = roc_auc_score(y_test, y_pred_proba)\n",
    "    ap_score = average_precision_score(y_test, y_pred_proba)\n",
    "    \n",
    "    print(f\"ğŸ¯ {model_info['model_name']} ìƒì„¸ ì„±ëŠ¥ í‰ê°€\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"ğŸ“Š ê¸°ë³¸ ë¶„ë¥˜ ì„±ëŠ¥:\")\n",
    "    print(f\"  â€¢ Accuracy (ì •í™•ë„):     {accuracy:.4f}\")\n",
    "    print(f\"  â€¢ Precision (ì •ë°€ë„):    {precision:.4f}\")\n",
    "    print(f\"  â€¢ Recall (ì¬í˜„ìœ¨):       {recall:.4f}\")\n",
    "    print(f\"  â€¢ F1-Score:             {f1:.4f}\")\n",
    "    print(f\"\\\\nğŸ“ˆ í™•ë¥  ê¸°ë°˜ ì„±ëŠ¥:\")\n",
    "    print(f\"  â€¢ ROC-AUC:              {auc:.4f}\")\n",
    "    print(f\"  â€¢ Average Precision:    {ap_score:.4f}\")\n",
    "    \n",
    "    # í˜¼ë™ í–‰ë ¬\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    tn, fp, fn, tp = cm.ravel()\n",
    "    \n",
    "    print(f\"\\\\nğŸ” í˜¼ë™ í–‰ë ¬ ë¶„ì„:\")\n",
    "    print(f\"  â€¢ True Negatives (TN):   {tn:,} (ì˜¬ë°”ë¥¸ ë¹„ì´íƒˆ ì˜ˆì¸¡)\")\n",
    "    print(f\"  â€¢ False Positives (FP):  {fp:,} (ì˜ëª»ëœ ì´íƒˆ ì˜ˆì¸¡)\")\n",
    "    print(f\"  â€¢ False Negatives (FN):  {fn:,} (ë†“ì¹œ ì´íƒˆ ê³ ê°)\")\n",
    "    print(f\"  â€¢ True Positives (TP):   {tp:,} (ì˜¬ë°”ë¥¸ ì´íƒˆ ì˜ˆì¸¡)\")\n",
    "    \n",
    "    # ì¶”ê°€ ì§€í‘œ\n",
    "    specificity = tn / (tn + fp) if (tn + fp) > 0 else 0\n",
    "    npv = tn / (tn + fn) if (tn + fn) > 0 else 0\n",
    "    \n",
    "    print(f\"\\\\nğŸ“‹ ì¶”ê°€ ì„±ëŠ¥ ì§€í‘œ:\")\n",
    "    print(f\"  â€¢ Specificity (íŠ¹ì´ë„):  {specificity:.4f}\")\n",
    "    print(f\"  â€¢ NPV (ìŒì„±ì˜ˆì¸¡ë„):      {npv:.4f}\")\n",
    "    print(f\"  â€¢ False Positive Rate:  {fp/(fp+tn):.4f}\")\n",
    "    print(f\"  â€¢ False Negative Rate:  {fn/(fn+tp):.4f}\")\n",
    "    \n",
    "    # ë¶„ë¥˜ ë³´ê³ ì„œ\n",
    "    print(f\"\\\\nğŸ“‘ ìƒì„¸ ë¶„ë¥˜ ë³´ê³ ì„œ:\")\n",
    "    print(classification_report(y_test, y_pred, \n",
    "                              target_names=['ë¹„ì´íƒˆ', 'ì´íƒˆ'],\n",
    "                              digits=4))\n",
    "else:\n",
    "    print(\"âš ï¸ ì‹¤ì œ ë¼ë²¨ì´ ì—†ì–´ ì„±ëŠ¥ í‰ê°€ë¥¼ ê±´ë„ˆëœë‹ˆë‹¤.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dc28735",
   "metadata": {},
   "source": [
    "## 4. ì‹œê°í™” í‰ê°€\n",
    "\n",
    "ROC ê³¡ì„ , Precision-Recall ê³¡ì„ , í˜¼ë™ í–‰ë ¬ ë“±ì„ í†µí•œ ì‹œê°ì  í‰ê°€ë¥¼ ì§„í–‰í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2d807da",
   "metadata": {},
   "outputs": [],
   "source": [
    "if y_test is not None:\n",
    "    # ì‹œê°í™” ì„¤ì •\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "    fig.suptitle(f'{model_info[\"model_name\"]} - ëª¨ë¸ ì„±ëŠ¥ ì‹œê°í™”', fontsize=16, y=1.02)\n",
    "    \n",
    "    # 1. ROC ê³¡ì„ \n",
    "    fpr, tpr, roc_thresholds = roc_curve(y_test, y_pred_proba)\n",
    "    axes[0, 0].plot(fpr, tpr, linewidth=2, label=f'ROC (AUC = {auc:.3f})')\n",
    "    axes[0, 0].plot([0, 1], [0, 1], 'k--', alpha=0.5, label='Random')\n",
    "    axes[0, 0].set_xlabel('False Positive Rate')\n",
    "    axes[0, 0].set_ylabel('True Positive Rate')\n",
    "    axes[0, 0].set_title('ROC Curve')\n",
    "    axes[0, 0].legend()\n",
    "    axes[0, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 2. Precision-Recall ê³¡ì„ \n",
    "    precision_vals, recall_vals, pr_thresholds = precision_recall_curve(y_test, y_pred_proba)\n",
    "    axes[0, 1].plot(recall_vals, precision_vals, linewidth=2, \n",
    "                   label=f'PR (AP = {ap_score:.3f})')\n",
    "    axes[0, 1].axhline(y=np.mean(y_test), color='k', linestyle='--', alpha=0.5, \n",
    "                      label=f'Baseline = {np.mean(y_test):.3f}')\n",
    "    axes[0, 1].set_xlabel('Recall')\n",
    "    axes[0, 1].set_ylabel('Precision')\n",
    "    axes[0, 1].set_title('Precision-Recall Curve')\n",
    "    axes[0, 1].legend()\n",
    "    axes[0, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 3. í˜¼ë™ í–‰ë ¬ íˆíŠ¸ë§µ\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "                xticklabels=['ë¹„ì´íƒˆ', 'ì´íƒˆ'],\n",
    "                yticklabels=['ë¹„ì´íƒˆ', 'ì´íƒˆ'],\n",
    "                ax=axes[0, 2])\n",
    "    axes[0, 2].set_title('Confusion Matrix')\n",
    "    axes[0, 2].set_xlabel('Predicted')\n",
    "    axes[0, 2].set_ylabel('Actual')\n",
    "    \n",
    "    # 4. ì˜ˆì¸¡ í™•ë¥  ë¶„í¬\n",
    "    axes[1, 0].hist(y_pred_proba[y_test == 0], bins=30, alpha=0.7, \n",
    "                   label='ë¹„ì´íƒˆ ê³ ê°', color='skyblue', density=True)\n",
    "    axes[1, 0].hist(y_pred_proba[y_test == 1], bins=30, alpha=0.7, \n",
    "                   label='ì´íƒˆ ê³ ê°', color='salmon', density=True)\n",
    "    axes[1, 0].axvline(x=0.5, color='red', linestyle='--', alpha=0.8, label='ì„ê³„ê°’ 0.5')\n",
    "    axes[1, 0].set_xlabel('Predicted Probability')\n",
    "    axes[1, 0].set_ylabel('Density')\n",
    "    axes[1, 0].set_title('Prediction Probability Distribution')\n",
    "    axes[1, 0].legend()\n",
    "    axes[1, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 5. ì„ê³„ê°’ë³„ ì„±ëŠ¥ ë³€í™”\n",
    "    thresholds = np.linspace(0, 1, 100)\n",
    "    precisions = []\n",
    "    recalls = []\n",
    "    f1_scores = []\n",
    "    \n",
    "    for threshold in thresholds:\n",
    "        y_pred_thresh = (y_pred_proba >= threshold).astype(int)\n",
    "        if len(np.unique(y_pred_thresh)) == 2:  # ë‘ í´ë˜ìŠ¤ ëª¨ë‘ ì˜ˆì¸¡ëœ ê²½ìš°ë§Œ\n",
    "            prec = precision_score(y_test, y_pred_thresh, zero_division=0)\n",
    "            rec = recall_score(y_test, y_pred_thresh, zero_division=0)\n",
    "            f1 = f1_score(y_test, y_pred_thresh, zero_division=0)\n",
    "        else:\n",
    "            prec = rec = f1 = 0\n",
    "        \n",
    "        precisions.append(prec)\n",
    "        recalls.append(rec)\n",
    "        f1_scores.append(f1)\n",
    "    \n",
    "    axes[1, 1].plot(thresholds, precisions, label='Precision', linewidth=2)\n",
    "    axes[1, 1].plot(thresholds, recalls, label='Recall', linewidth=2)\n",
    "    axes[1, 1].plot(thresholds, f1_scores, label='F1-Score', linewidth=2)\n",
    "    axes[1, 1].axvline(x=0.5, color='red', linestyle='--', alpha=0.8, label='Default Threshold')\n",
    "    axes[1, 1].set_xlabel('Threshold')\n",
    "    axes[1, 1].set_ylabel('Score')\n",
    "    axes[1, 1].set_title('Performance vs Threshold')\n",
    "    axes[1, 1].legend()\n",
    "    axes[1, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 6. ì˜ˆì¸¡ ê²°ê³¼ ë¶„í¬ (ì‹¤ì œ vs ì˜ˆì¸¡)\n",
    "    result_df = pd.DataFrame({\n",
    "        'Actual': y_test,\n",
    "        'Predicted': y_pred,\n",
    "        'Probability': y_pred_proba\n",
    "    })\n",
    "    \n",
    "    # ì„±ëŠ¥ ë§¤íŠ¸ë¦­ìŠ¤\n",
    "    metrics_data = {\n",
    "        'Metric': ['Accuracy', 'Precision', 'Recall', 'F1-Score', 'ROC-AUC', 'PR-AUC'],\n",
    "        'Score': [accuracy, precision, recall, f1, auc, ap_score]\n",
    "    }\n",
    "    metrics_df = pd.DataFrame(metrics_data)\n",
    "    \n",
    "    bars = axes[1, 2].bar(metrics_df['Metric'], metrics_df['Score'], \n",
    "                         color=['skyblue', 'lightgreen', 'salmon', 'gold', 'lightcoral', 'plum'])\n",
    "    axes[1, 2].set_ylim(0, 1)\n",
    "    axes[1, 2].set_title('Performance Metrics Summary')\n",
    "    axes[1, 2].set_xlabel('Metrics')\n",
    "    axes[1, 2].set_ylabel('Score')\n",
    "    axes[1, 2].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # ë§‰ëŒ€ ìœ„ì— ê°’ í‘œì‹œ\n",
    "    for bar, score in zip(bars, metrics_df['Score']):\n",
    "        axes[1, 2].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01, \n",
    "                       f'{score:.3f}', ha='center', va='bottom', fontweight='bold')\n",
    "    \n",
    "    axes[1, 2].grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # ìµœì  ì„ê³„ê°’ ì°¾ê¸° (F1-Score ê¸°ì¤€)\n",
    "    optimal_idx = np.argmax(f1_scores)\n",
    "    optimal_threshold = thresholds[optimal_idx]\n",
    "    optimal_f1 = f1_scores[optimal_idx]\n",
    "    \n",
    "    print(f\"\\\\nğŸ¯ ìµœì  ì„ê³„ê°’ ë¶„ì„ (F1-Score ê¸°ì¤€):\")\n",
    "    print(f\"  â€¢ ìµœì  ì„ê³„ê°’: {optimal_threshold:.3f}\")\n",
    "    print(f\"  â€¢ ìµœì  F1-Score: {optimal_f1:.4f}\")\n",
    "    print(f\"  â€¢ í˜„ì¬ ì„ê³„ê°’ (0.5) F1-Score: {f1:.4f}\")\n",
    "    print(f\"  â€¢ ê°œì„  ê°€ëŠ¥ì„±: {optimal_f1 - f1:+.4f}\")\n",
    "    \n",
    "else:\n",
    "    print(\"âš ï¸ ì‹¤ì œ ë¼ë²¨ì´ ì—†ì–´ ì‹œê°í™”ë¥¼ ê±´ë„ˆëœë‹ˆë‹¤.\")\n",
    "    \n",
    "print(\"\\nğŸ“Š ì‹œê°í™” ì™„ë£Œ!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c10df2d4",
   "metadata": {},
   "source": [
    "## 4.5. ëª¨ë¸ í•´ì„ì„± ë¶„ì„\n",
    "\n",
    "ëª¨ë¸ì˜ ì˜ˆì¸¡ ê²°ê³¼ë¥¼ í•´ì„í•˜ê³  ì£¼ìš” í”¼ì²˜ë“¤ì˜ ì¤‘ìš”ë„ë¥¼ ë¶„ì„í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec5d788d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ğŸ” ëª¨ë¸ í•´ì„ì„± ë¶„ì„\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# ëª¨ë¸ì˜ í”¼ì²˜ ì¤‘ìš”ë„ ë¶„ì„ (íŠ¸ë¦¬ ê¸°ë°˜ ëª¨ë¸ë“¤)\n",
    "model_name = model_info.get('model_name', 'Unknown Model')\n",
    "print(f\"ğŸ“‹ ë¶„ì„ ëŒ€ìƒ ëª¨ë¸: {model_name}\")\n",
    "\n",
    "# í˜„ì¬ ëª¨ë¸ì´ í”¼ì²˜ ì¤‘ìš”ë„ë¥¼ ì œê³µí•˜ëŠ”ì§€ í™•ì¸\n",
    "show_feature_importance = False\n",
    "if 'model' in locals() and hasattr(model, 'feature_importances_'):\n",
    "    show_feature_importance = True\n",
    "    print(\"âœ… í”¼ì²˜ ì¤‘ìš”ë„ ì •ë³´ ì‚¬ìš© ê°€ëŠ¥\")\n",
    "else:\n",
    "    print(\"âš ï¸ í˜„ì¬ ëª¨ë¸ì€ í”¼ì²˜ ì¤‘ìš”ë„ë¥¼ ì§ì ‘ ì œê³µí•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.\")\n",
    "\n",
    "if show_feature_importance:\n",
    "    try:\n",
    "        # í”¼ì²˜ ì´ë¦„ ê°€ì ¸ì˜¤ê¸°\n",
    "        if 'feature_names' in model_info and model_info['feature_names']:\n",
    "            feature_names = model_info['feature_names']\n",
    "        elif hasattr(X_test, 'columns'):\n",
    "            feature_names = list(X_test.columns)\n",
    "        else:\n",
    "            feature_names = [f'feature_{i}' for i in range(len(model.feature_importances_))]\n",
    "        \n",
    "        # í”¼ì²˜ ì´ë¦„ê³¼ ì¤‘ìš”ë„ì˜ ê¸¸ì´ë¥¼ ë§ì¶¤\n",
    "        if len(feature_names) != len(model.feature_importances_):\n",
    "            print(f\"âš ï¸ í”¼ì²˜ ì´ë¦„ ìˆ˜({len(feature_names)})ì™€ ì¤‘ìš”ë„ ìˆ˜({len(model.feature_importances_)})ê°€ ì¼ì¹˜í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.\")\n",
    "            feature_names = [f'feature_{i}' for i in range(len(model.feature_importances_))]\n",
    "        \n",
    "        # í”¼ì²˜ ì¤‘ìš”ë„ ë°ì´í„°í”„ë ˆì„ ìƒì„±\n",
    "        importance_df = pd.DataFrame({\n",
    "            'feature': feature_names,\n",
    "            'importance': model.feature_importances_\n",
    "        }).sort_values('importance', ascending=True)\n",
    "        \n",
    "        # í”¼ì²˜ ì¤‘ìš”ë„ ì‹œê°í™”\n",
    "        print(f\"\\nğŸŒ³ í”¼ì²˜ ì¤‘ìš”ë„ ì‹œê°í™”:\")\n",
    "        \n",
    "        # ìƒìœ„ 15ê°œ í”¼ì²˜ë§Œ í‘œì‹œ\n",
    "        top_features = importance_df.tail(15)\n",
    "        \n",
    "        plt.figure(figsize=(12, 8))\n",
    "        bars = plt.barh(range(len(top_features)), top_features['importance'], \n",
    "                       color='skyblue', edgecolor='navy', alpha=0.7)\n",
    "        plt.yticks(range(len(top_features)), top_features['feature'])\n",
    "        plt.xlabel('Feature Importance')\n",
    "        plt.title(f'{model_name} - Top 15 Feature Importances')\n",
    "        plt.grid(True, alpha=0.3, axis='x')\n",
    "        \n",
    "        # ë§‰ëŒ€ ëì— ê°’ í‘œì‹œ\n",
    "        for i, bar in enumerate(bars):\n",
    "            width = bar.get_width()\n",
    "            plt.text(width + width*0.01, bar.get_y() + bar.get_height()/2, \n",
    "                    f'{width:.4f}', ha='left', va='center', fontsize=9)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        # ìƒìœ„ 10ê°œ ì¤‘ìš” í”¼ì²˜ ì¶œë ¥\n",
    "        print(f\"\\nğŸ“Š ìƒìœ„ 10ê°œ ì¤‘ìš” í”¼ì²˜:\")\n",
    "        top_10 = importance_df.tail(10)\n",
    "        for i, (idx, row) in enumerate(top_10.iterrows(), 1):\n",
    "            print(f\"  {i:2d}. {row['feature']:<25}: {row['importance']:.6f}\")\n",
    "        \n",
    "        # í”¼ì²˜ ì¤‘ìš”ë„ í†µê³„\n",
    "        print(f\"\\nğŸ“ˆ í”¼ì²˜ ì¤‘ìš”ë„ í†µê³„:\")\n",
    "        print(f\"  â€¢ ì´ í”¼ì²˜ ìˆ˜: {len(importance_df)}\")\n",
    "        print(f\"  â€¢ í‰ê·  ì¤‘ìš”ë„: {importance_df['importance'].mean():.6f}\")\n",
    "        print(f\"  â€¢ ì¤‘ìš”ë„ í‘œì¤€í¸ì°¨: {importance_df['importance'].std():.6f}\")\n",
    "        print(f\"  â€¢ ìƒìœ„ 10ê°œ í”¼ì²˜ì˜ ëˆ„ì  ì¤‘ìš”ë„: {top_10['importance'].sum():.3f}\")\n",
    "        print(f\"  â€¢ ìƒìœ„ 5ê°œ í”¼ì²˜ì˜ ëˆ„ì  ì¤‘ìš”ë„: {importance_df.tail(5)['importance'].sum():.3f}\")\n",
    "        \n",
    "        # ì¤‘ìš”ë„ ë²”ì£¼ë³„ ë¶„ì„\n",
    "        high_importance = importance_df[importance_df['importance'] > importance_df['importance'].quantile(0.9)]\n",
    "        medium_importance = importance_df[\n",
    "            (importance_df['importance'] > importance_df['importance'].quantile(0.7)) & \n",
    "            (importance_df['importance'] <= importance_df['importance'].quantile(0.9))\n",
    "        ]\n",
    "        \n",
    "        print(f\"\\nğŸ¯ ì¤‘ìš”ë„ ë²”ì£¼ë³„ ë¶„ì„:\")\n",
    "        print(f\"  â€¢ ê³ ì¤‘ìš”ë„ í”¼ì²˜ (ìƒìœ„ 10%): {len(high_importance)}ê°œ\")\n",
    "        print(f\"  â€¢ ì¤‘ê°„ì¤‘ìš”ë„ í”¼ì²˜ (70-90%): {len(medium_importance)}ê°œ\")\n",
    "        print(f\"  â€¢ ì €ì¤‘ìš”ë„ í”¼ì²˜ (í•˜ìœ„ 70%): {len(importance_df) - len(high_importance) - len(medium_importance)}ê°œ\")\n",
    "        \n",
    "        # ê³ ì¤‘ìš”ë„ í”¼ì²˜ë“¤ì˜ íŠ¹ì„± ë¶„ì„\n",
    "        if len(high_importance) > 0:\n",
    "            print(f\"\\nğŸ”¥ ê³ ì¤‘ìš”ë„ í”¼ì²˜ ëª©ë¡:\")\n",
    "            for idx, row in high_importance.iterrows():\n",
    "                print(f\"  â€¢ {row['feature']}: {row['importance']:.6f}\")\n",
    "                \n",
    "    except Exception as e:\n",
    "        print(f\"âš ï¸ í”¼ì²˜ ì¤‘ìš”ë„ ë¶„ì„ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}\")\n",
    "        import traceback\n",
    "        print(f\"ìƒì„¸ ì˜¤ë¥˜: {traceback.format_exc()}\")\n",
    "else:\n",
    "    print(f\"\\nğŸ’¡ ëŒ€ì•ˆì  ëª¨ë¸ í•´ì„ ë°©ë²•:\")\n",
    "    print(f\"  â€¢ SHAP (SHapley Additive exPlanations) ê°’ ê³„ì‚°\")\n",
    "    print(f\"  â€¢ LIME (Local Interpretable Model-agnostic Explanations)\")\n",
    "    print(f\"  â€¢ Permutation Importance ê³„ì‚°\")\n",
    "    print(f\"  â€¢ ë¶€ë¶„ ì˜ì¡´ì„± í”Œë¡¯ (Partial Dependence Plots)\")\n",
    "\n",
    "print(f\"\\nğŸ” ëª¨ë¸ í•´ì„ì„± ë¶„ì„ ì™„ë£Œ!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc8689a9",
   "metadata": {},
   "source": [
    "## 5. ë¹„ì¦ˆë‹ˆìŠ¤ ì„íŒ©íŠ¸ ë¶„ì„\n",
    "\n",
    "ëª¨ë¸ì˜ ì„±ëŠ¥ì„ ì‹¤ì œ ë¹„ì¦ˆë‹ˆìŠ¤ ê´€ì ì—ì„œ í‰ê°€í•˜ê³  ê²½ì œì  ê°€ì¹˜ë¥¼ ë¶„ì„í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89952261",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ë¹„ì¦ˆë‹ˆìŠ¤ ê°€ì •ê°’ ì„¤ì •\n",
    "business_assumptions = {\n",
    "    'avg_customer_value': 1000,      # ê³ ê° í‰ê·  ìƒì• ê°€ì¹˜ ($)\n",
    "    'retention_cost': 50,            # ê³ ê° ìœ ì§€ ë¹„ìš© ($)\n",
    "    'acquisition_cost': 200,         # ì‹ ê·œ ê³ ê° íšë“ ë¹„ìš© ($)\n",
    "    'intervention_success_rate': 0.3, # ì´íƒˆ ë°©ì§€ ìº í˜ì¸ ì„±ê³µë¥ \n",
    "    'total_customers': 10000         # ì „ì²´ ê³ ê° ìˆ˜ (ì˜ˆìƒ)\n",
    "}\n",
    "\n",
    "print(\"ğŸ’¼ ë¹„ì¦ˆë‹ˆìŠ¤ ì„íŒ©íŠ¸ ë¶„ì„\")\n",
    "print(\"=\" * 60)\n",
    "print(\"ğŸ“‹ ë¹„ì¦ˆë‹ˆìŠ¤ ê°€ì •:\")\n",
    "for key, value in business_assumptions.items():\n",
    "    if 'rate' in key:\n",
    "        print(f\"  â€¢ {key}: {value:.1%}\")\n",
    "    elif 'cost' in key or 'value' in key:\n",
    "        print(f\"  â€¢ {key}: ${value:,}\")\n",
    "    else:\n",
    "        print(f\"  â€¢ {key}: {value:,}\")\n",
    "\n",
    "if y_test is not None:\n",
    "    # í˜¼ë™ í–‰ë ¬ ê¸°ë°˜ ë¹„ì¦ˆë‹ˆìŠ¤ ì„íŒ©íŠ¸ ê³„ì‚°\n",
    "    print(f\"\\\\nğŸ¯ ì˜ˆì¸¡ ì„±ëŠ¥ ê¸°ë°˜ ë¹„ì¦ˆë‹ˆìŠ¤ ì‹œë‚˜ë¦¬ì˜¤:\")\n",
    "    \n",
    "    # ì „ì²´ ê³ ê° ìˆ˜ë¡œ ìŠ¤ì¼€ì¼ë§\n",
    "    scale_factor = business_assumptions['total_customers'] / len(y_test)\n",
    "    \n",
    "    scaled_tp = int(tp * scale_factor)\n",
    "    scaled_fp = int(fp * scale_factor)\n",
    "    scaled_fn = int(fn * scale_factor)\n",
    "    scaled_tn = int(tn * scale_factor)\n",
    "    \n",
    "    print(f\"\\\\nğŸ“Š ì „ì²´ ê³ ê° {business_assumptions['total_customers']:,}ëª… ëŒ€ìƒ ì˜ˆìƒ ê²°ê³¼:\")\n",
    "    print(f\"  â€¢ ì˜¬ë°”ë¥¸ ì´íƒˆ ì˜ˆì¸¡ (TP): {scaled_tp:,}ëª…\")\n",
    "    print(f\"  â€¢ ì˜ëª»ëœ ì´íƒˆ ì˜ˆì¸¡ (FP): {scaled_fp:,}ëª…\")\n",
    "    print(f\"  â€¢ ë†“ì¹œ ì´íƒˆ ê³ ê° (FN): {scaled_fn:,}ëª…\")\n",
    "    print(f\"  â€¢ ì˜¬ë°”ë¥¸ ìœ ì§€ ì˜ˆì¸¡ (TN): {scaled_tn:,}ëª…\")\n",
    "    \n",
    "    # ë¹„ìš©-í¸ìµ ë¶„ì„\n",
    "    # 1. ì˜¬ë°”ë¥¸ ì´íƒˆ ì˜ˆì¸¡ ì‹œ ë¹„ìš©/í¸ìµ\n",
    "    prevented_churn = scaled_tp * business_assumptions['intervention_success_rate']\n",
    "    intervention_cost_tp = scaled_tp * business_assumptions['retention_cost']\n",
    "    saved_value_tp = prevented_churn * business_assumptions['avg_customer_value']\n",
    "    \n",
    "    # 2. ì˜ëª»ëœ ì´íƒˆ ì˜ˆì¸¡ ì‹œ ë¹„ìš©\n",
    "    intervention_cost_fp = scaled_fp * business_assumptions['retention_cost']\n",
    "    \n",
    "    # 3. ë†“ì¹œ ì´íƒˆ ê³ ê° ì†ì‹¤\n",
    "    lost_value_fn = scaled_fn * business_assumptions['avg_customer_value']\n",
    "    replacement_cost_fn = scaled_fn * business_assumptions['acquisition_cost']\n",
    "    \n",
    "    # ì´ ë¹„ìš© ë° í¸ìµ\n",
    "    total_intervention_cost = intervention_cost_tp + intervention_cost_fp\n",
    "    total_saved_value = saved_value_tp\n",
    "    total_lost_value = lost_value_fn + replacement_cost_fn\n",
    "    \n",
    "    net_benefit = total_saved_value - total_intervention_cost - total_lost_value\n",
    "    \n",
    "    print(f\"\\\\nğŸ’° ê²½ì œì  ì˜í–¥ ë¶„ì„:\")\n",
    "    print(f\"\\\\nğŸ“ˆ í¸ìµ (ìˆ˜ìµ):\")\n",
    "    print(f\"  â€¢ ë°©ì§€ëœ ì´íƒˆ ê³ ê°: {prevented_churn:.0f}ëª…\")\n",
    "    print(f\"  â€¢ ì ˆì•½ëœ ê³ ê° ê°€ì¹˜: ${total_saved_value:,.0f}\")\n",
    "    \n",
    "    print(f\"\\\\nğŸ“‰ ë¹„ìš© (ì§€ì¶œ):\")\n",
    "    print(f\"  â€¢ ì´íƒˆ ë°©ì§€ ìº í˜ì¸ ë¹„ìš©: ${total_intervention_cost:,.0f}\")\n",
    "    print(f\"    - ì˜¬ë°”ë¥¸ ì˜ˆì¸¡ ëŒ€ìƒ: ${intervention_cost_tp:,.0f}\")\n",
    "    print(f\"    - ì˜ëª»ëœ ì˜ˆì¸¡ ëŒ€ìƒ: ${intervention_cost_fp:,.0f}\")\n",
    "    print(f\"  â€¢ ë†“ì¹œ ì´íƒˆë¡œ ì¸í•œ ì†ì‹¤: ${total_lost_value:,.0f}\")\n",
    "    print(f\"    - ê³ ê° ê°€ì¹˜ ì†ì‹¤: ${lost_value_fn:,.0f}\")\n",
    "    print(f\"    - ì‹ ê·œ ê³ ê° íšë“ ë¹„ìš©: ${replacement_cost_fn:,.0f}\")\n",
    "    \n",
    "    print(f\"\\\\nğŸ¯ ìµœì¢… ê²°ê³¼:\")\n",
    "    print(f\"  â€¢ ìˆœ í¸ìµ (Net Benefit): ${net_benefit:,.0f}\")\n",
    "    \n",
    "    if net_benefit > 0:\n",
    "        print(f\"  âœ… ëª¨ë¸ ë„ì… ê¶Œì¥! ì—°ê°„ ì•½ ${net_benefit:,.0f} ì ˆì•½ ì˜ˆìƒ\")\n",
    "    else:\n",
    "        print(f\"  âš ï¸ ëª¨ë¸ ê°œì„  í•„ìš”. í˜„ì¬ ${abs(net_benefit):,.0f} ì†ì‹¤ ì˜ˆìƒ\")\n",
    "    \n",
    "    # ROI ê³„ì‚°\n",
    "    investment_cost = total_intervention_cost\n",
    "    if investment_cost > 0:\n",
    "        roi = (total_saved_value - investment_cost) / investment_cost * 100\n",
    "        print(f\"  â€¢ ROI (íˆ¬ì ìˆ˜ìµë¥ ): {roi:.1f}%\")\n",
    "    \n",
    "    # ì„ê³„ê°’ ìµœì í™”ë¥¼ í†µí•œ ë¹„ì¦ˆë‹ˆìŠ¤ ê°œì„  ê°€ëŠ¥ì„±\n",
    "    print(f\"\\\\nğŸ”§ ì„ê³„ê°’ ìµœì í™” ê¶Œì¥ì‚¬í•­:\")\n",
    "    if optimal_threshold != 0.5:\n",
    "        y_pred_optimal = (y_pred_proba >= optimal_threshold).astype(int)\n",
    "        cm_optimal = confusion_matrix(y_test, y_pred_optimal)\n",
    "        tn_opt, fp_opt, fn_opt, tp_opt = cm_optimal.ravel()\n",
    "        \n",
    "        # ìµœì  ì„ê³„ê°’ìœ¼ë¡œ ë‹¤ì‹œ ê³„ì‚°\n",
    "        scaled_tp_opt = int(tp_opt * scale_factor)\n",
    "        scaled_fp_opt = int(fp_opt * scale_factor)\n",
    "        scaled_fn_opt = int(fn_opt * scale_factor)\n",
    "        \n",
    "        prevented_churn_opt = scaled_tp_opt * business_assumptions['intervention_success_rate']\n",
    "        intervention_cost_opt = (scaled_tp_opt + scaled_fp_opt) * business_assumptions['retention_cost']\n",
    "        saved_value_opt = prevented_churn_opt * business_assumptions['avg_customer_value']\n",
    "        lost_value_opt = scaled_fn_opt * (business_assumptions['avg_customer_value'] + business_assumptions['acquisition_cost'])\n",
    "        \n",
    "        net_benefit_opt = saved_value_opt - intervention_cost_opt - lost_value_opt\n",
    "        improvement = net_benefit_opt - net_benefit\n",
    "        \n",
    "        print(f\"  â€¢ ìµœì  ì„ê³„ê°’ {optimal_threshold:.3f} ì ìš© ì‹œ:\")\n",
    "        print(f\"    - ì¶”ê°€ ìˆœ í¸ìµ: ${improvement:,.0f}\")\n",
    "        print(f\"    - ì´ ìˆœ í¸ìµ: ${net_benefit_opt:,.0f}\")\n",
    "        \n",
    "        if improvement > 0:\n",
    "            print(f\"    âœ… ì„ê³„ê°’ ì¡°ì • ê¶Œì¥!\")\n",
    "        else:\n",
    "            print(f\"    âš ï¸ í˜„ì¬ ì„ê³„ê°’ ìœ ì§€ ê¶Œì¥\")\n",
    "    \n",
    "    # ì‹œë‚˜ë¦¬ì˜¤ ë¶„ì„ ì‹œê°í™”\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
    "    \n",
    "    # ë¹„ìš©-í¸ìµ ë¶„ì„ ì°¨íŠ¸\n",
    "    categories = ['ì ˆì•½ëœ\\\\nê³ ê° ê°€ì¹˜', 'ìº í˜ì¸\\\\në¹„ìš©', 'ë†“ì¹œ ì´íƒˆ\\\\nì†ì‹¤']\n",
    "    values = [total_saved_value, -total_intervention_cost, -total_lost_value]\n",
    "    colors = ['green', 'orange', 'red']\n",
    "    \n",
    "    bars = axes[0].bar(categories, values, color=colors, alpha=0.7)\n",
    "    axes[0].axhline(y=0, color='black', linestyle='-', alpha=0.5)\n",
    "    axes[0].set_title('ë¹„ìš©-í¸ìµ ë¶„ì„')\n",
    "    axes[0].set_ylabel('ê¸ˆì•¡ ($)')\n",
    "    \n",
    "    # ë§‰ëŒ€ ìœ„ì— ê°’ í‘œì‹œ\n",
    "    for bar, value in zip(bars, values):\n",
    "        height = bar.get_height()\n",
    "        axes[0].text(bar.get_x() + bar.get_width()/2, height + (50000 if height > 0 else -50000), \n",
    "                    f'${abs(value):,.0f}', ha='center', va='bottom' if height > 0 else 'top',\n",
    "                    fontweight='bold')\n",
    "    \n",
    "    # ìˆœ í¸ìµ í‘œì‹œ\n",
    "    axes[0].text(0.5, 0.95, f'ìˆœ í¸ìµ: ${net_benefit:,.0f}', \n",
    "                transform=axes[0].transAxes, ha='center', va='top',\n",
    "                bbox=dict(boxstyle='round', facecolor='lightblue', alpha=0.8),\n",
    "                fontsize=12, fontweight='bold')\n",
    "    \n",
    "    # ì˜ˆì¸¡ ì •í™•ë„ë³„ ë¹„êµ\n",
    "    prediction_types = ['ì˜¬ë°”ë¥¸ ì´íƒˆ\\\\nì˜ˆì¸¡ (TP)', 'ì˜ëª»ëœ ì´íƒˆ\\\\nì˜ˆì¸¡ (FP)', \n",
    "                       'ë†“ì¹œ ì´íƒˆ\\\\nê³ ê° (FN)', 'ì˜¬ë°”ë¥¸ ìœ ì§€\\\\nì˜ˆì¸¡ (TN)']\n",
    "    prediction_counts = [scaled_tp, scaled_fp, scaled_fn, scaled_tn]\n",
    "    prediction_colors = ['darkgreen', 'orange', 'red', 'lightgreen']\n",
    "    \n",
    "    wedges, texts, autotexts = axes[1].pie(prediction_counts, labels=prediction_types, \n",
    "                                          colors=prediction_colors, autopct='%1.1f%%',\n",
    "                                          startangle=90)\n",
    "    axes[1].set_title('ì˜ˆì¸¡ ê²°ê³¼ ë¶„í¬')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "else:\n",
    "    print(\"âš ï¸ ì‹¤ì œ ë¼ë²¨ì´ ì—†ì–´ ë¹„ì¦ˆë‹ˆìŠ¤ ì„íŒ©íŠ¸ ë¶„ì„ì„ ê±´ë„ˆëœë‹ˆë‹¤.\")\n",
    "\n",
    "print(\"\\\\nğŸ’¼ ë¹„ì¦ˆë‹ˆìŠ¤ ë¶„ì„ ì™„ë£Œ!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbeace7f",
   "metadata": {},
   "source": [
    "## 6. ë°°í¬ ê¶Œì¥ì‚¬í•­\n",
    "\n",
    "ëª¨ë¸ì„ ì‹¤ì œ ìš´ì˜ í™˜ê²½ì— ë°°í¬í•˜ê¸° ìœ„í•œ ì‹¤ìš©ì ì¸ ê°€ì´ë“œë¼ì¸ì„ ì œì‹œí•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32266bc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ë°°í¬ ê¶Œì¥ì‚¬í•­ ìƒì„±\n",
    "deployment_recommendations = {\n",
    "    \"ëª¨ë¸ ì„±ëŠ¥\": {\n",
    "        \"í˜„ì¬ ì„±ëŠ¥\": f\"AUC {auc:.3f}, F1-Score {f1:.3f}\" if y_test is not None else \"í‰ê°€ ë¶ˆê°€\",\n",
    "        \"ê¶Œì¥ ì„ê³„ê°’\": f\"{optimal_threshold:.3f}\" if y_test is not None else \"0.500\",\n",
    "        \"ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§\": \"ì›”ë³„ AUC 0.75 ì´ìƒ ìœ ì§€ í•„ìš”\"\n",
    "    },\n",
    "    \"ë¹„ì¦ˆë‹ˆìŠ¤ ê°€ì¹˜\": {\n",
    "        \"ì˜ˆìƒ ROI\": f\"{roi:.1f}%\" if y_test is not None and 'roi' in locals() else \"ê³„ì‚° ë¶ˆê°€\",\n",
    "        \"ìˆœ í¸ìµ\": f\"${net_benefit:,.0f}\" if y_test is not None and 'net_benefit' in locals() else \"ë¶„ì„ í•„ìš”\",\n",
    "        \"ì ìš© ë²”ìœ„\": \"ì „ì²´ ê³ ê° ëŒ€ìƒ ì¼ê´„ ì ìš© ê¶Œì¥\"\n",
    "    },\n",
    "    \"ìš´ì˜ ê³ ë ¤ì‚¬í•­\": {\n",
    "        \"ì˜ˆì¸¡ ì£¼ê¸°\": \"ì›” 1íšŒ ë°°ì¹˜ ì²˜ë¦¬\",\n",
    "        \"ë°ì´í„° ì—…ë°ì´íŠ¸\": \"ì‹¤ì‹œê°„ í”¼ì²˜ ì—…ë°ì´íŠ¸ í•„ìš”\",\n",
    "        \"A/B í…ŒìŠ¤íŠ¸\": \"ì‹ ê·œ ê³ ê° 10% ëŒ€ìƒ í…ŒìŠ¤íŠ¸ í›„ í™•ëŒ€\"\n",
    "    },\n",
    "    \"ìœ„í—˜ ê´€ë¦¬\": {\n",
    "        \"ëª¨ë¸ ë“œë¦¬í”„íŠ¸\": \"ë¶„ê¸°ë³„ ì¬í•™ìŠµ í•„ìš”\",\n",
    "        \"ì„±ëŠ¥ ì„ê³„ì¹˜\": \"AUC 0.70 ë¯¸ë§Œ ì‹œ ì¦‰ì‹œ ì¬í•™ìŠµ\",\n",
    "        \"ë°±ì—… ì „ëµ\": \"ë£° ê¸°ë°˜ ëª¨ë¸ ë³‘í–‰ ìš´ì˜\"\n",
    "    }\n",
    "}\n",
    "\n",
    "print(\"ğŸš€ ëª¨ë¸ ë°°í¬ ê¶Œì¥ì‚¬í•­\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for category, items in deployment_recommendations.items():\n",
    "    print(f\"\\\\nğŸ“‹ {category}:\")\n",
    "    for key, value in items.items():\n",
    "        print(f\"  â€¢ {key}: {value}\")\n",
    "\n",
    "# ë°°í¬ ì²´í¬ë¦¬ìŠ¤íŠ¸\n",
    "deployment_checklist = [\n",
    "    \"âœ… ëª¨ë¸ ì„±ëŠ¥ ê²€ì¦ ì™„ë£Œ\",\n",
    "    \"âœ… ë¹„ì¦ˆë‹ˆìŠ¤ ì¼€ì´ìŠ¤ ê²€ì¦ ì™„ë£Œ\", \n",
    "    \"âš ï¸ A/B í…ŒìŠ¤íŠ¸ ê³„íš ìˆ˜ë¦½\",\n",
    "    \"âš ï¸ ëª¨ë‹ˆí„°ë§ ì‹œìŠ¤í…œ êµ¬ì¶•\",\n",
    "    \"âš ï¸ ë¡¤ë°± ê³„íš ìˆ˜ë¦½\",\n",
    "    \"âš ï¸ ë°ì´í„° íŒŒì´í”„ë¼ì¸ êµ¬ì¶•\",\n",
    "    \"âš ï¸ ì„±ëŠ¥ ëŒ€ì‹œë³´ë“œ ê°œë°œ\",\n",
    "    \"âš ï¸ ìš´ì˜íŒ€ êµìœ¡ ì™„ë£Œ\"\n",
    "]\n",
    "\n",
    "print(f\"\\\\nğŸ“ ë°°í¬ ì¤€ë¹„ ì²´í¬ë¦¬ìŠ¤íŠ¸:\")\n",
    "for item in deployment_checklist:\n",
    "    print(f\"  {item}\")\n",
    "\n",
    "# ìµœì¢… ê¶Œì¥ì‚¬í•­ ìš”ì•½\n",
    "print(f\"\\\\nğŸ¯ ìµœì¢… ê¶Œì¥ì‚¬í•­:\")\n",
    "\n",
    "if y_test is not None:\n",
    "    if auc >= 0.8 and net_benefit > 0:\n",
    "        recommendation = \"ì¦‰ì‹œ ë°°í¬ ê¶Œì¥\"\n",
    "        urgency = \"ğŸŸ¢ HIGH\"\n",
    "    elif auc >= 0.7 and net_benefit > 0:\n",
    "        recommendation = \"ì¡°ê±´ë¶€ ë°°í¬ ê¶Œì¥ (A/B í…ŒìŠ¤íŠ¸ í›„)\"\n",
    "        urgency = \"ğŸŸ¡ MEDIUM\"\n",
    "    else:\n",
    "        recommendation = \"ëª¨ë¸ ê°œì„  í›„ ì¬í‰ê°€\"\n",
    "        urgency = \"ğŸ”´ LOW\"\n",
    "else:\n",
    "    recommendation = \"ì‹¤ì œ ë°ì´í„°ë¡œ ì¬í‰ê°€ í•„ìš”\"\n",
    "    urgency = \"âšª PENDING\"\n",
    "\n",
    "print(f\"  â€¢ ë°°í¬ ìš°ì„ ìˆœìœ„: {urgency}\")\n",
    "print(f\"  â€¢ ê¶Œì¥ì‚¬í•­: {recommendation}\")\n",
    "\n",
    "# ë‹¤ìŒ ë‹¨ê³„\n",
    "next_steps = [\n",
    "    \"1. ì‹¤ì œ í”„ë¡œë•ì…˜ ë°ì´í„°ë¡œ ëª¨ë¸ ì¬ê²€ì¦\",\n",
    "    \"2. A/B í…ŒìŠ¤íŠ¸ ì„¤ê³„ ë° ì‹¤í–‰ ê³„íš ìˆ˜ë¦½\",\n",
    "    \"3. ëª¨ë¸ ëª¨ë‹ˆí„°ë§ ë° ì•ŒëŒ ì‹œìŠ¤í…œ êµ¬ì¶•\",\n",
    "    \"4. ì´íƒˆ ë°©ì§€ ìº í˜ì¸ í”„ë¡œì„¸ìŠ¤ ì„¤ê³„\",\n",
    "    \"5. ì„±ê³¼ ì¸¡ì • KPI ë° ëŒ€ì‹œë³´ë“œ ê°œë°œ\"\n",
    "]\n",
    "\n",
    "print(f\"\\\\nğŸ“… ë‹¤ìŒ ë‹¨ê³„:\")\n",
    "for step in next_steps:\n",
    "    print(f\"  {step}\")\n",
    "\n",
    "# ì—°ë½ì²˜ ë° ë¬¸ì„œí™”\n",
    "print(f\"\\\\nğŸ“„ ê´€ë ¨ ë¬¸ì„œ ë° ë¦¬ì†ŒìŠ¤:\")\n",
    "print(f\"  â€¢ ëª¨ë¸ ì €ì¥ ìœ„ì¹˜: /results/models/\")\n",
    "print(f\"  â€¢ í•™ìŠµ ë…¸íŠ¸ë¶: 03_Modeling.ipynb\")\n",
    "print(f\"  â€¢ í”„ë¡œì íŠ¸ ë¬¸ì„œ: docs/\")\n",
    "print(f\"  â€¢ ê¸°ìˆ  ë¬¸ì˜: ë°ì´í„°íŒ€\")\n",
    "print(f\"  â€¢ ë¹„ì¦ˆë‹ˆìŠ¤ ë¬¸ì˜: ë§ˆì¼€íŒ…íŒ€\")\n",
    "\n",
    "print(f\"\\\\n\" + \"=\"*60)\n",
    "print(f\"ğŸ ê³ ê° ì´íƒˆ ì˜ˆì¸¡ ëª¨ë¸ í‰ê°€ ì™„ë£Œ!\")\n",
    "print(f\"ğŸ“Š í‰ê°€ ì™„ë£Œ ì‹œê°„: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print(f\"ğŸ’¡ ì¶”ê°€ ì§ˆë¬¸ì´ë‚˜ ê°œì„ ì‚¬í•­ì´ ìˆìœ¼ì‹œë©´ í”„ë¡œì íŠ¸ íŒ€ì— ë¬¸ì˜í•´ì£¼ì„¸ìš”.\")\n",
    "print(\"=\"*60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "abc-bootcamp-FP-2025",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
